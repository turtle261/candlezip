{
  "total_results": 10,
  "papers": [
    {
      "id": "cs/0603100v1",
      "title": "Efficient Compression of Prolog Programs",
      "authors": [
        "Alin Suciu",
        "Kalman Pusztai"
      ],
      "abstract": "We propose a special-purpose class of compression algorithms for efficient\ncompression of Prolog programs. It is a dictionary-based compression method,\nspecially designed for the compression of Prolog code, and therefore we name it\nPCA (Prolog Compression Algorithm). According to the experimental results this\nmethod provides better compression than state-of-the-art general-purpose\ncompression algorithms. Since the algorithm works with Prolog syntactic\nentities (e.g. atoms, terms, etc.) the implementation of a Prolog prototype is\nstraightforward and very easy to use in any Prolog application that needs\ncompression. Although the algorithm is designed for Prolog programs, the idea\ncan be easily applied for the compression of programs written in other (logic)\nlanguages.",
      "categories": [
        "cs.PL"
      ],
      "published": "2006-03-26T20:27:40+00:00",
      "url": "http://arxiv.org/pdf/cs/0603100v1",
      "resource_uri": "arxiv://cs/0603100v1"
    },
    {
      "id": "1203.2720v1",
      "title": "Compressible Modules",
      "authors": [
        "Abhay K. Singh"
      ],
      "abstract": "The main purpose of this paper is to study under what condition compressible\nmodules are critically compressible. A sufficient condition for the injective\nhull of a critically compressible module to be critically compressible is also\nprovided. Furthermore we prove sufficient conditions for a critically\ncompressible module to be continuous. In addition, some characterization of\ncritically compressible modules in terms of CS modules, nonsingular modules and\ncyclic modules are also provided",
      "categories": [
        "math.AC",
        "math.RA",
        "16D50, 16D70, 16D80"
      ],
      "published": "2012-03-13T05:54:44+00:00",
      "url": "http://arxiv.org/pdf/1203.2720v1",
      "resource_uri": "arxiv://1203.2720v1"
    },
    {
      "id": "1604.00700v1",
      "title": "From compressed sensing to compressed bit-streams: practical encoders, tractable decoders",
      "authors": [
        "Rayan Saab",
        "Rongrong Wang",
        "\u007fOzg\u007fur Yilmaz"
      ],
      "abstract": "Compressed sensing is now established as an effective method for dimension\nreduction when the underlying signals are sparse or compressible with respect\nto some suitable basis or frame. One important, yet under-addressed problem\nregarding the compressive acquisition of analog signals is how to perform\nquantization. This is directly related to the important issues of how\n\"compressed\" compressed sensing is (in terms of the total number of bits one\nends up using after acquiring the signal) and ultimately whether compressed\nsensing can be used to obtain compressed representations of suitable signals.\nBuilding on our recent work, we propose a concrete and practicable method for\nperforming \"analog-to-information conversion\". Following a compressive signal\nacquisition stage, the proposed method consists of a quantization stage, based\non $\\Sigma\\Delta$ (sigma-delta) quantization, and a subsequent encoding\n(compression) stage that fits within the framework of compressed sensing\nseamlessly. We prove that, using this method, we can convert analog compressive\nsamples to compressed digital bitstreams and decode using tractable algorithms\nbased on convex optimization. We prove that the proposed AIC provides a nearly\noptimal encoding of sparse and compressible signals. Finally, we present\nnumerical experiments illustrating the effectiveness of the proposed\nanalog-to-information converter.",
      "categories": [
        "cs.IT",
        "math.IT"
      ],
      "published": "2016-04-03T23:13:02+00:00",
      "url": "http://arxiv.org/pdf/1604.00700v1",
      "resource_uri": "arxiv://1604.00700v1"
    },
    {
      "id": "1304.5702v2",
      "title": "Tree Compression with Top Trees",
      "authors": [
        "Philip Bille",
        "Inge Li Goertz",
        "Gad M. Landau",
        "Oren Weimann"
      ],
      "abstract": "We introduce a new compression scheme for labeled trees based on top trees.\nOur compression scheme is the first to simultaneously take advantage of\ninternal repeats in the tree (as opposed to the classical DAG compression that\nonly exploits rooted subtree repeats) while also supporting fast navigational\nqueries directly on the compressed representation. We show that the new\ncompression scheme achieves close to optimal worst-case compression, can\ncompress exponentially better than DAG compression, is never much worse than\nDAG compression, and supports navigational queries in logarithmic time.",
      "categories": [
        "cs.DS"
      ],
      "published": "2013-04-21T07:38:50+00:00",
      "url": "http://arxiv.org/pdf/1304.5702v2",
      "resource_uri": "arxiv://1304.5702v2"
    },
    {
      "id": "quant-ph/0109074v1",
      "title": "Generic Quantum Block Compression",
      "authors": [
        "John Langford"
      ],
      "abstract": "A generic approach for compiling any classical block compression algorithm\ninto a quantum block compression algorithm is presented. Using this technique,\ncompression asymptoticaly approaching the von Neumann entropy of a qubit source\ncan be achieved. The automatically compiled algorithms are competitive (in time\nand space complexity) with hand constructed quantum block compression\nalgorithms.",
      "categories": [
        "quant-ph"
      ],
      "published": "2001-09-17T16:16:30+00:00",
      "url": "http://arxiv.org/pdf/quant-ph/0109074v1",
      "resource_uri": "arxiv://quant-ph/0109074v1"
    },
    {
      "id": "2302.06183v1",
      "title": "Anti-Compression Contrastive Facial Forgery Detection",
      "authors": [
        "Jiajun Huang",
        "Xinqi Zhu",
        "Chengbin Du",
        "Siqi Ma",
        "Surya Nepal",
        "Chang Xu"
      ],
      "abstract": "Forgery facial images and videos have increased the concern of digital\nsecurity. It leads to the significant development of detecting forgery data\nrecently. However, the data, especially the videos published on the Internet,\nare usually compressed with lossy compression algorithms such as H.264. The\ncompressed data could significantly degrade the performance of recent detection\nalgorithms. The existing anti-compression algorithms focus on enhancing the\nperformance in detecting heavily compressed data but less consider the\ncompression adaption to the data from various compression levels. We believe\ncreating a forgery detection model that can handle the data compressed with\nunknown levels is important. To enhance the performance for such models, we\nconsider the weak compressed and strong compressed data as two views of the\noriginal data and they should have similar representation and relationships\nwith other samples. We propose a novel anti-compression forgery detection\nframework by maintaining closer relations within data under different\ncompression levels. Specifically, the algorithm measures the pair-wise\nsimilarity within data as the relations, and forcing the relations of weak and\nstrong compressed data close to each other, thus improving the discriminate\npower for detecting strong compressed data. To achieve a better strong\ncompressed data relation guided by the less compressed one, we apply video\nlevel contrastive learning for weak compressed data, which forces the model to\nproduce similar representations within the same video and far from the negative\nsamples. The experiment results show that the proposed algorithm could boost\nperformance for strong compressed data while improving the accuracy rate when\ndetecting the clean data.",
      "categories": [
        "cs.CV"
      ],
      "published": "2023-02-13T08:34:28+00:00",
      "url": "http://arxiv.org/pdf/2302.06183v1",
      "resource_uri": "arxiv://2302.06183v1"
    },
    {
      "id": "1307.6923v1",
      "title": "A Deterministic Construction of Projection matrix for Adaptive Trajectory Compression",
      "authors": [
        "Rajib Rana",
        "Mingrui Yang",
        "Tim Wark",
        "Chun Tung Chou",
        "Wen Hu"
      ],
      "abstract": "Compressive Sensing, which offers exact reconstruction of sparse signal from\na small number of measurements, has tremendous potential for trajectory\ncompression. In order to optimize the compression, trajectory compression\nalgorithms need to adapt compression ratio subject to the compressibility of\nthe trajectory. Intuitively, the trajectory of an object moving in starlight\nroad is more compressible compared to the trajectory of a object moving in\nwinding roads, therefore, higher compression is achievable in the former case\ncompared to the later. We propose an in-situ compression technique underpinning\nthe support vector regression theory, which accurately predicts the\ncompressibility of a trajectory given the mean speed of the object and then\napply compressive sensing to adapt the compression to the compressibility of\nthe trajectory. The conventional encoding and decoding process of compressive\nsensing uses predefined dictionary and measurement (or projection) matrix\npairs. However, the selection of an optimal pair is nontrivial and exhaustive,\nand random selection of a pair does not guarantee the best compression\nperformance. In this paper, we propose a deterministic and data driven\nconstruction for the projection matrix which is obtained by applying singular\nvalue decomposition to a sparsifying dictionary learned from the dataset. We\nanalyze case studies of pedestrian and animal trajectory datasets including GPS\ntrajectory data from 127 subjects. The experimental results suggest that the\nproposed adaptive compression algorithm, incorporating the deterministic\nconstruction of projection matrix, offers significantly better compression\nperformance compared to the state-of-the-art alternatives.",
      "categories": [
        "cs.IT",
        "math.IT"
      ],
      "published": "2013-07-26T04:59:26+00:00",
      "url": "http://arxiv.org/pdf/1307.6923v1",
      "resource_uri": "arxiv://1307.6923v1"
    },
    {
      "id": "2101.10096v1",
      "title": "With Measured Words: Simple Sentence Selection for Black-Box Optimization of Sentence Compression Algorithms",
      "authors": [
        "Yotam Shichel",
        "Meir Kalech",
        "Oren Tsur"
      ],
      "abstract": "Sentence Compression is the task of generating a shorter, yet grammatical\nversion of a given sentence, preserving the essence of the original sentence.\nThis paper proposes a Black-Box Optimizer for Compression (B-BOC): given a\nblack-box compression algorithm and assuming not all sentences need be\ncompressed -- find the best candidates for compression in order to maximize\nboth compression rate and quality. Given a required compression ratio, we\nconsider two scenarios: (i) single-sentence compression, and (ii)\nsentences-sequence compression. In the first scenario, our optimizer is trained\nto predict how well each sentence could be compressed while meeting the\nspecified ratio requirement. In the latter, the desired compression ratio is\napplied to a sequence of sentences (e.g., a paragraph) as a whole, rather than\non each individual sentence. To achieve that, we use B-BOC to assign an optimal\ncompression ratio to each sentence, then cast it as a Knapsack problem, which\nwe solve using bounded dynamic programming. We evaluate B-BOC on both scenarios\non three datasets, demonstrating that our optimizer improves both accuracy and\nRouge-F1-score compared to direct application of other compression algorithms.",
      "categories": [
        "cs.CL"
      ],
      "published": "2021-01-25T14:00:56+00:00",
      "url": "http://arxiv.org/pdf/2101.10096v1",
      "resource_uri": "arxiv://2101.10096v1"
    },
    {
      "id": "1012.0955v1",
      "title": "Compressive Sensing Over Networks",
      "authors": [
        "Soheil Feizi",
        "Muriel Medard",
        "Michelle Effros"
      ],
      "abstract": "In this paper, we demonstrate some applications of compressive sensing over\nnetworks. We make a connection between compressive sensing and traditional\ninformation theoretic techniques in source coding and channel coding. Our\nresults provide an explicit trade-off between the rate and the decoding\ncomplexity. The key difference of compressive sensing and traditional\ninformation theoretic approaches is at their decoding side. Although optimal\ndecoders to recover the original signal, compressed by source coding have high\ncomplexity, the compressive sensing decoder is a linear or convex optimization.\nFirst, we investigate applications of compressive sensing on distributed\ncompression of correlated sources. Here, by using compressive sensing, we\npropose a compression scheme for a family of correlated sources with a\nmodularized decoder, providing a trade-off between the compression rate and the\ndecoding complexity. We call this scheme Sparse Distributed Compression. We use\nthis compression scheme for a general multicast network with correlated\nsources. Here, we first decode some of the sources by a network decoding\ntechnique and then, we use a compressive sensing decoder to obtain the whole\nsources. Then, we investigate applications of compressive sensing on channel\ncoding. We propose a coding scheme that combines compressive sensing and random\nchannel coding for a high-SNR point-to-point Gaussian channel. We call this\nscheme Sparse Channel Coding. We propose a modularized decoder providing a\ntrade-off between the capacity loss and the decoding complexity. At the\nreceiver side, first, we use a compressive sensing decoder on a noisy signal to\nobtain a noisy estimate of the original signal and then, we apply a traditional\nchannel coding decoder to find the original signal.",
      "categories": [
        "cs.IT",
        "math.IT"
      ],
      "published": "2010-12-04T22:54:18+00:00",
      "url": "http://arxiv.org/pdf/1012.0955v1",
      "resource_uri": "arxiv://1012.0955v1"
    },
    {
      "id": "1006.1193v1",
      "title": "Genbit Compress Tool(GBC): A Java-Based Tool to Compress DNA Sequences and Compute Compression Ratio(bits/base) of Genomes",
      "authors": [
        "P. Raja Rajeswari",
        "Allam Apparo",
        "V. K. Kumar"
      ],
      "abstract": "We present a Compression Tool, \"GenBit Compress\", for genetic sequences based\non our new proposed \"GenBit Compress Algorithm\". Our Tool achieves the best\ncompression ratios for Entire Genome (DNA sequences) . Significantly better\ncompression results show that GenBit compress algorithm is the best among the\nremaining Genome compression algorithms for non-repetitive DNA sequences in\nGenomes. The standard Compression algorithms such as gzip or compress cannot\ncompress DNA sequences but only expand them in size. In this paper we consider\nthe problem of DNA compression. It is well known that one of the main features\nof DNA Sequences is that they contain substrings which are duplicated except\nfor a few random Mutations. For this reason most DNA compressors work by\nsearching and encoding approximate repeats. We depart from this strategy by\nsearching and encoding only exact repeats. our proposed algorithm achieves the\nbest compression ratio for DNA sequences for larger genome. As long as 8 lakh\ncharacters can be given as input While achieving the best compression ratios\nfor DNA sequences, our new GenBit Compress program significantly improves the\nrunning time of all previous DNA compressors. Assigning binary bits for\nfragments of DNA sequence is also a unique concept introduced in this program\nfor the first time in DNA compression.",
      "categories": [
        "cs.MS"
      ],
      "published": "2010-06-07T07:37:49+00:00",
      "url": "http://arxiv.org/pdf/1006.1193v1",
      "resource_uri": "arxiv://1006.1193v1"
    }
  ]
}