{"status": "success", "message": "Paper already available", "resource_uri": "file:///app/papers/2412.04948v1.md", "pdf_uri": "https://arxiv.org/pdf/2412.04948v1.pdf", "content": "## **KaLM: Knowledge-aligned Autoregressive Language Modeling via** **Dual-view Knowledge Graph Contrastive Learning**\n\n**Peng Yu** **[1]** **, Cheng Deng** **[1]** **, Beiya Dai** **[1]** **, Xinbing Wang** **[1]** **, Ying Wen** **[1]** [*]\n\n1 Shanghai Jiao Tong University\n{pursuit_yp, davendw, beiya_dai, xwang8, ying.wen}@sjtu.edu.cn\n\n\n\n**Abstract**\n\n\nAutoregressive large language models (LLMs)\npre-trained by next token prediction are inherently proficient in generative tasks. However,\ntheir performance on knowledge-driven tasks\nsuch as factual knowledge querying remains unsatisfactory. Knowledge graphs (KGs), as highquality structured knowledge bases, can provide reliable knowledge for LLMs, potentially\ncompensating for their knowledge deficiencies.\nAligning LLMs with explicit, structured knowledge from KGs has been a challenge; previous attempts either failed to effectively align\nknowledge representations or compromised the\ngenerative capabilities of LLMs, leading to lessthan-optimal outcomes. This paper proposes\n**KaLM**, a _Knowledge-aligned Language Mod-_\n_eling_ approach, which fine-tunes autoregressive LLMs to align with KG knowledge via the\njoint objective of explicit knowledge alignment\nand implicit knowledge alignment. The explicit knowledge alignment objective aims to directly optimize the knowledge representation of\nLLMs through dual-view knowledge graph contrastive learning. The implicit knowledge alignment objective focuses on incorporating textual patterns of knowledge into LLMs through\ntriple completion language modeling. Notably,\nour method achieves a significant performance\nboost in evaluations of knowledge-driven tasks,\nspecifically embedding-based knowledge graph\ncompletion and generation-based knowledge\ngraph question answering.\n\n\n**1** **Introduction**\n\n\nLarge language models (LLMs) like PaLM 2 (Anil\net al., 2023) and GPT-4 (Achiam et al., 2023) have\nrecently made remarkable advancements in a wide\nrange of natural language processing tasks (Li et al.,\n2022; Su et al., 2019). However, LLMs still face\nchallenges in tasks requiring factual or domainspecific knowledge, resulting in unsatisfactory performance in knowledge-driven tasks. From the\n\n\n  - Ying Wen is the corresponding author.\n\n\n\nperspective of knowledge representation, LLMs\nserve as parametric knowledge bases, providing implicit, non-deterministic knowledge, while knowledge graphs (KGs) function as structured knowledge bases, offering explicit, deterministic knowledge. KGs, commonly organized as factual knowledge triples describing relations between entities,\ncan serve as a reliable knowledge source for LLMs.\nAligning LLMs with KG knowledge can enhance\nthe knowledge reasoning capabilities of LLMs and\nimprove their performance on knowledge-driven\ntasks, such as knowledge graph completion (KGC)\nand knowledge graph question answering (KGQA).\nAutoregressive LLMs pre-trained through next\ntoken prediction tasks often exhibit limitations in\nknowledge representation, leading to embeddings\nthat lack diversity and specificity. This limitation\nbecomes evident in tasks that demand distinctive\n\nsentence embeddings, such as dense retrieval and\nsemantic search (Muennighoff, 2022; Ma et al.,\n2023). As demonstrated in Figure 1(a), the representations generated by LLMs tend to be overly\nhomogeneous across different pieces of knowledge,\nundermining their effectiveness in applications requiring fine-grained semantic distinctions.\nThe concept of explicit knowledge alignment\nis introduced to directly optimize the knowledge\nrepresentation within language models by devising\ndirect knowledge training objectives. This strategy\nemerges in response to the observed degradation\nin knowledge representation within autoencoderbased pre-trained language models (PLMs), a phenomenon termed _representation anisotropy_ (Ethayarajh, 2019). This issue is characterized by the\nclustering of learned token and sentence embeddings within a constrained area of the representation space, leading to a lack of distributional uniformity (Li et al., 2020). While previous efforts\nto address representation anisotropy have largely\nconcentrated on promoting uniformity among token representations, they often overlook the critical\n\n\n\n1\n\n\n(a) LLaMA\n\n\n\n(b) KaLM\n\n\n\nFigure 1: Similarity matrix of knowledge representations of (a) Llama-2-7B (Touvron et al., 2023) and (b) KaLM.\nThe values denote the cosine similarity between the head-relation and tail embedding. The diagonal elements\nrepresent positive <head-relation, tail> pairs from the same KG triple, which should maintain high similarity (darker\ncolor); off-diagonal elements represent negative <head-relation, tail> pairs from different KG triples, which should\nhave lower similarity (lighter color). In an ideal setting, knowledge representations should be able to distinguish\nbetween different triples, while maintaining alignment and uniformity of the representation, as shown in Figure 1(b).\n\n\n\nalignment of similar sentence representations (Su\net al., 2021; Li et al., 2020; Su et al., 2022). More\nrecent works advocate for integrating KG triples\nand using knowledge graph embedding losses to\nfine-tune PLMs, aiming to bolster their knowledge\nrepresentation abilities (Shen et al., 2022; Wang\net al., 2022b). Nonetheless, such approaches may\nlimit themselves to optimizing at the token level or\nreduce the model to a mere text encoder, thereby\ndiminishing its inherent generative capabilities.\n\n\nConversely, implicit knowledge alignment leverages the pre-training or fine-tuning of language\nmodels with external knowledge sources, employing the vanilla language modeling objective or its\nvariations. This approach predominantly preserves\nthe next token prediction framework, essentially retaining the native text generation prowess of LLMs.\nIn the realm of implicit knowledge alignment, the\nprevalent practice involves the fine-tuning of LLMs\nwith KG triples and their textual descriptions, as\nopposed to directly altering the hidden knowledge representations (Chen et al., 2022; Yao et al.,\n2023). Nevertheless, the efficacy of these methods on knowledge graph completion tasks remains\nsubstantially inferior when compared to strategies\nthat directly fine-tune knowledge representations\n(Wang et al., 2022b,a). Intriguing findings from\n(Fu et al., 2023) reveal that fine-tuning PLMs with\nrandomly unaligned KG triples can achieve performance on par with that obtained through fine\n\n\ntuning with aligned triples in various tasks, including named entity recognition and relation classification. Their findings suggest that the hidden states\nof entities, whether infused with aligned or random\nknowledge, exhibit remarkable similarity. Consequently, existing implicit alignment methods fail to\neffectively utilize the injected knowledge or accurately discern the connection between newly introduced knowledge and the model\u2019s inherent knowledge, culminating in suboptimal performance.\n\n\nIn this paper, we propose **KaLM**, a _Knowledge-_\n_aligned Language Modeling_ approach for aligning\nLLMs with KG knowledge. Specifically, we use\nKG triples and their textual descriptions to finetune LLMs via the joint objective of _explicit knowl-_\n_edge alignment_ and _implicit knowledge alignment._\n\n\nThe explicit knowledge alignment objective aims\nto directly optimize the hidden representations of\nknowledge in LLMs through _dual-view knowledge_\n_graph contrastive learning_ . We theoretically prove\nand empirically show that this objective can facilitate knowledge representation alignment and alleviate representation anisotropy. For KG triples, we\nconsider tail entity description and the concatenation of head entity description and relation description as two distinct views of the same knowledge.\n_The key insight is that: (1) representations of two_\n_different views of the same knowledge (i.e., from_\n_the same triple) should be pulled together, while (2)_\n_representations of different knowledge (i.e., from_\n\n\n\n2\n\n\n_different triples) should be pushed apart._ The first\nterm encourages semantically similar knowledge to\nremain close in the representation space, promoting\nknowledge representation alignment. The second\nterm forces dissimilar knowledge to be as far apart\nas possible in the vector space, improving knowledge representation uniformity and mitigating representation anisotropy. As shown in Figure 1(b),\nour method can obtain the ideal knowledge representations that are both aligned and uniform.\nThe implicit knowledge alignment objective focuses on incorporating textual patterns of knowledge into LLMs through _triple completion lan-_\n_guage modeling_, which can maintain the generative capability of LLMs and boost performance on\nknowledge inference tasks. We constructed a triple\ncompletion dataset based on the KG triples to finetune LLMs, improving their instruction-following\nability and facilitating implicit knowledge alignment. We also show the implicit knowledge alignment objective can further boost knowledge representation performance. This confirms that both explicit alignment and implicit alignment are crucial\nfor knowledge alignment, as they both essentially\nrequire a deep understanding of knowledge.\n\nOur contributions are summarized as follows:\n\n\n  - We introduce **KaLM**, a _knowledge-aligned_\n_language modeling_ approach that aligns autoregressive LLMs with KG knowledge via\nthe joint objective of _explicit knowledge align-_\n_ment_ and _implicit knowledge alignment._\n\n\n  - We _theoretically prove and empirically demon-_\n_strate_ that the explicit knowledge alignment\nobjective achieved through dual-view knowledge graph contrastive learning can facilitate\nknowledge representation alignment and alleviate the issue of representation anisotropy.\n\n\n  - The experimental results on knowledge-driven\ntasks demonstrate the effectiveness of _KaLM_ .\n\nIn the embedding-based KGC task, KaLM significantly improves Mean Rank and Hit@10\nmetrics compared to previous state-of-the-art\nmethods. In the generation-based KGQA task,\nKaLM achieves a notable improvement in answering accuracy compared to the base LLM.\n\n\n**2** **Related Work**\n\n\nOur work is closely related to Knowledge Enhancement for LLMs and Representation Anisotropy of\n\n\n\nLanguage Models. A more detailed review of related work can be found in Appendix A.\n**Knowledge Enhancement for LLMs** Knowledge enhancement aims to incorporate factual and\ndomain-specific knowledge into LLMs to address\ntheir knowledge deficiencies. This can be divided\ninto retrieval-based augmentation and trainingbased integration. _Retrieval-based knowledge aug-_\n_mentation_ methods leverage external retrieval modules to provide additional knowledge, aiming to\nimprove the knowledge reasoning capability of\nLLMs (Sun et al., 2023; Jiang et al., 2023). However, this approach may lead to knowledge conflicts\n(Feng et al., 2023), where knowledge in LLMs\nand knowledge in the retrieved documents are inconsistent or the retrieved multiple documents are\ncontradictory. _Training-based knowledge integra-_\n_tion_ methods involve using KG triple descriptions\nto pre-train or fine-tune LLMs, aiming to achieve\nknowledge alignment. These methods can be divided into explicit alignment (Wang et al., 2021b;\nYasunaga et al., 2022) and implicit alignment (Yao\net al., 2023; Zhang et al., 2023) based on whether\nthey directly optimize the knowledge representation. Nevertheless, prior methods have either sacrificed the generative capability or lacked effective\nrepresentation alignment. Our approach enhances\nthe knowledge of LLMs via a unique joint objective\nof explicit alignment and implicit alignment, improving the quality of knowledge representations\nand generative knowledge reasoning capabilities.\n**Representation Anisotropy of Language Models**\nPLMs have long been plagued by representation\nanisotropy (Ethayarajh, 2019), where the learned\ntoken and sentence embeddings are confined to a\nnarrow cone within the entire representation space.\nThe issue of representation anisotropy not only results in model degradation (Su et al., 2022) but\nalso leads to poor performance on discriminative\ntasks. Previous work on alleviating representation\nanisotropy has mainly focused on post-processing\ntechniques such as normalizing flows (Li et al.,\n2020) or whitening operations (Su et al., 2021). Su\net al. (2022) propose a contrastive training objective\nto encourage learning isotropic token representations. However, these methods mainly improve the\nisotropy of token representations without enhancing the discriminability of sentence representations.\nOur method improves the token-level and sentencelevel representation anisotropy of LLMs through\ndual-view knowledge graph contrastive learning,\nand it has rigorous theoretical guarantees.\n\n\n\n3\n\n\n**3** **Knowledge-aligned Autoregressive**\n**Language Modeling**\n\n\nIn this section, we introduce **KaLM**, a _Knowledge-_\n_aligned Language Modeling_ approach for aligning\nLLMs with KG knowledge via the joint objective\nof _explicit knowledge alignment_ and _implicit knowl-_\n_edge alignment_ . The overview is shown in Figure 2.\n\n\n**3.1** **Notations and Preliminaries**\n\n\nA KG _G_ stores factual knowledge, denoted as _G_ =\n( _E, R, T, D_ ) . _E_ and _R_ are the set of entities and\nrelations, respectively. _D_ is the description set of\nall entities and relations. _D_ _e_ and _D_ _r_ are the textual\ndescription of entity _e_ and relation _r_, respectively.\n_T_ = _{_ ( _h, r, t_ ) _|h, t \u2208E, r \u2208R}_ is the triple set. A\ntriple ( _h, r, t_ ) depicts the fact that there is a relation\n_r_ between the head entity _h_ and the tail entity _t_ .\n\n\n**3.2** **Explicit Knowledge Alignment**\n\n\nFor KG triples, the textual description of the tail\nentity and the concatenation of the textual descriptions of the head entity and relation can be seen as\ntwo distinct views of the same knowledge. This\ninspires _KaLM_ to align representations of two distinct views of the same knowledge (i.e., from the\nsame triple), while separating representations of\ndifferent knowledge (i.e., from different triples).\nThe LLM, denoted as _E_ _LLM_, is fine-tuned with\nthe _dual-view knowledge graph contrastive learn-_\n_ing_ loss. The training corpus contains paired textual\ndescriptions, _{_ ( _D_ _hr_ _, D_ _t_ ) _}_ _[N]_ _i_ =1 [, where] _[ D]_ _[t]_ [ is the tail]\nentity description, and _D_ _hr_ is the concatenation of\nthe head entity description and relation description.\nGiven a training pair ( _D_ _hr_ _, D_ _t_ ), the same _E_ _LLM_\nis used to compute the embeddings of _D_ _hr_ and _D_ _t_\nindependently. Moreover, we prepend the [bos] token to the beginning and append the [eos] token to\nthe end of the textual description. The augmented\ninput is fed into _E_ _LLM_, and the hidden representation corresponding to the [eos] token from the last\nlayer is used as the final embedding of the input.\n\n\n_e_ _hr_ = _E_ _LLM_ ([bos] _hr_ _\u2295D_ _hr_ _\u2295_ [eos] _hr_ ) _,_\n\n\n_e_ _t_ = _E_ _LLM_ ([bos] _t_ _\u2295D_ _t_ _\u2295_ [eos] _t_ ) _,_\n\n\nwhere _\u2295_ is the operation to concatenate two strings\nand _D_ _hr_ = _D_ _h_ _\u2295D_ _r_ . For stable training, we adopt\n\u201c[\u201d as [bos] _hr_ and \u201c]\u201d as [eos] _hr_, while using \u201c{\u201d\nas [bos] _t_ and \u201c}\u201d as [eos] _t_ .\nWe utilize the knowledge graph contrastive learning loss to directly optimize the knowledge representation of the LLM by _encouraging semantically_\n\n\n\nwhere _M_ is the instruction-tuning batch size.\n\n\n\n_similar knowledge to stay close in the representa-_\n_tion space and pushing dissimilar knowledge to be_\n_far apart in the representation space_ . More specifically, we apply the InfoNCE loss with an additive\nmargin over the in-batch negatives to fine-tune the\nmodel. The row-direction loss _\u2113_ _r_ is as follows for\na given positive pair, and the column-direction loss\n_\u2113_ _c_ is defined similarly (see Appendix C.2).\n\n\n_e_ [(] _[\u03d5]_ [(] _[e]_ _[hr]_ _[,e]_ _[t]_ [)] _[\u2212][\u03b3]_ [)] _[/\u03c4]_\n_\u2113_ _r_ = _\u2212_ log _e_ [(] _[\u03d5]_ [(] _[e]_ _[hr]_ _[,e]_ _[t]_ [)] _[\u2212][\u03b3]_ [)] _[/\u03c4]_ + ~~[\ufffd]~~ _[N]_ _i_ =1 _[e]_ _\u03d5_ ( _e_ _hr_ _,e_ _t\u2032i_ [)] _[/\u03c4]_ _[,]_\n\n(1)\n\n\nwhere _N_ is the negative batch size, _\u03c4_ is the trainable temperature that controls the strength of penalties on hard negative samples, _\u03d5_ is the cosine similarity function that measures the plausibility of a\ntriple, and _\u03b3_ is the additive margin that encourages\nincreasing the similarity score of positive pairs.\nThe training objective for **exp** licit knowledge\nalignment is the sum of the _\u2113_ _r_ and the _\u2113_ _c_ losses:\n\n\n\n_L_ _exp_ = _N_ [1]\n\n\n\n\ufffd ( _\u2113_ _r_ + _\u2113_ _c_ ) _/_ 2 _._ (2)\n\n( _D_ _hr_ _,D_ _t_ )\n\n\n\n**3.3** **Implicit Knowledge Alignment**\n\n\nThe implicit knowledge alignment objective focuses on incorporating textual patterns of knowledge into the LLM to prevent catastrophic forgetting of previous knowledge and maintain its generative capability. We constructed an instructiontuning dataset based on the KG triple descriptions\nto fine-tune the model through _triple completion_\n_language modeling_ . We also show that the implicit\nknowledge alignment objective can bring performance boosts on knowledge representation evaluations. This indicates that explicit alignment and\nimplicit alignment are both imperative for effective\nknowledge alignment, as they both essentially necessitate a profound understanding of knowledge.\nWe follow the recipe of Stanford Alpaca (Taori\net al., 2023) and use the provided template to construct the instruction-tuning dataset. The instruction passed to the template, abbreviated as inst,\nis: _\u201cGiven the head entity and relation, write a tail_\n_entity that completes the triple\u201d._ The input and\noutput are _D_ _hr_ and _D_ _t_, respectively. The training\nobjective for **imp** licit knowledge alignment is:\n\n\n\n_L_ _imp_ = [1]\n\n_M_\n\n\n\n\ufffd _\u2212_ log _P_ ( _D_ _t_ _|_ inst _, D_ _hr_ ) _,_ (3)\n\n( _D_ _hr_ _,D_ _t_ )\n\n\n\n4\n\n\nFigure 2: The overall framework of **KaLM** . **Up** : The explicit knowledge alignment objective ( _L_ _exp_ ) aims to directly\noptimize the knowledge representation of LLMs via dual-view knowledge graph contrastive learning. **Down** : The\nimplicit knowledge alignment objective ( _L_ _imp_ ) focuses on incorporating textual patterns of knowledge into LLMs\nvia triple completion language modeling. The final training objective is the weighted average of _L_ _exp_ and _L_ _imp_ .\n\n\n\n**3.4** **Knowledge-aligned Language Modeling**\n\n\nThe ultimate training objective of our proposed\n**KaLM** is the weighted average of _L_ _exp_ and _L_ _imp_ :\n\n\n_L_ _KaLM_ = _L_ _exp_ + _\u03bb \u00b7 L_ _imp_ _,_ (4)\n\n\nwhere _\u03bb_ is a hyperparameter that adjusts the relative\nweight between them. Notably, this formulation\nallows us to use different batch sizes for explicit\nknowledge alignment ( _N_ ) and implicit knowledge\nalignment ( _M_ ). Previous work has shown that a\nsufficiently large batch size is key to the success\nof contrastive representation learning (Chen et al.,\n2020). With Equation 4, we can significantly increase the explicit knowledge alignment batch size\nwhile keeping the implicit knowledge alignment\nbatch size fixed to save computational resources.\n\n\n**4** **Theoretical Analysis**\n\n\nWe theoretically prove that the explicit knowledge\nalignment objective implemented through dualview knowledge graph contrastive learning can facilitate knowledge representation alignment and\nalleviate the issue of representation anisotropy.\n\n\n**4.1** **Dual-view Contrastive Learning for**\n**Knowledge Representation Alignment**\n\n\nThe outstanding performance of contrastive representation learning has attracted researchers to analyze its underlying reasons for success from a theoretical perspective. Wang and Isola (2020) identify\n\n\n\nwhere _p_ pos denotes the distribution of positive pairs\n_{_ ( _D_ _hr_ _, D_ _t_ ) _}_ _[N]_ _i_ =1 [and] _[ p]_ [data] [ represents the data dis-]\ntribution of textual descriptions _{D_ _i_ _}_ _[N]_ _i_ =1 [.]\nSince the learned knowledge representations are\nL2-normalized, we have _\u03d5_ ( _e_ _hr_ _, e_ _t_ ) = _f_ ( _x_ ) _[\u22a4]_ _f_ ( _y_ ) .\nThe additive margin _\u03b3_ encourages the model to\nlearn more robust features without affecting the\nasymptotic analysis, thus we ignore it. For ease of\nanalysis, we reformulate the contrastive learning\n\n\n\nalignment and uniformity as two key properties of\ncontrastive learning and propose two quantifiable\nmetrics to measure the quality of representations.\nWe concentrate on understanding the dual-view\nknowledge graph contrastive learning loss from the\nknowledge alignment and uniformity perspective.\nTo simplify the notation, we use _f_ to denote _E_ _LLM_ .\n_Alignment_ computes the expected distance between positive pairs and encourages the learned\nrepresentations for positive pairs to be similar. _Uni-_\n_formity_ evaluates the even distribution of representations and encourages the separation of features\nfrom randomly selected negative samples.\n\n\n_\u2113_ align ( _f_ ; _\u03b1_ ) \u225c E [ _\u2225f_ ( _D_ _hr_ ) _\u2212_ _f_ ( _D_ _t_ ) _\u2225_ 2 _[\u03b1]_ []] _[,]_\n( _D_ _hr_ _,D_ _t_ ) _\u223cp_ pos\n\n\n\n_\u2113_ uniform ( _f_ ; _t_ ) \u225c log E\n_i.i.d._\n_D_ _i_ _,D_ _j_ _\u223c_ _p_ data\n\n\n\n_e_ _[\u2212][t][\u2225][f]_ [(] _[D]_ _[i]_ [)] _[\u2212][f]_ [(] _[D]_ _[j]_ [)] _[\u2225]_ 2 [2] _,_\n\ufffd \ufffd\n\n\n\n5\n\n\nobjective of Equation 1 and 2 as follows:\n\n\n_\u0338_\n\n\n\n**Theorem 2** (Alleviation of Anisotropy) **.** _When_\n_p_ _data_ _is uniform over finite samples_ _{D_ _i_ _}_ _[N]_ _i_ =1 _[, the]_\n_second term of Equation 6 is the upper bound of_\n_the sentence-level anisotropy of {D_ _i_ _}_ _[N]_ _i_ =1 _[, i.e.,]_\n\n\n_\u0338_\n\n\n\n_L_ exp ( _f_ ; _\u03c4, N_ ) \u225c E\n( _D_ _hr_ _,D_ _t_ ) _\u223cp_ pos\n_i.i.d._\n_{D_ _t_ _[\u2032]_ _i_ _[}]_ _[N]_ _i_ =1 _\u223c_ _p_ data\n\uf8ee\n\n_[\u22a4]_\n\n\n_\u0338_\n\n\n\n1\n\n_\u00b7_\n\n_\u2265_ _[N]_ _\u03c4N_ _[ \u2212]_ [1] anisotropy _{D}_ + _\u03c4N_ _[.]_\n\n\n_\u0338_\n\n\n\n(8)\n\n\n_\u0338_\n\n\n\n\ufffd _e_ _[f]_ [(] _[D]_ _i_ _[\u2212]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[i]_ [)] _[/\u03c4]_ [\ufffd\ufffd]\n\n\n_\u0338_\n\n\n\n\ufffd\n\n\n_\u0338_\n\n\n\nlog E\n_D_ _i_ _[\u2212]_ _[\u223c][p]_ _[data]_\n\n\n_\u0338_\n\n\n\n_e_ _[f]_ [(] _[D]_ _[hr]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[t]_ [)] _[/\u03c4]_\n\n_\u2212_\nlog\n\n_N_\n_e_ _[f]_ [(] _[D]_ _[hr]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[t]_ [)] _[/\u03c4]_ + \ufffd _e_ _[f]_ [(] _[D]_\n\n\uf8ef\uf8ef\uf8ef\uf8f0 _i_ =1\n\n\n_\u0338_\n\n\n\nE\n_D_ _i_ _\u223cp_ _data_\n\n\n_\u0338_\n\n\n\n_N_\n_e_ _[f]_ [(] _[D]_ _[hr]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[t]_ [)] _[/\u03c4]_ + \ufffd\n\n\n_\u0338_\n\n\n\n\ufffd _e_ _[f]_ [(] _[D]_ _[hr]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[t]_ _i_ _[\u2032]_ [)] _[/\u03c4]_\n\n_i_ =1\n\n\n_\u0338_\n\n\n\n\uf8f9\n\n\n_,_\n\uf8fa\uf8fa\uf8fa\uf8fb\n\n\n_\u0338_\n\n\n\n(5)\n\n\nFollowing Wang and Isola (2020), we analyze\nthe asymptotics of the objective in Equation 5.\n\n\n**Theorem 1** (Asymptotics of _L_ exp ) **.** _For tempera-_\n_ture_ _\u03c4 >_ 0 _, as the number of negative samples_\n_N \u2192\u221e_ _, the normalized dual-view knowledge_\n_graph contrastive loss in Equation 5 converges to_\n\n\nlim\n_N\u2192\u221e_ _[L]_ [exp] [(] _[f]_ [;] _[ \u03c4,][ N]_ [)] _[ \u2212]_ [log] _[ N]_ [ =]\n\n\n_\u0338_\n\n\n\n_\u2212_ [1] E\n\n_\u03c4_ ( _D_ _hr_ _,D_ _t_ ) _\u223cp_ pos\n\n\n_\u0338_\n\n\n\n_f_ ( _D_ _hr_ ) _[\u22a4]_ _f_ ( _D_ _t_ )\n\ufffd \ufffd\n\n\n_\u0338_\n\n\n\n_We have the following result: By optimizing the_\n_second term of Equation 6, we essentially minimize_\n_the upper bound of the sentence-level anisotropy_\n_of corpus_ _{D_ _i_ _}_ _[N]_ _i_ =1 _[, thereby directly alleviating the]_\n_representation anisotropy problem._\n\n\n_Proof._ See Appendix B.2.\n\n\n**5** **Experiments**\n\n\nIn this section, we assess the effectiveness of KaLM\nin knowledge alignment. The experimental setup\nis outlined in 5.1. In 5.2 and 5.3, we present results\non knowledge graph completion (KGC) and knowledge graph question answering (KGQA). In 5.4, we\nprovide further analysis of knowledge representation and present case studies of KGQA generations.\n\n\n**5.1** **Experimental Setup**\n\n\n**Datasets.** We use WN18RR (Dettmers et al., 2018)\nand FB15k-237 (Toutanova and Chen, 2015) as the\nKGs for knowledge alignment training. WN18RR\nand FB15k-237 are derived from WordNet and\n\nFreebase, respectively (Bordes et al., 2013). We use\nthe information provided by KG-BERT (Yao et al.,\n2019) for textual descriptions. Following Wang\net al. (2022a), we add an inverse triple ( _t, r_ _[\u2212]_ [1] _, h_ )\nfor each triple ( _h, r, t_ ) in the triple set, where _r_ _[\u2212]_ [1]\n\nis the inverse relation of the original relation _r_ .\n**Model Training.** We choose Llama-2-7B, Llama3-8B, and Mistral-7B as base LLMs and fine-tune\nthem through the joint objective of explicit knowledge alignment and implicit knowledge alignment.\nTo save computational resources for parameterefficient fine-tuning, we use LoRA (Hu et al., 2021)\nto fine-tune the feed-forward network of the model.\n**Evaluation Details.** Experiments mainly focus on\ntwo aspects: knowledge representation assessment\nand knowledge inference evaluation. For _knowl-_\n_edge representation assessment_, we evaluate the\nembedding-based KGC task and illustrate the alle\n_\u0338_ viation of representation anisotropy. We report five\n\nautomated metrics: Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hit@ _k_ ( _k \u2208{_ 1 _,_ 3 _,_ 10 _}_ ).\n\n\n\n\ufffd _e_ _[f]_ [(] _[D]_ _i_ _[\u2212]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[i]_ [)] _[/\u03c4]_ [\ufffd\ufffd] _._\n\n\n_\u0338_\n\n\n\n+ E\n_D_ _i_ _\u223cp_ _data_\n\n\n_\u0338_\n\n\n\n\ufffd\n\n\n_\u0338_\n\n\n\nlog E\n_D_ _i_ _[\u2212]_ _[\u223c][p]_ [data]\n\n\n_\u0338_\n\n\n\n(6)\n\n\n_We have the following conclusions:_\n\n\n_1._ _By pulling together the representations of two_\n_different views of the same knowledge, the first_\n_term of Equation 6 is minimized, and the en-_\n_coder E_ _LLM_ _is perfectly knowledge-aligned._\n\n\n_2._ _Assuming the perfect uniform knowledge en-_\n_coder_ _E_ _LLM_ _exists, it precisely minimizes the_\n_second term of Equation 6 by pushing away_\n_the representations of different knowledge._\n\n\n_Proof._ See Appendix B.1.\n\n\n**4.2** **Alleviation of Representation Anisotropy**\n\n\nWe then prove that the dual-view knowledge graph\ncontrastive learning objective can directly alleviate\nrepresentation anisotropy and improve the discriminability of knowledge representations.\nLet **E** be the sentence embedding matrix of\n_{D_ _i_ _}_ _[N]_ _i_ =1 [, where the] _[ i]_ [-th row of] **[ E]** [ is] _[ e]_ _[i]_ [. Following]\nEthayarajh (2019), the sentence-level representation anisotropy value of _{D_ _i_ _}_ _[N]_ _i_ =1 [is defined as:]\n\n\n_\u0338_\n\n\n\n_N_\n\ufffd _e_ _[\u22a4]_ _i_ _[e]_ _[j]_ _[.]_\n\n_j_ =1 _,j_ = _\u0338_ _i_\n\n\n\n1\nanisotropy _{D}_ =\n_N_ ( _N \u2212_ 1)\n\n_\u0338_\n\n\n\n_N_\n\ufffd\n\n\n_i_ =1 _\u0338_\n\n\n\n_\u0338_\n\n(7)\nWe can further derive the following theorem.\n\n\n\n_\u0338_\n\n\n6\n\n\nTable 1: Embedding-based KGC results on WN18RR and FB15k-237. Baseline results are from their papers, with\n\u201c-\u201d indicating a missing result. The best and second-best results are marked by **bold** and underline, respectively.\n\n|Method|WN18RR|FB15k-237|\n|---|---|---|\n|**Method**|**MR**<br>**MRR**<br>**H@1**<br>**H@3**<br>**H@10**|**MR**<br>**MRR**<br>**H@1**<br>**H@3**<br>**H@10**|\n\n\n\n_structure-based methods_\n\nTransE 2300 0.243 0.043 0.441 0.532 323 0.279 0.198 0.376 0.441\n\nDistMult 7000 0.444 0.412 0.470 0.504 512 0.281 0.199 0.301 0.446\n\nRotatE 3340 0.476 0.428 0.492 0.571 177 0.338 0.241 0.375 0.533\n\n_description-based methods (autoencoder PLMs)_\n\nStAR 51 0.401 0.243 0.491 0.709 117 0.296 0.205 0.322 0.482\n\nC-LMKE 72 0.598 0.480 0.675 0.806 183 **0.404** **0.324** **0.439** **0.556**\n\nSimKGC - **0.671** **0.587** **0.731** 0.817 - 0.333 0.246 0.362 0.510\n\n_description-based methods (autoregressive LLMs)_\n\nLlama-2-7B 15969 0.010 0.004 0.010 0.020 5359 0.006 0.002 0.004 0.012\n\n**Llama2-7B** _KaLM_ **19** 0.556 0.409 0.656 0.851 **114** 0.299 0.204 0.325 0.502\n**Llama3-8B** _KaLM_ 23 0.588 0.446 0.676 0.860 121 0.308 0.212 0.337 0.509\n**Mistral-7B** _KaLM_ 20 0.612 0.484 0.702 **0.869** 116 0.317 0.225 0.351 0.518\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|60|Col2|Col3|Col4|55.9 6|9 61.6|Col7|LL|LaMA|\n|---|---|---|---|---|---|---|---|---|\n|head<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>scores or accuracy<br>~~7.8~~||||~~55.9~~<br>|||K|aLM|\n|head<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>scores or accuracy<br>~~7.8~~|||||42.|3<br>42.0|3<br>42.0||\n|head<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>scores or accuracy<br>~~7.8~~|||||||||\n|head<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>scores or accuracy<br>~~7.8~~|||~~28.5~~||||||\n|head<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>scores or accuracy<br>~~7.8~~|~~7.~~|11<br>~~16.2~~|.6|12.1|||||\n|head<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>scores or accuracy<br>~~7.8~~|||3|.7||4.81<br>~~4~~|4.81<br>~~4~~|~~98~~|\n|head<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>scores or accuracy<br>~~7.8~~||pred<br>tail|pred<br>relati|n pred<br>triple cls<br>M|n pred<br>triple cls<br>M|LU<br>PPL|LU<br>PPL||\n\n\nFigure 3: Comparison of generative knowledge inference performance between Llama-2-7B and KaLM. _\u2191_\nmeans higher is better and _\u2193_ means lower is better.\n\n\nWe compare KaLM with structure- and descriptionbased methods. Structured-based methods include\n\nTransE (Bordes et al., 2013), DistMult (Yang et al.,\n2015), and RotatE (Sun et al., 2018). Descriptionbased methods include StAR (Wang et al., 2021a),\nC-LMKE (Wang et al., 2022b), and SimKGC\n(Wang et al., 2022a). For _knowledge inference eval-_\n_uation_, we evaluate the generation-based KGQA\ntask and analyze the PPL metric and MMLU score\n(Hendrycks et al., 2020). We report the prediction\naccuracy over entities, relations, and triples. We\nalso provide case studies of KGQA generations.\nAdditional experimental results and detailed ablation studies can be found in Appendix D and E.\n\n\n**5.2** **Knowledge Representation Assessment**\n\n\nThe embedding-based KGC results are shown in Table 1. The base LLM failed to finish this task, with\nall metrics lagging far behind. On the WN18RR\ndataset, our method surpasses prior methods by a\nsubstantial margin in terms of MR and Hit@10.\n\n\n\n(a) LLaMA (b) KaLM\n\n\nFigure 4: Similarity matrix on the Wikitext-103 test set.\nFrom top-left to bottom-right, element ( _i, j_ ) denotes the\ncosine similarity between the _i_ -th and the _j_ -th sentence.\n\n\nOther metrics fall slightly short of state-of-the-art\nmethods, yet remain competitive. The performance\nof KaLM on FB15k-237 is slightly inferior, but\nit still achieves the best MR. Previous descriptionbased methods generally perform poorly on FB15k237, possibly due to the absence of effective textual\ndescriptions. An example relation description from\nFB15k-237 is \u201c _/music/artist/origin_ \u201d, which is quite\nvague and abstract. SimKGC uses a large batch size\nthrough intricate negative sampling methods and incorporates neighbor description augmentation and\nneighbor-based re-ranking techniques. C-LMKE\nuses self-adversarial negative sampling and utilizes\nextra entity degree information. These tricks enable\nSimKGC and C-LMKE to achieve higher performance. _Using a larger batch size and more tech-_\n_niques can further improve other metrics of KaLM._\nOverall, the results reveal that KaLM notably enhances the quality of knowledge representation,\nbringing performance boosts in KGC tasks.\n\n\n\n7\n\n\nFigure 5: Case studies of Llama-2-7B and KaLM on KGQA tasks. Note that the head entity, relation, and tail entity\nare denoted by different colors. The mark indicates the correct answer, while signifies an incorrect answer.\n\n\n\n**5.3** **Knowledge Inference Evaluation**\n\n\nThe generation-based KGQA results are depicted\nin Figure 3. Llama-2-7B performs poorly in entity prediction and relation prediction. Our method\ndemonstrates a significant performance boost in all\ngeneration-based KGQA tasks, including head/tail\nentity prediction, relation prediction, and triple classification. Furthermore, despite a slight increase in\nperplexity (PPL) scores on Wikitext-103 (Merity\net al., 2016) test set, our method still shows competitive performance in the MMLU test. The results\ndemonstrate that KaLM achieves effective knowl\nedge alignment, bringing in significantly improved\nKGQA performance while preserving the original\ngenerative and knowledge inference capabilities.\n\n\n**5.4** **Visualization of Knowledge**\n**Representation and Case Studies**\n\n\n**We provide visualization results to illustrate**\n**knowledge representation improvements.** Figure 4 shows the sentence similarity matrix of\nLlama-2-7B and KaLM on Wikitext-103. The di\nagonal elements denote the similarity of the same\nsentence, so the values are always 1. From color\nintensity, it is evident that KaLM learns more discriminative sentence representations, while Llama2-7B assigns high similarity for arbitrary sentences.\nThe sentences are organized by celebrities and their\ncareers, thus there should also be a high similarity\nbetween adjacent sentences. This phenomenon is\nreflected in the similarity matrix of KaLM in Figure 4(b), manifested in the smaller matrices with\ndarker colors along the diagonal. _More concretely,_\n_numerical analysis shows that after training with_\n_our method, the sentence-level anisotropy value_\n_significantly decreased from 0.83 to 0.21._\n\n\n\n**We present KGQA generation cases to demon-**\n**strate knowledge inference enhancements.** Figure 5 illustrates concrete examples of KGQA generation results on the WN18RR dataset. We show\ncase the responses generated by Llama-2-7B and\nKaLM for four tasks involving head entity prediction, relation prediction, tail entity prediction, and\ntriple classification. The prompt templates for each\nsubtask are shown in the second column of Figure 5,\nwhere the \u201c _inverse relation_ \u201d is the original relation\ndescription with a prefix word \u201c _inverse_ \u201d and the\n\u201c _relation list_ \u201d consists of all relations concatenated\n\nby the symbol \u201c|\u201d. We display the generated answers for triple _<salviniaceae, member meronym,_\n_salvinia>_ and triple < _refrigerator, hypernym, white_\n_goods_ >. The base LLaMA frequently gives wrong\nanswers and tends to identify keywords from the input prompts for prediction. In contrast, our method\ncan understand the questions and correctly answer\nvarious KGQA tasks in most cases.\n\n\n**6** **Conclusion**\n\n\nIn this work, we show that the subpar performance\nof LLMs on knowledge-driven tasks stems from a\nlack of effective knowledge alignment. We present\n**KaLM**, a novel knowledge-aligned language modeling approach for aligning autoregressive LLMs\nwith KG knowledge. Specifically, we identify two\nimperative objectives to achieve knowledge alignment: _explicit knowledge alignment_ and _implicit_\n_knowledge alignment_ . We conducted comprehensive experiments and analyses on embedding-based\nKGC and generation-based KGQA. Experimental\nresults demonstrate that our method achieves ef\nfective knowledge alignment and consistently improves performance on knowledge-driven tasks.\n\n\n\n8\n\n\n**Limitations**\n\n\nThere are several future directions to improve this\nwork. Firstly, due to the limitation of computational\nresources, we used the limited-scale LLMs to train\nand evaluate our method. Evaluations on largerscale LLMs, such as the 13B and 70B models, can\nfurther validate the effectiveness of our approach.\nSecondly, we use a simple linear combination of explicit alignment loss and implicit alignment loss as\nthe final training objective for KaLM. Further investigations into various forms of loss combinations\nremain to be explored to maximize the utility of\nknowledge-aligned language modeling. Finally, we\ncan delve into the performance of the knowledge\nrepresentations obtained from knowledge-aligned\nlanguage modeling in cross-domain applications\nsuch as retrieval-augmented generation, to gain\nbroader insights into the generalization capabilities\nof the proposed approach.\n\n\n**References**\n\n\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\n_arXiv preprint arXiv:2303.08774_ .\n\n\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. _arXiv_\n_preprint arXiv:2305.10403_ .\n\n\nAntoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multirelational data. _Advances in neural information pro-_\n_cessing systems_, 26.\n\n\nChen Chen, Yufei Wang, Bing Li, and Kwok-Yan Lam.\n2022. Knowledge is flat: A seq2seq generative framework for various knowledge graph completion. In\n_Proceedings of the 29th International Conference on_\n_Computational Linguistics_, pages 4005\u20134017.\n\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoffrey Hinton. 2020. A simple framework for\ncontrastive learning of visual representations. In _In-_\n_ternational conference on machine learning_, pages\n1597\u20131607. PMLR.\n\n\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp,\nand Sebastian Riedel. 2018. Convolutional 2d knowledge graph embeddings. In _Proceedings of the AAAI_\n_conference on artificial intelligence_, volume 32.\n\n\nKawin Ethayarajh. 2019. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. In\n\n\n\n_Proceedings of the 2019 Conference on Empirical_\n_Methods in Natural Language Processing and the 9th_\n_International Joint Conference on Natural Language_\n_Processing (EMNLP-IJCNLP)_, pages 55\u201365.\n\n\nZhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang,\nHaotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023. Trends in integration of knowledge and large language models: A\nsurvey and taxonomy of methods, benchmarks, and\napplications. _arXiv preprint arXiv:2311.05876_ .\n\n\nPeng Fu, Yiming Zhang, Haobo Wang, Weikang Qiu,\nand Junbo Zhao. 2023. Revisiting the knowledge\ninjection frameworks. In _Proceedings of the 2023_\n_Conference on Empirical Methods in Natural Lan-_\n_guage Processing_, pages 10983\u201310997.\n\n\nBeliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoyanov. 2020. Supervised contrastive learning for pretrained language model fine-tuning. _arXiv preprint_\n_arXiv:2011.01403_ .\n\n\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor BergKirkpatrick, and Graham Neubig. 2021. Towards a\nunified view of parameter-efficient transfer learning.\n_arXiv preprint arXiv:2110.04366_ .\n\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_ .\n\n\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. _arXiv preprint_\n_arXiv:2106.09685_ .\n\n\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye,\nWayne Xin Zhao, and Ji-Rong Wen. 2023. Structgpt: A general framework for large language model\nto reason over structured data. _arXiv preprint_\n_arXiv:2305.09645_ .\n\n\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\n_Proceedings of the 2020 Conference on Empirical_\n_Methods in Natural Language Processing (EMNLP)_,\npages 9119\u20139130.\n\n\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie,\nand Ji-Rong Wen. 2022. Pretrained language models for text generation: A survey. _arXiv preprint_\n_arXiv:2201.05273_ .\n\n\nSong Liu, Haoqi Fan, Shengsheng Qian, Yiru Chen,\nWenkui Ding, and Zhongyuan Wang. 2021. Hit: Hierarchical transformer with momentum contrast for\nvideo-text retrieval. In _Proceedings of the IEEE/CVF_\n_International Conference on Computer Vision_, pages\n11915\u201311925.\n\n\nXueguang Ma, Liang Wang, Nan Yang, Furu Wei, and\nJimmy Lin. 2023. Fine-tuning llama for multi-stage\ntext retrieval. _arXiv preprint arXiv:2310.08319_ .\n\n\n\n9\n\n\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture models. In _International Conference on Learning Repre-_\n_sentations_ .\n\n\nNiklas Muennighoff. 2022. Sgpt: Gpt sentence\nembeddings for semantic search. _arXiv preprint_\n_arXiv:2202.08904_ .\n\n\nJianhao Shen, Chenguang Wang, Linyuan Gong, and\nDawn Song. 2022. Joint language semantic and structure embedding for knowledge graph completion. In\n_Proceedings of the 29th International Conference on_\n_Computational Linguistics_, pages 1965\u20131978.\n\n\nDan Su, Yan Xu, Genta Indra Winata, Peng Xu,\nHyeondey Kim, Zihan Liu, and Pascale Fung. 2019.\nGeneralizing question answering system with pretrained language model fine-tuning. In _Proceedings_\n_of the 2nd Workshop on Machine Reading for Ques-_\n_tion Answering_, pages 203\u2013211.\n\n\nJianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou.\n2021. Whitening sentence representations for better semantics and faster retrieval. _arXiv preprint_\n_arXiv:2103.15316_ .\n\n\nYixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel Collier. 2022. A contrastive\nframework for neural text generation. _Advances in_\n_Neural Information Processing Systems_, 35:21548\u2013\n21561.\n\n\nJiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo\nWang, Chen Lin, Yeyun Gong, Heung-Yeung Shum,\nand Jian Guo. 2023. Think-on-graph: Deep and\nresponsible reasoning of large language model with\nknowledge graph. _arXiv preprint arXiv:2307.07697_ .\n\n\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian\nTang. 2018. Rotate: Knowledge graph embedding by\nrelational rotation in complex space. In _International_\n_Conference on Learning Representations_ .\n\n\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian\nTang. 2019. Rotate: Knowledge graph embedding by\nrelational rotation in complex space. _arXiv preprint_\n_arXiv:1902.10197_ .\n\n\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. [https://](https://github.com/tatsu-lab/stanford_alpaca)\n[github.com/tatsu-lab/stanford_alpaca.](https://github.com/tatsu-lab/stanford_alpaca)\n\n\nKristina Toutanova and Danqi Chen. 2015. Observed\nversus latent features for knowledge base and text\ninference. In _Proceedings of the 3rd workshop on_\n_continuous vector space models and their composi-_\n_tionality_, pages 57\u201366.\n\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint_\n_arXiv:2307.09288_ .\n\n\n\nBo Wang, Tao Shen, Guodong Long, Tianyi Zhou, Ying\nWang, and Yi Chang. 2021a. Structure-augmented\ntext representation learning for efficient knowledge\ngraph completion. In _Proceedings of the Web Confer-_\n_ence 2021_, pages 1737\u20131748.\n\n\nFeng Wang and Huaping Liu. 2021. Understanding\nthe behaviour of contrastive loss. In _Proceedings of_\n_the IEEE/CVF conference on computer vision and_\n_pattern recognition_, pages 2495\u20132504.\n\n\nLiang Wang, Wei Zhao, Zhuoyu Wei, and Jingming\nLiu. 2022a. Simkgc: Simple contrastive knowledge\ngraph completion with pre-trained language models.\nIn _Proceedings of the 60th Annual Meeting of the_\n_Association for Computational Linguistics (Volume_\n_1: Long Papers)_, pages 4281\u20134294.\n\n\nTongzhou Wang and Phillip Isola. 2020. Understanding\ncontrastive representation learning through alignment\nand uniformity on the hypersphere. In _International_\n_Conference on Machine Learning_, pages 9929\u20139939.\nPMLR.\n\n\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021b.\nKepler: A unified model for knowledge embedding\nand pre-trained language representation. _Transac-_\n_tions of the Association for Computational Linguis-_\n_tics_, 9:176\u2013194.\n\n\nXintao Wang, Qianyu He, Jiaqing Liang, and Yanghua\nXiao. 2022b. Language models as knowledge embeddings. _arXiv preprint arXiv:2206.12617_ .\n\n\nBishan Yang, Scott Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Embedding entities\nand relations for learning and inference in knowledge\nbases. In _Proceedings of the International Confer-_\n_ence on Learning Representations (ICLR) 2015_ .\n\n\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019. Kgbert: Bert for knowledge graph completion. _arXiv_\n_preprint arXiv:1909.03193_ .\n\n\nLiang Yao, Jiazhen Peng, Chengsheng Mao, and\nYuan Luo. 2023. Exploring large language models for knowledge graph completion. _arXiv preprint_\n_arXiv:2308.13916_ .\n\n\nMichihiro Yasunaga, Antoine Bosselut, Hongyu Ren,\nXikun Zhang, Christopher D Manning, Percy S\nLiang, and Jure Leskovec. 2022. Deep bidirectional\nlanguage-knowledge graph pretraining. _Advances in_\n_Neural Information Processing Systems_, 35:37309\u2013\n37323.\n\n\nYichi Zhang, Zhuo Chen, Wen Zhang, and Huajun Chen.\n2023. Making large language models perform better in knowledge graph completion. _arXiv preprint_\n_arXiv:2310.06671_ .\n\n\n\n10\n\n\n**A** **More Detailed Review of Related Work**\n\n\nThis work focuses on fine-tuning autoregressive\nLLMs to align with KG knowledge. Our work intersects with the following research areas: Knowledge\nEnhancement for LLMs, Knowledge Graph Completion, Contrastive Representation Learning, and\nRepresentation Anisotropy of Language Models.\n\n\n**A.1** **Knowledge Enhancement for LLMs**\n\n\nKnowledge enhancement aims to incorporate factual and domain-specific knowledge into LLMs\nto address their knowledge deficiencies. This can\nbe divided into retrieval-based knowledge augmentation and training-based knowledge integration.\n_Retrieval-based knowledge augmentation_ methods\nleverage external retrieval modules to provide additional knowledge, aiming to improve the knowledge reasoning capability of LLMs (Sun et al.,\n2023; Jiang et al., 2023). However, this approach\nmay lead to knowledge conflicts (Feng et al., 2023),\nwhere the knowledge in LLMs and the knowledge in the retrieved documents are inconsistent or\nthe retrieved multiple documents are contradictory.\n_Training-based knowledge integration_ methods involve using the textual descriptions of KG triples\nto pre-train or fine-tune LLMs, aiming to achieve\nknowledge alignment. These methods can be categorized into explicit alignment (Wang et al., 2021b;\nYasunaga et al., 2022) and implicit alignment (Yao\net al., 2023; Zhang et al., 2023) based on whether\nthey directly optimize the knowledge representation. Nevertheless, these methods have either sacrificed the generative capability or lacked effective\nrepresentation alignment. Our approach enhances\nthe knowledge of LLMs via a unique joint objective\nof explicit alignment and implicit alignment, improving the quality of knowledge representations\nand generative knowledge reasoning capabilities.\n\n\n**A.2** **Knowledge Graph Completion**\n\n\nKnowledge graph completion (KGC) refers to inferring missing triples from an incomplete KG,\nwhich can be used to evaluate the knowledge reasoning ability and knowledge representation quality\nof LLMs. Existing KGC methods can be categorized into structure-based and description-based.\n_Structure-based methods_ represent entities and relations as fixed-dimensional vector embeddings\nand use scoring functions to assess the plausibility\nof triples (Bordes et al., 2013; Sun et al., 2019).\n_Description-based methods_ further incorporate the\n\n\n\ntextual descriptions of KG triples and leverage pretrained language models to learn knowledge representations of entities and relations (Yao et al., 2019;\nShen et al., 2022; Wang et al., 2022b). However,\nstructure-based methods fail to generalize to unseen entities and relations, while description-based\nmethods lack interpretability and exhibit lower efficiency when dealing with extremely large KGs.\n\n\n**A.3** **Contrastive Representation Learning**\n\n\nContrastive learning has demonstrated remarkable\nsuccess in learning representations across various\ndomains (Chen et al., 2020; Liu et al., 2021; Gunel\net al., 2020). The goal is to learn representations\nthat capture shared information between positive\npairs while remaining invariant to perturbing noise.\nThe commonly used contrastive learning objectives\nshare a standardized design involving a softmax\nfunction over cosine similarity of paired features,\nwith a temperature parameter to control the penalty\nstrength on hard negative samples. Wang and Isola\n(2020) propose understanding contrastive learning\nthrough the lens of alignment and uniformity on the\nhypersphere. Wang and Liu (2021) show that temperature in the contrastive loss controls the strength\nof penalties over negative samples.\n\n\n**A.4** **Representation Anisotropy of Language**\n\n**Models**\n\n\nPLMs have long been plagued by _representation_\n_anisotropy_ (Ethayarajh, 2019), where the learned\ntoken and sentence representations are confined to a\nnarrow cone within the entire representation space.\nThe issue of representation anisotropy not only results in model degradation (Su et al., 2022) but also\nleads to poor performance on discriminative tasks\n(Muennighoff, 2022). Previous work on alleviating representation anisotropy has mainly focused\non post-processing techniques such as normalizing\nflows (Li et al., 2020) or whitening operations (Su\net al., 2021) to obtain isotropic representations. Su\net al. (2022) propose a contrastive training objective\nto encourage learning isotropic token representations. However, these methods mainly improve the\nisotropy of token representations without enhancing the discriminability of sentence representations.\nOur method improves the token-level and sentencelevel representation anisotropy of LLMs through\ndual-view knowledge graph contrastive learning,\nand it has rigorous theoretical guarantees.\n\n\n\n11\n\n\n\uf8ec _e_ _[f]_ [(] _[D]_ _[hr]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[t]_ [)] _[/\u03c4]_\n\uf8ec\n\uf8ec _N_\n\uf8ed\n\n\n_\u0338_\n\n\n\n\uf8f6\uf8f9\n\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\uf8fa\uf8fa\uf8fa\uf8fb\n\n\n_\u0338_\n\n\n\n**B** **Proofs for Theoretical Analysis**\n\n\nIn this section, we present proofs for theorems in\nSections 4.1 and 4.2 of the main paper.\n\n\n**B.1** **Proof of Theorem 1 in Section 4.1**\n\n\nRecall the reformulated dual-view knowledge\ngraph contrastive learning objective (Equation 5):\n\n\n_\u0338_\n\n\n\n+ E\n_D_ _i_ _\u223cp_ _data_\n\n\n_\u0338_\n\n\n\n\uf8eb\n\n\n_\u0338_\n\n\n\n+ E\n\n\n_\u0338_\n\n\n\n\uf8ee\n\n\nlim\n_N\u2192\u221e_ [log]\n\uf8ef\uf8ef\uf8ef\uf8f0\n\n\n_\u0338_\n\n\n\n+\n_N_\n\n\n_\u0338_\n\n\n\n_N_\n\ufffd _e_ _[f]_ [(] _[D]_ _[t]_ _i_ _[\u2032]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[t]_ [)] _[/\u03c4]_\n\n_i_ =1\n\n_N_\n\n\n_\u0338_\n\n\n\n= _\u2212_ [1] E\n\n_\u03c4_ ( _D_ _hr_ _,D_ _t_ ) _\u223cp_ pos\n\n\n_\u0338_\n\n\n\n_f_ ( _D_ _hr_ ) _[\u22a4]_ _f_ ( _D_ _t_ )\n\ufffd \ufffd\n\n\n_\u0338_\n\n\n\n\ufffd _e_ _[f]_ [(] _[D]_ _i_ _[\u2212]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[i]_ [)] _[/\u03c4]_ [\ufffd\ufffd]\n\n\n_\u0338_\n\n\n\nlog E\n\n\ufffd _D_ _i_ _[\u2212]_ _[\u223c][p]_ [data]\n\n\n_\u0338_\n\n\n\n_._\n\n\n_\u0338_\n\n\n\n_L_ exp ( _f_ ; _\u03c4, N_ ) \u225c E\n( _D_ _hr_ _,D_ _t_ ) _\u223cp_ pos\n_i.i.d._\n_{D_ _t_ _[\u2032]_ _i_ _[}]_ _[N]_ _i_ =1 _\u223c_ _p_ data\n\uf8ee\n\n_[\u22a4]_\n\n\n_\u0338_\n\n\n\nWe now finish the _proof of Theorem 1_ .\n\n\nlim\n_N\u2192\u221e_ _[L]_ [exp] [(] _[f]_ [;] _[ \u03c4,][ N]_ [)] _[ \u2212]_ [log] _[ N]_ [ =]\n\n\n_\u0338_\n\n\n\n_\u2212_ [1] E _f_ ( _D_ _hr_ ) _[\u22a4]_ _f_ ( _D_ _t_ )\n\n_\u03c4_ ( _D_ _hr_ _,D_ _t_ ) _\u223cp_ pos \ufffd \ufffd\n\n\n_\u0338_\n\n\n\n_e_ _[f]_ [(] _[D]_ _[hr]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[t]_ [)] _[/\u03c4]_\n\n_\u2212_\nlog\n\n_N_\n_e_ _[f]_ [(] _[D]_ _[hr]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[t]_ [)] _[/\u03c4]_ + \ufffd _e_ _[f]_ [(] _[D]_\n\n\uf8ef\uf8ef\uf8ef\uf8f0 _i_ =1\n\n\n_\u0338_\n\n\n\n_N_\n_e_ _[f]_ [(] _[D]_ _[hr]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[t]_ [)] _[/\u03c4]_ + \ufffd\n\n\n_\u0338_\n\n\n\n\ufffd _e_ _[f]_ [(] _[D]_ _[hr]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[t]_ _i_ _[\u2032]_ [)] _[/\u03c4]_\n\n_i_ =1\n\n\n_\u0338_\n\n\n\n\uf8f9\n\n\n_._\n\uf8fa\uf8fa\uf8fa\uf8fb\n\n\n_\u0338_\n\n\n\nFrom the symmetry of _p_, we can derive:\n\n\n_L_ exp ( _f_ ; _\u03c4, N_ ) =\n\n\n_\u0338_\n\n\n\n+ E\n_D_ _i_ _\u223cp_ _data_\n\n\n_\u0338_\n\n\n\n\ufffd _e_ _[f]_ [(] _[D]_ _i_ _[\u2212]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[i]_ [)] _[/\u03c4]_ [\ufffd\ufffd]\n\n\n_\u0338_\n\n\n\n\ufffd\n\n\n_\u0338_\n\n\n\nlog E\n_D_ _i_ _[\u2212]_ _[\u223c][p]_ [data]\n\n\n_\u0338_\n\n\n\n_._\n\n\n_\u0338_\n\n\n\nE _\u2212f_ ( _D_ _hr_ ) _[\u22a4]_ _f_ ( _D_ _t_ ) _/\u03c4_ + E\n( _D_ _hr_ _,D_ _t_ ) _\u223cp_ pos \ufffd \ufffd ( _D_ _hr_ _,D_ _t_ ) _\u223cp_ pos\n\n_i.i.d._\n_{D_ _t_ _[\u2032]_ _i_ _[}]_ _[N]_ _i_ =1 _\u223c_ _p_ data\n\n\n_N_\n\nlog _e_ _[f]_ [(] _[D]_ _[hr]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[t]_ [)] _[/\u03c4]_ + \ufffd _e_ _[f]_ [(] _[D]_ _[t]_ _i_ _[\u2032]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[t]_ [)] _[/\u03c4]_ _._\n\n\ufffd \ufffd _i_ =1 \ufffd\ufffd\n\n\n_\u0338_\n\n\n\nE\n( _D_ _hr_ _,D_ _t_ ) _\u223cp_ pos\n\n\n_\u0338_\n\n\n\n_N_\n\ufffd\n\n\n_\u0338_\n\n\n\n_N_\n\ufffd _e_ _[f]_ [(] _[D]_ _[t]_ _i_ _[\u2032]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[t]_ [)] _[/\u03c4]_\n\n_i_ =1 \ufffd\ufffd\n\n\n_\u0338_\n\n\n\nlog\n\n\n_\u0338_\n\n\n\n\ufffd\n\n\n_\u0338_\n\n\n\n_e_ _[f]_ [(] _[D]_ _[hr]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[t]_ [)] _[/\u03c4]_ +\n\n\n_\u0338_\n\n\n\n_._\n\n\n_\u0338_\n\n\n\nNote that we can have the following limits almost\nsurely by the strong law of large numbers (SLLN):\n\n\n_\u0338_\n\n\n\n\uf8eb\n\n\n_\u0338_\n\n\n\n**B.2** **Proof of Theorem 2 in Section 4.2**\n\n\nRecall the asymptotics of the explicit knowledge\nalignment objective when the number of negative\nsamples approaches infinity (Equation 6):\n\n\nlim\n_N\u2192\u221e_ _[L]_ [exp] [(] _[f]_ [;] _[ \u03c4,][ N]_ [)] _[ \u2212]_ [log] _[ N]_ [ =]\n\n\n_\u0338_\n\n\n\n_\u2212_ [1] E\n\n_\u03c4_ ( _D_ _hr_ _,D_ _t_ ) _\u223cp_ pos\n\n\n_\u0338_\n\n\n\n\uf8f6\n\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n\n\n_\u0338_\n\n\n\n_f_ ( _D_ _hr_ ) _[\u22a4]_ _f_ ( _D_ _t_ )\n\ufffd \ufffd\n\n\n_\u0338_\n\n\n\n\uf8ec _e_ _[f]_ [(] _[D]_ _[hr]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[t]_ [)] _[/\u03c4]_\n\uf8ec\n\uf8ec _N_\n\uf8ed\n\n\n_\u0338_\n\n\n\nlim\n_N\u2192\u221e_ [log]\n\n\n_\u0338_\n\n\n\n+\n_N_\n\n\n_\u0338_\n\n\n\n_N_\n\ufffd _e_ _[f]_ [(] _[D]_ _[t]_ _i_ _[\u2032]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[t]_ [)] _[/\u03c4]_\n\n_i_ =1\n\n_N_\n\n\n_\u0338_\n\n\n\n= log E _f_ ( _D_ _i_ _[\u2212]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[i]_ [)] _[/\u03c4.]_\n_D_ _i_ _[\u2212]_ _[\u223c][p]_ [data]\n\n\nThen we can derive the following limits:\n\n\nlim\n_N\u2192\u221e_ _[L]_ [exp] [(] _[f]_ [;] _[ \u03c4,][ N]_ [)] _[ \u2212]_ [log] _[ N]_\n\n\n_\u0338_\n\n\n\nRecall the definition of sentence-level anisotropy\nvalue of corpus _{D_ _i_ _}_ _[N]_ _i_ =1 [(Equation][ 7][):]\n\n\n_\u0338_\n\n\n\n\ufffd _e_ _[f]_ [(] _[D]_ _i_ _[\u2212]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[i]_ [)] _[/\u03c4]_ [\ufffd\ufffd] _._\n\n\n_\u0338_\n\n\n\n+ E\n_D_ _i_ _\u223cp_ _data_\n\n\n_\u0338_\n\n\n\nlog E\n\n\ufffd _D_ _i_ _[\u2212]_ _[\u223c][p]_ [data]\n\n\n_\u0338_\n\n\n\n= E\n( _D_ _hr_ _,D_ _t_ ) _\u223cp_ pos\n\n\n_\u0338_\n\n\n\n_\u2212f_ ( _D_ _hr_ ) _[\u22a4]_ _f_ ( _D_ _t_ ) _/\u03c4_\n\ufffd \ufffd\n\n\n_\u0338_\n\n\n\n+ lim E\n_N\u2192\u221e_ ( _D_ _hr_ _,D_ _t_ ) _\u223cp_ pos\n_i.i.d._\n_{D_ _t_ _[\u2032]_ _i_ _[}]_ _[N]_ _i_ =1 _\u223c_ _p_ data\n\n\n_\u0338_\n\n\uf8ee \uf8eb\n\n_[\u22a4]_\n\n\n\n_N_\n\ufffd _e_ _[\u22a4]_ _i_ _[e]_ _[j]_ _[.]_\n\n_j_ =1 _,j_ = _\u0338_ _i_\n\n\n\n_N_\n\ufffd\n\n\n_i_ =1 _\u0338_\n\n\n\n1\nanisotropy _{D}_ =\n_N_ ( _N \u2212_ 1)\n\n_\u0338_\n\n\n\n_\u0338_\n\n\uf8f6\uf8f9\n\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\uf8fa\uf8fa\uf8fa\uf8fb\n\n\n12\n\n\n\n_\u0338_\n\n\nlog\n\uf8ef\uf8ef\uf8ef\uf8f0\n\n\n\n_\u0338_\n\n\uf8eb\n\n\n\n_\u0338_\n\n\uf8ec _e_ _[f]_ [(] _[D]_ _[hr]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[t]_ [)] _[/\u03c4]_\n\uf8ec\n\uf8ec _N_\n\uf8ed\n\n\n\n_\u0338_\n\n\n+\n_N_\n\n\n\n_N_\n\n_\u0338_\n\n\ufffd _e_ _[f]_ [(] _[D]_ _[t]_ _i_ _[\u2032]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[t]_ [)] _[/\u03c4]_\n\n_i_ =1\n\n_N_\n\n\n\n_\u0338_\n\n\nWe can further derive the inequality below from the\nsecond term of Equation 6 with Jensen\u2019s inequality\nwhen _p_ data is uniform over finite samples _{D_ _i_ _}_ _[N]_ _i_ =1 [:]\n\n\n\n_\u0338_\n\n\n= E\n( _D_ _hr_ _,D_ _t_ ) _\u223cp_ pos\n\n\n\n_\u0338_\n\n\n_\u2212f_ ( _D_ _hr_ ) _[\u22a4]_ _f_ ( _D_ _t_ ) _/\u03c4_\n\ufffd \ufffd\n\n\n\ufffd _e_ _[f]_ [(] _[D]_ _i_ _[\u2212]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[i]_ [)] _[/\u03c4]_ [\ufffd\ufffd]\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n\nE\n_D_ _i_ _\u223cp_ _data_\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n\nlog E\n\n\ufffd _D_ _i_ _[\u2212]_ _[\u223c][p]_ _[data]_\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n\n\uf8eb\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n\n\uf8ed _N_ [1]\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n\n\uf8f6\n\n\uf8f8\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n\n= [1]\n\n_N_\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n\n_N_\n\ufffd log\n\n\n_i_ =1\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n\n_N_\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n\n_N_\n\ufffd _e_ _[e]_ _i_ _[\u22a4]_ _[e]_ _[j]_ _[/\u03c4]_\n\n_j_ =1\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n\n_N_\n\ufffd _e_ _[\u22a4]_ _i_ _[e]_ _[j]_\n\n_j_ =1\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n\n1\n_\u2265_\n_\u03c4N_ [2]\n\n\n1\n\n=\n_\u03c4N_ [2]\n\n_\u0338_\n\n\n_\u0338_\n\n\n\n_N_\n\ufffd\n\n\n_i_ =1\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n\n_N_\n\n\uf8eb\n\n\ufffd\n\uf8ed _i_ =1 _\u0338_\n\n\n_\u0338_\n\n\n\n\uf8eb\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n\n_N_\n\ufffd\n\n_\u0338_\n\n\n_\u0338_\n\n\n\n\uf8f6\n\n_\u0338_ \uf8f8\n\n\n_\u0338_\n\n\n\n_i_ =1 _\u0338_\n\n\n_\u0338_\n\n\n\n_N_\n\ufffd _e_ _[\u22a4]_ _i_ _[e]_ _[j]_ [+] _[ N]_\n\n_j_ =1 _,j_ = _\u0338_ _i_\n\n\n_\u0338_\n\n\n\n_\u0338_\n\n\n_N_\n\n1\n\n\ufffd _e_ _[\u22a4]_ _i_ _[e]_ _[j]_ [+] _\u03c4N_\n\n_j_ =1 _,j_ = _\u0338_ _i_\n\n\n\n_\u0338_\n\n\n1\n\n= _[N][ \u2212]_ [1] _\u00b7_\n\n_\u03c4N_ _N_ ( _N \u2212_ 1)\n\n_\u0338_\n\n\n\n_\u0338_\n\n\n_N_\n\ufffd\n\n\n_i_ =1 _\u0338_\n\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n1\n\n= _[N]_ _\u03c4N_ _[ \u2212]_ [1] _\u00b7_ anisotropy _{D}_ + _\u03c4N_ _[.]_\n\n\n\n_\u0338_\n\n\n_\u0338_\n\n\nWe now finish the _proof of Theorem 2_ .\n\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n\ufffd _e_ _[f]_ [(] _[D]_ _i_ _[\u2212]_ [)] _[\u22a4]_ _[f]_ [(] _[D]_ _[i]_ [)] _[/\u03c4]_ [\ufffd\ufffd]\n\n\n\n_\u0338_\n\n\n_\u0338_\n\n\nE\n_D_ _i_ _\u223cp_ _data_\n\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n\ufffd\n\n\n\n_\u0338_\n\n\n_\u0338_\n\n\nlog E\n_D_ _i_ _[\u2212]_ _[\u223c][p]_ _[data]_\n\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n1\n\n_\u00b7_\n\n_\u2265_ _[N]_ _\u03c4N_ _[ \u2212]_ [1] anisotropy _{D}_ + _\u03c4N_ _[.]_\n\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n**C** **Further Details about Implementation**\n**and Experimental Setup**\n\n\n**C.1** **Dataset Details**\n\n\nWN18RR and FB15k-237 are commonly used KGs\nderived from WordNet and Freebase, respectively\n(Bordes et al., 2013). They have been carefully\nconstructed to prevent test set leakage by removing\ninverse relations. We use these datasets for training\nand evaluation. The statistics are shown in Table 2.\n\n\nTable 2: Statistics of the datasets.\n\n|Dataset|#Entity #Relation #Train #Valid #Test|\n|---|---|\n|WN18RR<br>FB15k-237|40_,_ 943<br>11<br>86_,_ 835<br>3_,_ 034<br>3_,_ 134<br>14_,_ 541<br>237<br>272_,_ 115<br>17_,_ 535<br>20_,_ 466|\n\n\n\n**C.2** **KaLM Implementation Details**\n\n\nWe initially choose Llama-2-7B as the base LLM\nand fine-tune it through the training objective in\nEquation 4. We use varying batch sizes for explicit knowledge alignment and implicit knowledge\nalignment. For WN18RR, we use a batch size of\n24 for explicit alignment and 4 for implicit alignment. For FB15k-237, the batch sizes are 40 for\nexplicit alignment and 6 for implicit alignment. To\n\n\n\nsave computing resources for parameter-efficient\nfine-tuning, we use the LoRA (Hu et al., 2021)\nmethod to fine-tune the [\u201c _gate_ _ _proj_ \u201d, \u201c _up_ _ _proj_ \u201d,\n\u201c _down_ _ _proj_ \u201d] modules in the feed-forward net\nwork of the Llama-2-7B model. We conducted all\n\ntraining on an NVIDIA 4090 _\u00d7_ 8 GPU. The hyperparameters utilized for training KaLM (based on\nLlama-2-7B) are enumerated in Table 3.\n\n\nTable 3: Hyper-parameters for training KaLM.\n\n\nHyper-parameters WN18RR FB15k-237\n\n\n_\u0338_ epochs 20 15\n\nmax-description-length 50 50\nmax-language-modeling-length 256 256\nexplicit-alignment-batch-size 24 40\n\n_\u0338_ implicit-alignment-batch-size 4 6\n\nlora-module ffn ffn\n\nlora-alpha 16.0 16.0\nlora-drouout 0.05 0.05\n\nlora-rank 8 8\n\nbnb-config load-in-8bit load-in-8bit\nlearning-rate 1e-4 1e-4\nLR-sheduler-type cosine cosine\nweight-decay 0.001 0.001\ngradient-checkpointing True True\noptimizer AdamW AdamW\nAdamW-beta1 0.9 0.9\n\nAdamW-beta2 0.999 0.999\n\nbf16 True True\n\n\nWe also implemented KaLM based on other\nLLMs to demonstrate the generalizability of our\napproach, including Llama-3-8B, Mistral-7B-v0.1,\nOPT-6.7B, Pythia-6.9B, and Pythia-2.8B. It is important to note that the feed-forward network layers\nin the Pythia model are named [\u201c _dense_ _ _h_ _ _to_ _4 _h_ \u201d,\n\u201c _dense_ _4 _h_ _ _to_ _ _h_ \u201d], while in the OPT model they\nare named [\u201c _fc_ 1 \u201d, \u201c _fc_ 2 \u201d]. This differs from the\nfeed-forward network layers in the Llama and Mistral model series. The parameters used in these\nexperiments are shown in Table 4 (only the differing parameters are listed; the unlisted parameters\nremain consistent with Table 3).\n\n\nFor the cosine similarity matrix composed of\nhead entity-relation embeddings (row direction)\nand tail entity embeddings (column direction), we\ncalculate the cross-entropy loss in the row direction\n(i.e., a head entity-relation embedding matching\ndifferent tail entity embeddings) and the column\ndirection (i.e., a tail entity embedding matching different head entity-relation embeddings) separately.\nWe then take the average of the two losses to obtain\nthe final InfoNCE loss. Similar to Equation 1, the\n\n\n\n_\u0338_\n\n\n_\u0338_\n\n\n13\n\n\nTable 4: Additional Hyper-parameters for training _KaLM_ with different LLMs.\n\n\nModels epochs explicit-batch-size implicit-batch-size bnb-config\n\n\nLlama-3-8B-WN 20 18 3 load-in-8bit\n\nLlama-3-8B-FB 15 36 5 load-in-8bit\n\nMistral-7B-v0.1-WN 20 40 5 load-in-4bit\n\nMistral-7B-v0.1-FB 15 72 8 load-in-4bit\n\nOPT-6.7B-WN 20 24 3 load-in-8bit\n\nOPT-6.7B-FB 15 40 6 load-in-8bit\n\nPythia-6.9B-WN 20 24 4 load-in-8bit\nPythia-6.9B-FB 15 42 6 load-in-8bit\nPythia-2.8B-WN 20 48 8 load-in-8bit\nPythia-2.8B-FB 15 96 10 load-in-8bit\n\n\n\ncolumn-direction loss is defined as follows:\n\n\n_e_ [(] _[\u03d5]_ [(] _[e]_ _[t]_ _[,e]_ _[hr]_ [)] _[\u2212][\u03b3]_ [)] _[/\u03c4]_\n_\u2113_ _c_ = _\u2212_ log _\u03d5_ ( _e_ _t_ _,e_ _hr\u2032_ _[.]_\n\n_e_ [(] _[\u03d5]_ [(] _[e]_ _[t]_ _[,e]_ _[hr]_ [)] _[\u2212][\u03b3]_ [)] _[/\u03c4]_ + ~~[\ufffd]~~ _[N]_ _j_ =1 _[e]_ _j_ [)] _[/\u03c4]_\n\n\n**C.3** **More Details about Evaluations**\n\n\nFor the embedding-based KGC task, we report five\nautomated metrics: Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hit@ _k_ ( _k \u2208{_ 1 _,_ 3 _,_ 10 _}_ ).\nMR is the mean rank of all test triplets and MRR denotes the average reciprocal rank of all test triples.\nHit@ _k_ measures the proportion of entities correctly\nranked in the top _k_ . Following previous work, our\nmethod is evaluated under the _filtering setting_ (Bordes et al., 2013), where the scores of all true triples\nin the training, validation, and testing set are ignored. All results are averaged over the tail direction (a <head entity-relation> embedding matching\ndifferent tail entity embeddings, i.e., tail entity prediction) and head direction (a <tail entity-inverse\nrelation> embedding matching different head entity\nembeddings, i.e., head entity prediction).\n\n\nFor the generation-based KGQA task, we report\nthe prediction accuracy over head entities, tail entities, relations, and relation classifications. To better\nprompt LLMs for the knowledge graph questionanswering task, we selected several triples from the\nvalidation set and constructed few-shot examples\nusing the corresponding templates from Table 5.\n\n\n**D** **Addition Experimental Results**\n\n\nIn this section, we provide more experimental results to show the effectiveness of our method.\n\n\n\n**D.1** **More Experiments on Knowledge**\n**Representation Assessment**\n\n\nIn Table 5, we present additional knowledge representation results (the embedding-based KGC task)\nto demonstrate the effectiveness of KaLM in knowl\nedge alignment. The best and second-best experimental results are indicated by **bold** and underline\ntexts, respectively. Overall, the proposed method\nachieved excellent performance on the embeddingbased KGC task, delivering impressive results in\nthe MR and Hit@10 metrics, while also being\nhighly competitive in other metrics.\n\nThe experimental results based on LLMs of different sources and scales demonstrate the effective\nness and generalizability of our proposed method.\nUnder similar experimental settings, more powerful LLMs (such as Llama3-8B and Mistral-7B)\nachieved better metrics after being fine-tuned with\nKaLM, which also demonstrates the scalability of\nour method. It is worth noting that for LLMs of the\nsame origin but different scales (Pythia-6.9B and\nPythia-2.8B), the smaller-scale Pythia-2.8B benefited from a larger training batch size during finetuning. As a result, its final experimental metrics\nmatched or even surpassed those of the more powerful Pythia-6.9B model. This also highlights the\nimportance of large batch sizes for the embeddingbased KGC task, suggesting that using more powerful computing resources and larger GPU memory\ncould further enhance the effectiveness of the proposed KaLM method.\n\n\n**D.2** **More Experiments on Knowledge**\n**Inference Evaluation**\n\n\nIn Figure 6, we present additional knowledge inference results (generation-based KGQA) to demon\n\n\n14\n\n\nTable 5: More Embedding-based KGC results with various LLMs on WN18RR and FB15k-237.\n\n|Method|WN18RR|FB15k-237|\n|---|---|---|\n|**Method**|**MR**<br>**MRR**<br>**H@1**<br>**H@3**<br>**H@10**|**MR**<br>**MRR**<br>**H@1**<br>**H@3**<br>**H@10**|\n\n\n\n_structure-based methods_\n\nTransE 2300 0.243 0.043 0.441 0.532 323 0.279 0.198 0.376 0.441\n\nDistMult 7000 0.444 0.412 0.470 0.504 512 0.281 0.199 0.301 0.446\n\nRotatE 3340 0.476 0.428 0.492 0.571 177 0.338 0.241 0.375 0.533\n\n_description-based methods (autoencoder PLMs)_\n\nKG-BERT 97 0.216 0.041 0.302 0.524 153 - - - 0.420\n\nStAR 51 0.401 0.243 0.491 0.709 117 0.296 0.205 0.322 0.482\n\nC-LMKE 72 0.598 0.480 0.675 0.806 183 **0.404** **0.324** **0.439** **0.556**\n\nSimKGC - **0.671** **0.587** **0.731** 0.817 - 0.333 0.246 0.362 0.510\n\n_description-based methods (autoregressive LLMs)_\n\nLlama-2-7B 15969 0.010 0.004 0.010 0.020 5359 0.006 0.002 0.004 0.012\n\n**Llama2-7B** _KaLM_ **19** 0.556 0.409 0.656 0.851 **114** 0.299 0.204 0.325 0.502\n**Llama3-8B** _KaLM_ 23 0.588 0.446 0.676 0.860 121 0.308 0.212 0.337 0.509\n**Mistral-7B** _KaLM_ 20 0.612 0.484 0.702 **0.869** 116 0.317 0.225 0.351 0.518\n**OPT-6.7B** _KaLM_ 24 0.514 0.397 0.603 0.822 126 0.288 0.199 0.312 0.486\n**Pythia-6.9B** _KaLM_ 28 0.508 0.394 0.598 0.818 130 0.289 0.199 0.310 0.484\n**Pythia-2.8B** _KaLM_ 30 0.539 0.398 0.644 0.829 133 0.292 0.205 0.318 0.489\n\n\n\nstrate the effectiveness of KaLM in knowledge\nalignment. This section demonstrates the performance of various powerful LLMs (including\nLlama-2-7B, Llama-3-8B, and Mistral-7B) before\nand after fine-tuning with KaLM, across various\nknowledge graph question-answering tasks (including head entity prediction, tail entity prediction,\nrelation prediction, and triple classification).\n\n\nThe experimental results can be divided into\nthree groups by color: the green series, blue series,\nand red series correspond to the KGQA results of\nLlama-2-7B, Llama-3-8B, and Mistral-7B before\nand after training, respectively. It can be observed\nthat after fine-tuning with KaLM, all three LLMs\nachieved consistent improvements in prediction accuracy for the question-answering tasks.\n\n\nAt the KGQA task level, the most significant\noverall improvements were observed in tail entity\nprediction (an average increase of 14.1%) and triple\nclassification (an average increase of 12.7%), followed by relation prediction (an average increase\nof 8.6%) and head entity prediction (an average\nincrease of 6.9%). At the LLM level, the most exciting improvements were seen in Llama-3-8B (an\naverage increase of 11.1%) and Mistral-7B (an average increase of 10.8%), while Llama-2-7B showed\nrelatively smaller gains (an average increase of\n9.6%). This suggests that our method demonstrates\nbetter scalability with more powerful LLMs.\n\n\n\n**D.3** **More Visualizations on Knowledge**\n**Representation Matrix**\n\n\nFrom this section onward, unless stated otherwise,\nKaLM refers to the model checkpoint trained on\nLlama-2-7B using our method. We present more\nknowledge representation results to demonstrate\nthe effectiveness of KaLM in knowledge alignment. Figure 7 displays the sentence similarity\nmatrix of several similar entity descriptions from\nthe WN8RR dataset. Detailed information about\n\nentity names and descriptions can be found in Figure 8. It is evident that KaLM can obtain more\n\ndistinguishable knowledge representations, where\nthe similarity between related entities (diagonal\nelements) is high, while the similarity between unrelated entities (off-diagonal elements) is low.\n\n\n**D.4** **Detailed analysis of Representation**\n**Anisotropy**\n\n\nWe further analyze the sentence-level representation anisotropy on the Wikitext-103 test set using\nmodel checkpoints trained on the WN18RR dataset.\nThe sentence-level anisotropy value for a given\ncorpus _{D_ _i_ _}_ _[N]_ _i_ =1 [is defined in Equation][ 7][, where a]\nlower anisotropy value indicates better discriminative characteristics of sentence representations.\nFigure 9 plots the anisotropy value over different\nlayers for LLaMA and KaLM. We can observe\nthat the anisotropy value of LLaMA consistently\n\n\n\n15\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|6|9.4|\n|---|---|---|---|---|---|---|---|\n|||||||||\n|||~~Llam~~<br>Llama<br>~~Llam~~|~~-2-7B~~<br>~~-~~2~~-~~KaLM<br>~~-3-8B~~|||61.6<br>|65.8|\n|||Llama<br>Mistr<br>|~~-~~3~~-~~KaLM<br>l~~-~~7B<br>|||~~55.9~~<br>53.|6<br>~~49.3~~|\n||~~Mistra~~|~~Mistra~~|~~l-KaLM~~|~~l-KaLM~~||||\n||||~~28.5~~<br>|~~28.5~~<br>|~~8.1~~<br>29.8|~~29.0~~<br>~~36.7~~||\n||||~~18.6~~|~~18.6~~||||\n||~~16.2~~<br>11.<br>|~~16.2~~<br>11.<br>|11.6<br>9<br>14.<br>~~7.2~~<br>11.6<br>|11.6<br>9<br>14.<br>~~7.2~~<br>11.6<br>|12.1<br>5<br>1<br>~~17.9~~|2.8||\n||~~7.8~~|~~7.8~~|||3.7<br>3.1|||\n|||||||||\n\n\nFigure 6: Comparison of generative knowledge inference performance between Base LLMs and their fine-tuned\nKaLM versions, best viewed in three color groups. The symbol _\u2191_ means higher is better and _\u2193_ means lower is better.\n\n\n\nremains at a relatively high level, suggesting that\nthe base LLM suffers from severe representation\nanisotropy issues. In contrast, our proposed KaLM\nnotably mitigates this issue, with the anisotropy\nvalues decreasing gradually as the depth of the\nmodel increases, and dropping significantly from\n0.5 to 0.2 at the output layer. The anisotropy values\nof the last layer for LLaMA and KaLM show that\nafter training with our method, the sentence-level\nanisotropy value significantly decreased from 0.83\nto 0.21. The results indicate that our method can\n\neffectively reduce the anisotropy of representations\nacross layers in LLMs, resulting in a significant\nimprovement in knowledge representation.\nFigure 10 analyzes the changes in anisotropy values during the model training process. The results\nshow that the anisotropy values decrease rapidly after a few epochs of training and eventually stabilize\nat a low level. We assume that the initial epochs of\ntraining have completed the preliminary alignment\nof knowledge representation, while the subsequent\ntraining epochs mainly focus on integrating explicit\nand implicit representations.\n\n\n**E** **Ablation Studies**\n\n\nIn this section, we present concrete ablation studies\nto analyze the effectiveness of each component\nof our approach. We ablate the settings that led\nto the final design, including training objectives,\n\n\n\nfine-tuning modules, and training epochs. It is\nimportant to note that the results of the ablation\nexperiments in this section were obtained from\nearlier runs on an NVIDIA 3090 _\u00d7_ 4 GPU, which\nmay lead to slight differences compared to the full\nKGC results presented in the main text.\n\n\n**E.1** **The necessity of the implicit knowledge**\n**alignment objective (Equation 3)**\n\n\nIn Table 6, we train the model using different loss\nweights (i.e., the _\u03bb_ parameter in Equation 4) and\nanalyze its performance on the KGC task. Note\nthat this experiment is conducted solely for ablation\nanalysis, thus only 10 training epochs are used. Experimental results reveal that incorporating the implicit knowledge alignment objective (i.e., _\u03bb >_ 0 )\ngenerally leads to better performance in KGC, indicating further improvement in knowledge representation. The best performance in KGC is achieved\nwhen _\u03bb_ = 0 _._ 1 . The results confirm that both explicit alignment and implicit alignment are crucial\nfor knowledge alignment, as they both essentially\nrequire a deep understanding of knowledge.\nThe implicit knowledge alignment objective focuses on incorporating textual patterns of knowledge into the LLM to prevent catastrophic forgetting of previous knowledge and maintain its generative capability. We also conducted additional\nperplexity (PPL) evaluation experiments to illus\n\n\n16\n\n\n(a) LLaMA\n\n\n\n\n\n\n\n(b) KaLM\n\n\n\n\n\nFigure 7: Similarity matrix of selected similar entity descriptions from the WN8RR dataset.\n\n\nFigure 8: Selected entities and their corresponding textual descriptions.\n\n\n\ntrate the impact of the implicit knowledge alignment loss. The additional results show that for\n\nthe corresponding _\u03bb_ = 0 _,_ 0 _._ 01 _,_ 0 _._ 1 _,_ 1 _._ 0 in Table 6,\nthe model\u2019s PPL are **6.42**, _4.96_, _4.97_, and _4.98_,\nrespectively. Therefore, we can conclude that incorporating the implicit alignment loss maintains\nthe model\u2019s language modeling capability, whereas\nnot using the implicit alignment loss significantly\nimpairs the model\u2019s generative ability.\n\n\n**E.2** **The effects of fine-tuning different LLM**\n**modules using LoRA**\n\n\nIn Table 7, we fine-tune different modules of the\nmodel using the LoRA (Hu et al., 2021) method and\nanalyze their performance on KGC tasks and PPL\n\n\n\nTable 6: KGC results with different _\u03bb_ in Equation 4.\n\n|Method|WN18RR|PPL|\n|---|---|---|\n|**Method**|**MR**<br>**MRR**<br>**H@1**<br>**H@3**<br>**H@10**|**MR**<br>**MRR**<br>**H@1**<br>**H@3**<br>**H@10**|\n|KaLM (_\u03bb_ = 0)|21.2<br>0.512<br>0.355<br>0.611<br>0.815|6.42|\n|KaLM (_\u03bb_ = 0_._01)|**19.8**<br>0.510<br>0.352<br>0.604<br>0.818|4.96|\n|KaLM (_\u03bb_ = 0_._1)|20.1<br>**0.517**<br>**0.359**<br>**0.615**<br>**0.825**|4.98|\n|KaLM (_\u03bb_ = 1_._0)|21.6<br>0.500<br>0.336<br>0.596<br>0.806|4.98|\n\n\n\nevaluations. Note that this experiment is conducted\nsolely for ablation analysis, hence only 10 epochs\nof training were performed. \u201c _att_ \u201d indicates finetuning only the attention module, \u201c _ffn_ \u201d indicates\nfine-tuning only the feed-forward network, and \u201c _att-_\n_ffn_ \u201d indicates fine-tuning both the attention module\nand the feed-forward network simultaneously. The\n\n\n\n17\n\n\nFigure 9: layer-wise analysis of anisotropy. The vertical axis represents the sentence-level representation\nanisotropy value on the Wikitext-103 test set, while the\nhorizontal axis denotes the number of model layers.\n\n\nresults show that fine-tuning with the \u201c _att-ffn_ \u201d approach achieves the best KGC performance, but it\nalso leads to higher PPL values, suggesting that the\nmodel\u2019s generation capability may be significantly\ncompromised. Therefore, as a compromise, we\nchoose the \u201c _ffn_ \u201d fine-tuning approach, maintaining\nmoderate knowledge representation performance\nwhile preserving the original generation capability.\n\n\nThese experimental results are consistent with\nthe conclusions of (He et al., 2021), where the\nFFN learns local features and patterns within the\ninput sequence, allowing it to directly capture taskspecific text patterns. Meanwhile, attention provides the model with the ability to capture complex\ncontextual relationships, which is key to LLMs\u2019\nunderstanding and generation of natural language.\nUnder the knowledge-aligned language modeling\nobjective, we aim to align the internal knowledge\nrepresentations of LLMs while preserving their\ninherent natural language generation capabilities.\nTherefore, directly fine-tuning the FFN layers can\nreduce resource consumption and maximize the\neffectiveness of KaLM fine-tuning.\n\n\nTable 7: KGC results and PPL evaluation results when\n\nfine-tuning different network modules with LoRA.\n\n|Method|WN18RR<br>MR MRR H@1 H@3 H@10|PPL|\n|---|---|---|\n|KaLM (att)|21.9<br>0.47.5<br>0.331<br>0.580<br>0.784|5.03|\n|KaLM (ffn)|20.1<br>0.517<br>0.359<br>0.615<br>0.825|4.96|\n|KaLM (att-ffn)|**19.5**<br>**0.525**<br>**0.371**<br>**0.619**<br>**0.831**|5.07|\n\n\n\n18\n\n\n\nFigure 10: epoch-wise analysis of anisotropy. The vertical axis represents the sentence-level representation\nanisotropy value on the Wikitext-103 test set, while the\nhorizontal axis denotes the number of training epochs.\n\n\n**E.3** **The sustained gains and potential impacts**\n**of training for more epochs**\n\n\nIn Table 8, we fine-tune the model using different numbers of training epochs and analyze their\nperformance on KGC tasks. This experiment is\nmainly conducted to investigate whether additional\ntraining epochs can lead to further improvement\nin knowledge representations. The experimental\nresults show that using more training epochs can\ncontinuously improve the performance of KaLM on\nthe KGC task, resulting in higher MRR and Hit@k\nmetrics. The model trained with our method consis\ntently maintains an acceptable PPL value due to the\nimplicit knowledge alignment objective. However,\nthis also comes with more computational resource\nconsumption and training time. As a result, we\nselected a moderate number of training epochs.\n\n\nTable 8: KGC results with different training epochs.\n\n\n|Method|WN18RR|PPL|\n|---|---|---|\n|**Method**|**MR**<br>**MRR**<br>**H@1**<br>**H@3**<br>**H@10**|**MR**<br>**MRR**<br>**H@1**<br>**H@3**<br>**H@10**|\n|KaLM (epoch=10)|20.1<br>0.517<br>0.359<br>0.615<br>0.825|4.96|\n|KaLM (epoch=20)|**19.6**<br>0.554<br>0.402<br>0.650<br>0.848|4.98|\n|KaLM (epoch=30)|21.9<br>**0.576**<br>**0.427**<br>**0.673**<br>**0.854**|5.00|\n\n"}