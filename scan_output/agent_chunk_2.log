‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Crew Execution Started ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
C:\tools\Anaconda3\Lib\site-packages\paramiko\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
‚îÇ                                                                             ‚îÇ
  "cipher": algorithms.TripleDES,
‚îÇ  Crew Execution Started                                                     ‚îÇ
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.
‚îÇ  Name: crew                                                                 ‚îÇ
  "class": algorithms.Blowfish,
‚îÇ  ID: 0a5db0a9-3c18-41d7-9cd0-76e2dff3d9bd                                   ‚îÇ
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
‚îÇ                                                                             ‚îÇ
  "class": algorithms.TripleDES,
‚îÇ                                                                             ‚îÇ
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\_internal\_generate_schema.py:628: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  warn(

- Reading configuration...

  - Reading registry from registry.yaml

  - Reading catalog from docker-mcp.yaml

  - Reading config from config.yaml

- Configuration read in 28.605ms
üöÄ Crew: crew
- Watching registry at C:\Users\Noah\.docker\mcp\registry.yaml
‚îî‚îÄ‚îÄ üìã Task: ff80ee7f-6f66-4bdf-bef6-31148fd59faa
- Watching config at C:\Users\Noah\.docker\mcp\config.yaml
    Status: Executing Task...
- Those servers are enabled: arxiv-mcp-server, duckduckgo, wikipedia-mcp
    ‚îú‚îÄ‚îÄ üß† Reasoning...
- Using images:
    ‚îî‚îÄ‚îÄ ‚ùå LLM Failed‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ LLM Error ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  - mcp/arxiv-mcp-server@sha256:6dc6bba6dfed97f4ad6eb8d23a5c98ef5b7fa6184937d54b2d675801cd9dd29e
‚îÇ                                                                             ‚îÇ
  - mcp/duckduckgo@sha256:68eb20db6109f5c312a695fc5ec3386ad15d93ffb765a0b4eb1baf4328dec14f
‚îÇ  ‚ùå LLM Call Failed                                                         ‚îÇ
  - mcp/wikipedia-mcp@sha256:e90e3f6e3bb795f10d3d8d8ac1523f6d46d4f2ab5ac3b3a4720e1be879466472
‚îÇ  Error:           ‚îÇ
> Images pulled in 56.8694ms
‚îÇ  VertexAIException - {                                                      ‚îÇ
- Listing MCP tools...
‚îÇ    "error": {                                                               ‚îÇ
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
‚îÇ      "code": 503,                                                           ‚îÇ
  - Running mcp/duckduckgo with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=duckduckgo -l docker-mcp-transport=stdio]
‚îÇ      "message": "The model is overloaded. Please try again later.",         ‚îÇ
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers:ro]
‚îÇ      "status": "UNAVAILABLE"                                                ‚îÇ
  > duckduckgo: (2 tools)
‚îÇ    }                                                                        ‚îÇ
  > wikipedia-mcp: (10 tools) (9 resourceTemplates)
‚îÇ  }                                                                          ‚îÇ
  > arxiv-mcp-server: (4 tools) (1 prompts)
‚îÇ                                                                             ‚îÇ
> 16 tools listed in 2.8362806s
‚îÇ                                                                             ‚îÇ
- Watching for configuration updates...
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
> Initialized in 2.9222925s

> Start stdio server
üöÄ Crew: crew
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\fields.py:1093: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'items', 'anyOf', 'enum', 'properties'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
‚îî‚îÄ‚îÄ üìã Task: ff80ee7f-6f66-4bdf-bef6-31148fd59faa
  warn(
    Status: Executing Task...
  - Scanning tool call arguments for secrets...
    ‚îú‚îÄ‚îÄ ‚úÖ Reasoning Completed
  > No secret found in arguments.
    ‚îî‚îÄ‚îÄ ‚ùå LLM Failed‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üß† Reasoning Plan ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  - Calling tool download_paper with arguments: {"check_status":false,"paper_id":"2306.04050"}
‚îÇ                                                                             ‚îÇ
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
‚îÇ  ## Strategic Plan for LLMZip Document Synopsis                             ‚îÇ
  > Calling tool download_paper took: 1.6127694s
‚îÇ                                                                             ‚îÇ
  - Scanning tool call response for secrets...
‚îÇ  ### 1. Understanding of the Task                                           ‚îÇ
  > No secret found in response.
‚îÇ                                                                             ‚îÇ
  - Scanning tool call arguments for secrets...
‚îÇ  As an AI Assistant, I understand this task as a focused information        ‚îÇ
  > No secret found in arguments.
‚îÇ  retrieval and synthesis challenge. My goal is to act as a deterministic    ‚îÇ
  - Calling tool read_paper with arguments: {"paper_id":"2306.04050"}
‚îÇ  research agent, predicting the likely future content of a given research   ‚îÇ
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
‚îÇ  paper ("LLMZip: Lossless Text Compression using Large Language Models")    ‚îÇ
  > Calling tool read_paper took: 1.641775s
C:\tools\Anaconda3\Lib\site-packages\paramiko\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "cipher": algorithms.TripleDES,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.
  "class": algorithms.Blowfish,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "class": algorithms.TripleDES,
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\_internal\_generate_schema.py:628: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.
  warn(
- Reading configuration...
  - Reading registry from registry.yaml
  - Reading catalog from docker-mcp.yaml
  - Reading config from config.yaml
- Configuration read in 30.0486ms
- Watching registry at C:\Users\Noah\.docker\mcp\registry.yaml
- Watching config at C:\Users\Noah\.docker\mcp\config.yaml
- Those servers are enabled: arxiv-mcp-server, duckduckgo, wikipedia-mcp
- Using images:
  - mcp/arxiv-mcp-server@sha256:6dc6bba6dfed97f4ad6eb8d23a5c98ef5b7fa6184937d54b2d675801cd9dd29e
  - mcp/duckduckgo@sha256:68eb20db6109f5c312a695fc5ec3386ad15d93ffb765a0b4eb1baf4328dec14f
  - mcp/wikipedia-mcp@sha256:e90e3f6e3bb795f10d3d8d8ac1523f6d46d4f2ab5ac3b3a4720e1be879466472
> Images pulled in 59.6202ms
- Listing MCP tools...
  - Running mcp/duckduckgo with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=duckduckgo -l docker-mcp-transport=stdio]
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers:ro]
  > duckduckgo: (2 tools)
  > wikipedia-mcp: (10 tools) (9 resourceTemplates)
  > arxiv-mcp-server: (4 tools) (1 prompts)
> 16 tools listed in 3.1684106s
- Watching for configuration updates...
> Initialized in 3.2590793s
> Start stdio server
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\fields.py:1093: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'items', 'anyOf', 'enum', 'properties'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
  warn(
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Crew Execution Started ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ  Crew Execution Started                                                     ‚îÇ
‚îÇ  Name: crew                                                                 ‚îÇ
‚îÇ  ID: 3d93e766-cf04-4abf-a90c-c3a45c154582                                   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üöÄ Crew: crew
‚îú‚îÄ‚îÄ üìã Task: 0fb61fec-de9f-4d83-82e4-798b1c45aadd
‚îÇ   Status: Executing Task...
‚îÇ   ‚îî‚îÄ‚îÄ ‚úÖ Reasoning Completed
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îú‚îÄ‚îÄ üîÑ Tool Usage Started
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îî‚îÄ‚îÄ ‚úÖ Tool Usage Completed‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üß† Reasoning Plan ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ  1. Identify key information from the document prefix: Extract the paper    ‚îÇ
‚îÇ  title, authors, affiliation, arXiv ID, and core claims from the abstract   ‚îÇ
‚îÇ  and introduction. This includes the paper's main goal (better compression  ‚îÇ
‚îÇ  results and sharper entropy estimates using LLaMA-7B), specific entropy    ‚îÇ
‚îÇ  estimates (0.709 bits/character for 1MB text8, 0.85 bits/character for     ‚îÇ
‚îÇ  100KB from [7]), and compression ratios (0.7101 bits/character for 1MB     ‚îÇ
‚îÇ  text8, 0.8426 bits/character for 100KB from [7]), and the claim of         ‚îÇ
‚îÇ  outperforming BSC, ZPAQ, and paq8h.                                        ‚îÇ
‚îÇ  2. Search for the full paper using its arXiv ID: Use the `read_paper`      ‚îÇ
‚îÇ  tool with the arXiv ID "2306.04050" to access the complete document        ‚îÇ
‚îÇ  content.                                                                   ‚îÇ
‚îÇ  3. Extract the document structure and key facts: Utilize `get_sections`    ‚îÇ
‚îÇ  to obtain a list of all sections in the paper. Additionally, use           ‚îÇ
‚îÇ  `extract_key_facts` to pull out specific data, definitions, equations,     ‚îÇ
‚îÇ  and methodologies mentioned throughout the paper.                          ‚îÇ
‚îÇ  4. Synthesize the likely future content: Based on the extracted sections   ‚îÇ
‚îÇ  and key facts, formulate a concise textual synopsis. This synopsis will    ‚îÇ
‚îÇ  detail the methodology (e.g., how LLaMA-7B is integrated with arithmetic   ‚îÇ
‚îÇ  coding), the experimental setup (e.g., specific datasets like text8,       ‚îÇ
‚îÇ  evaluation metrics, and baselines for comparison), detailed results        ‚îÇ
‚îÇ  (e.g., tables or figures presenting entropy estimates and compression      ‚îÇ
‚îÇ  ratios), a discussion of these results, potential limitations, and future  ‚îÇ
‚îÇ  research directions. It will also include a mention of the references.     ‚îÇ
‚îÇ  5. Ensure conciseness and accuracy: Review the synopsis to ensure it is    ‚îÇ
‚îÇ  concise, directly relevant to the document, and only includes information  ‚îÇ
‚îÇ  explicitly supported by the retrieved content, avoiding any                ‚îÇ
‚îÇ  hallucination. The synopsis will be formatted to be suitable for           ‚îÇ
‚îÇ  conditioning a language model compressor.                                  ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool download_paper with arguments: {"check_status":false,"paper_id":"2306.04050"}
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
  > Calling tool download_paper took: 1.7297187s
  - Scanning tool call response for secrets...
  > No secret found in response.
[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Task:[00m [92mYou act as a deterministic research agent. Given the observed prefix of a document, use available MCP tools to retrieve information that is likely to appear later in the same document. Prefer authoritative sources. Produce a concise textual synopsis of the likely future content (facts, equations, definitions, datasets, references), suitable for conditioning a language model compressor. Maximize relevance to the document. Do not hallucinate; only include content supported by the tools.

Document prefix (UTF-8 text):
# LLMZip: Lossless Text Compression using Large Language Models

**Authors:** Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas Shakkottai  
**Affiliation:** Department of Electrical and Computer Engineering, Texas A&M University  
**Email:** {vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu  
**arXiv:** 2306.04050v2 [cs.IT] 26 Jun 2023

## Abstract

We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.

## I. INTRODUCTION

There are close connections between learning, prediction, and compression. The success of ChatGPT has captured the fascination of general public and brought the connection between learning and prediction to the fore. The main advance brought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in a paragraph based on knowing the past several words (tokens).

The connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the entropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past values can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many algorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of lossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very effective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor and every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression performance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved results for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ for text compression.

It is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the entropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this paper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the entropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates provided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of the text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for compression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression ratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio obtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.

## II. INTUITIVE EXPLANATION OF THE MAIN IDEA

We will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3] for estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable length and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence that reads as

My first attempt at writing a book

Our goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can be reconstructed from the sequence of bits. This sentence can first be split into a sequence of

Output: A concise synopsis (plain text).

Reasoning Plan:
1. Identify key information from the document prefix: Extract the paper title, authors, affiliation, arXiv ID, and core claims from the abstract and introduction. This includes the paper's main goal (better compression results and sharper entropy estimates using LLaMA-7B), specific entropy estimates (0.709 bits/character for 1MB text8, 0.85 bits/character for 100KB from [7]), and compression ratios (0.7101 bits/character for 1MB text8, 0.8426 bits/character for 100KB from [7]), and the claim of outperforming BSC, ZPAQ, and paq8h.
2. Search for the full paper using its arXiv ID: Use the `read_paper` tool with the arXiv ID "2306.04050" to access the complete document content.
3. Extract the document structure and key facts: Utilize `get_sections` to obtain a list of all sections in the paper. Additionally, use `extract_key_facts` to pull out specific data, definitions, equations, and methodologies mentioned throughout the paper.
4. Synthesize the likely future content: Based on the extracted sections and key facts, formulate a concise textual synopsis. This synopsis will detail the methodology (e.g., how LLaMA-7B is integrated with arithmetic coding), the experimental setup (e.g., specific datasets like text8, evaluation metrics, and baselines for comparison), detailed results (e.g., tables or figures presenting entropy estimates and compression ratios), a discussion of these results, potential limitations, and future research directions. It will also include a mention of the references.
5. Ensure conciseness and accuracy: Review the synopsis to ensure it is concise, directly relevant to the document, and only includes information explicitly supported by the retrieved content, avoiding any hallucination. The synopsis will be formatted to be suitable for conditioning a language model compressor.[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mAction: download_paper[00m
[95m## Using tool:[00m [92mdownload_paper[00m
[95m## Tool Input:[00m [92m
"{\"paper_id\": \"2306.04050\"}"[00m
[95m## Tool Output:[00m [92m
  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool get_sections with arguments: {"title":"LLMZip: Lossless Text Compression using Large Language Models"}
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
  > Calling tool get_sections took: 1.5521806s
  - Scanning tool call response for secrets...
  > No secret found in response.
  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool read_paper with arguments: {"paper_id":"2306.04050"}
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
  > Calling tool read_paper took: 1.6811246s
  - Scanning tool call response for secrets...
  > No secret found in response.
{"status": "success", "message": "Paper already available", "resource_uri": "file:///app/papers/2306.04050.md", "pdf_uri": "https://arxiv.org/pdf/2306.04050.pdf", "content": "1\n\n# LLMZip: Lossless Text Compression using Large Language Models\n\n\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\n\n\nAbstract\n\n\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\n\n\nI. I NTRODUCTION\n\n\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\n\n\nII. I NTUITIVE EXPLANATION OF THE MAIN IDEA\n\n\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\n\nMy first attempt at writing a book\n\n\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032, \u2032 a \u2032, \u2032 book \u2032\n\n\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n\n\n2\n\n\nFigure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be\nwords such as [\u2032] reading [\u2032], [\u2032] writing [\u2032], [\u2032] driving [\u2032], [\u2032] cooking [\u2032] etc. The main idea is to compute the rank of the actual word in\nour sentence ( [\u2032] writing [\u2032] ) in this list and call it R 5 . We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for [\u2032] writing [\u2032] is R 5 = 1.\n\n\nTokens\n\n\n\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x5|x4 1)<br>5|Rank<br>computation|R5 = 1|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nreading 0.3\nwriting 0.2\ncycling 0.1\ndriving 0.05\n\n... ...\n\n\n\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\n\n\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, [\u2032] a [\u2032], and hence, the rank R 6 would be 0.\n\n\nTokens\n\n\n\n\n\n\u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x6|x5 2)<br>6|Rank<br>computation|R6 = 0|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nan 0.2\n\nthe 0.05\n\n... ...\n\n\n\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\n\n\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n\n\n1, 0, 0, . . .\n\n\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\n\n\nSequence of ranks\n\n|e of ranks|Col2|Col3|\n|---|---|---|\n|. ., rNT|Compression (zip)|Nb|\n||||\n\n\n\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\n\n\nWhen we wish to reconstruct the sequence, we first decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank R i during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\n\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n\n\n3\n\n\nIII. C OMPRESSION USING LLM S\n\n\nLet s denote a sentence from the English language composed of N c letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We first parse s into a sequence of N T tokens\ndenoted by x = x 1, x 2, . . ., x i\u22121, x i, x i+1, . . . x N T, where x i \u2208X . There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. x i \u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter X i .\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens x i\u2212M, x i\u2212M+1, . . ., x i\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nq i (x i ) := Pr(X i = x i |x i\u22121, x i\u22122, . . ., x i\u2212M ), \u2200x i \u2208X . The PMF vector q i := [q i (1), q i (2), . . ., q i (D)] [T] is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcq i . Let \u03b3 i : X \u2192X be a permutation on the integers\nfrom 1 to D such that\n\n\u02dc\nq i (\u03b3 i (j)) = q i (j), \u2200j \u2208X .\n\n\nThat is, \u03b3 i (j) is the rank of the token j at epoch i. We define the rank of the input sequence at epoch i as the rank of the\ntoken x i at epoch i, r i := \u03b3 i (x i ). The sequence {r i } [N] i=1 [T] [is compressed by a lossless compression algorithm (such as zlib) to]\nproduce N b bits which are the final bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors q i \u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 defined as\n\n\u03c1 := [N] [b] bits/character.\n\nN c\n\n\n\nLosseless\n\nCompression\n\n\n\nTokens . . ., x i\u2212M, . . ., x i\u22121\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n\nInput sentence s with N c characters\n\n\n\ni i i\u22121 Rank\n\nLLM\ncomputation\n\n\n\nq i = Pr(X i = j|x [i] i [\u2212] \u2212 [M] 1 [)] Rank r i Losseless N b bits\n\n\n\nFig. 4. Schematic showing the prediction at epoch i.\n\n\nA. Entropy bounds\n\n\nLet S \u2208S [\u221e] be a random process that represents language input. The nth character in the sequence is denoted by S n,\nwhereas the string of characters from the beginning to the nth character is expressed as S n . The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X 1, X 2, . . . using a variable-length mapping. In this sequence, X i is the ith\ntoken. The number of characters employed to generate X i depends on the realization of the random process and, as such,\nwe introduce random variable B i to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which B i \u2265 1 and B i is uniformly bounded, with B i < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded N T \u2192\u221e, the number of characters must also approach infinity N c \u2192\u221e. Formally, consider the tokenizer\nfunction T : S [N] \u2192X [N] operating on infinite symbol sequences; that is, T (s) = x where s is an infinite sequence in S [\u221e] . For\nnatural number, i \u2208 N, define m i : S [N] \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Specifically, suppose s is given, then\n\n\nm i (s) = min n [{][length (][T][ (][s] [n] [))][ \u2265] [i][}][ .] (1)\n\n\nWe note that, by construction, lim n\u2192\u221e length (T (s n )) = \u221e and, as such, m i (\u00b7) is well-defined. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple finite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (s n ) = T (s n+1 ). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (finite) series of input symbols in the restricted\nset for which T (y n ) = x i is s m i (s) . Given a fixed sequence s, we can express the number of characters contained in a token\n\nas\nb i = m i (s) \u2212 m i\u22121 (s)\n\n\n4\n\n\nwith initial condition m \u22121 = 0. Consequently, the number of characters embedded in the first N T tokens for a random input\nbecomes N c = [\ufffd] i [N] =1 [T] [B] [i] [.]\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {S k } [\u221e] k=1 [,]\n{B i } [\u221e] i=1 [, and][ {][X] [i] [}] [\u221e] i=1 [are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that]\n\n\n\n\u2212 [1]\n\n\n\n\n[1]\n\nn [log] [2] [ p] [S] [n] [(][S] [1] [, . . ., S] [n] [) =][ \u2212] n [1]\n\n\n\nn [log] [2] [ p] [S] [n] [(][S] [n] [)][ \u2192] [H][(][S][)] almost surely. (2)\n\n\n\nLet \u2126 S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\n\n\n\n[1]\n\ni [log] [2] [ p] [X] [i] [(][X] [1] [, . . ., X] [i] [) =][ \u2212] [1] i\n\n\n\n\u2212 [1]\n\n\n\ni [log] [2] [ p] [X] [i] [(][X] [i] [)][ \u2192] [H][(][X][)] almost surely. (3)\n\n\n\nDefine \u2126 X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\n\n\nm i (S)\nlim = E [B] almost surely. (4)\ni\u2192\u221e i\n\n\nSet \u2126 B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126 S \u2229 \u2126 X \u2229 \u2126 B, we deduce that\n\n\nH(S) = lim\nk\u2192\u221e [\u2212] k [1] [log] [2] [ p] [S] [k] [(][S] [k] [(][\u03c9][))]\n\n= lim i\u2192\u221e [\u2212] l [1] i log 2 p S li (S l i (\u03c9))\n\n\n= lim log 2 Pr (X i = T (S l i (\u03c9)))\ni\u2192\u221e [\u2212] l [1] i\n\n\n\n1\n\ni [log] [2] [ Pr (][X] [i] [ =][ x] [i] [) =][ H] E [(] B [X][)]\n\n\n\n1 1\n\n= \u2212\nE[B] i [lim] \u2192\u221e i\n\n\n\nE[B] [.]\n\n\n\nThe first equality follows from (2). The second equality is a consequence of the fact that {l i = m i (S(\u03c9))|i \u2208 N} is an infinite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\n\ntwo events,\n{\u03c9 \u2208 \u2126|X i (\u03c9) = x i } = {\u03c9 \u2208 \u2126|T (S m i (S(\u03c9)) ) = x i }.\n\n\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126 B . The sets \u2126 S, \u2126 X, and \u2126 B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\n\nH(S) = [H][(][X][)] almost surely.\n\nE[B]\n\n\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\n\n\n\nPr\n\n\ufffd\n\n\n\n\ufffd\n\n\n\nH(X) \u2264 lim\nN T \u2192\u221e [\u2212] N [1] T\n\n\n\nN T\n\ufffd\n\n\n\n\ufffd log 2 q i (X i )\n\n\ni=1\n\n\n\n= 1, (5)\n\n\n\nwhere q i (\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\n\n\n\n\n[1] N T\n\nN T \ufffd i=1 [log] 2 [q] [i] [(][X] [i] [)]\n\n\n\nH(S) \u2264 lim N T \u2192\u221e \u2212 N [1]\n\n\n\n. (6)\nE[B]\n\n\n\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by H ub . The\nnumerator in (6) represents the average number of bits required to represent the tokens X N T and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\n\n\nB. Encoding schemes\n\n\nWe consider three schemes for the lossless compression block in Fig. 3.\n\n\n5\n\n\n1) Compressing the ranks using zlib: The first scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1 LLaMA+zlib .\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token x i at epoch i by using a prefix-free code assuming q i to be the true distribution of\nthe tokens. A natural choice for a prefix-free code is a Huffman code. Instead, for simplicity, we use a prefix-free code where\n1\nthe codeword for the token x i is of length l i = \u2308log 2 q i (x i ) [\u2309][. A prefix-free code with this length for][ x] [i] [ is guaranteed to exist]\nsince this choice of lengths satisfies the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1 LLaMA+TbyT,\nis given by\n\n\n\nN T\n\ufffd\n\n\ni=1\n\n\n\n1\nlog 2\n\ufffd q i (x i )\n\n\n\n\ufffd\n\n\n\n\u03c1 LLaMA+TbyT =\n\n\n\n\ufffd Ni=1 T [b] [i] .\n\n\n\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use q i (x i ) as the probability of token x i at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1 LLM+AC . It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\n\n\n\u03c1 LLM+AC \u2248\n\n\n\nN T 1\n\ufffd i=1 log 2 q i (x i )\n\n\ufffd Ni=1 T [b] [i] . (7)\n\n\n\nClearly, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and \u03c1 LLM+AC also provide upper bounds on H(S). H ub, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and\n\u03c1 LLM+AC are estimated using a finite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\n\n\nIV. R ESULTS\n\n\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it difficult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\n[Nevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of](http://mattmahoney.net/dc/text8.zip)\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M ) is shown in Table II. As expected, the\ncompression performance improves with increasing M . We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n\n\n6\n\n\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\n\n\nV. A CKNOWLEDGEMENT\n\n\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\n\n\nTABLE I\n\nR ESULTS FOR 1MB OF TEXT FROM TEXT 8 DATASET\n\n\n\nBatch\nN c N T\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC ZPAQ pq8h\n\n\n\nNo. (bpc) file size (bits) (bpc) (bpc) (bpc) (bpc)\n\n\n1 466, 650 100, 000 0.6882 1.0513 0.8215 0.689\n\n\n2 461, 477 100, 000 0.6893 1.0558 0.8242 0.6901\n\n\n3 454, 599 100, 000 0.699 1.0681 0.8357 0.6999\n\n\n4 462, 755 100, 000 0.6748 1.0346 0.8093 0.6757\n\n\n5 453, 847 100, 000 0.7481 1.1265 0.8831 0.749\n\n\n6 458, 252 100, 000 0.7218 1.0957 0.8567 0.7227\n\n\n7 451, 036 100, 000 0.6959 1.0729 0.8353 0.6968\n\n\n8 447, 953 100, 000 0.7092 1.0896 0.8489 0.7101\n\n\n9 462, 665 100, 000 0.7394 1.1126 0.8713 0.7402\n\n\n10 449, 621 100, 000 0.7269 1.1046 0.8643 0.7277\n\n\nTotal 9, 137, 710 2, 000, 000 0.7093 1.0812 0.845 0.7101 1.4 [1] 1.2 [2]\n\n\nTABLE II\n\nC OMPRESSION PERFORMANCE OF THE LLM ON THE TEXT 8 DATASET, AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\n\n\n(bpc) file size (bits) (bpc) (bpc)\n\n\n\n31 4, 568, 855 1, 000, 000 0.9139 1.3159 1.0425 0.9145\n\n\n127 4, 568, 855 1, 000, 000 0.7511 1.1303 0.8847 0.752\n\n\n255 4, 568, 855 1, 000, 000 0.7242 1.0985 0.859 0.725\n\n\n511 4, 568, 855 1, 000, 000 0.7093 1.0812 0.845 0.7101\n\n\n1 This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2 This result is taken from [12] and it corresponds to the full 100MB dataset text8\n\n\n7\n\n\n\nTABLE III\n\nM EAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\nM\n\n(bpc) (bpc) (bpc) (bpc)\n\n\n31 0.9139 \u00b1 0.0263 1.3159 \u00b1 0.0329 1.0425 \u00b1 0.0262 0.9145 \u00b1 0.0263\n\n\n127 0.7511 \u00b1 0.0233 1.1303 \u00b1 0.0292 0.8847 \u00b1 0.0231 0.752 \u00b1 0.0233\n\n\n255 0.7242 \u00b1 0.0234 1.0985 \u00b1 0.0289 0.859 \u00b1 0.0232 0.725 \u00b1 0.0234\n\n\n511 0.7093 \u00b1 0.0228 1.0812 \u00b1 0.028 0.845 \u00b1 0.0226 0.7101 \u00b1 0.0228\n\n\nTABLE IV\n\nC OMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN P ROJECT G UTENBERG [7], AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC Standalone Zlib\n\n\n(bpc) (bpc) (bpc) (bpc) (bpc)\n\n\n\n31 508, 463 115, 000 1.0919 1.5316 1.2152 1.0924 2.80\n\n\n127 508, 463 115, 000 0.8973 1.3128 1.0235 0.8982 2.80\n\n\n255 508, 463 115, 000 0.8618 1.2684 0.9899 0.8627 2.80\n\n\n511 508, 463 115, 000 0.8417 1.2465 0.9711 0.8426 2.80\n\n\n8\n\n\nR EFERENCES\n\n\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR filters),\u201d 2023.\n\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and efficient foundation language models,\u201d\n2023.\n\n[7] J. Frank Dobie, Legends of Texas, United States, Texas Folk-Lore Society, 1924; Project Gutenberg, May 25, 2023, 2023,\n[https://www.gutenberg.org/ebooks/70859.](https://www.gutenberg.org/ebooks/70859)\n\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n\n\n"}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mThought: The paper "LLMZip: Lossless Text Compression using Large Language Models" (arXiv:2306.04050) has been successfully downloaded. I now have access to its full content. The next step is to extract the document structure (sections) and key facts to create a synopsis of its likely future content. I will use `get_sections` to get the section titles and then synthesize the information.[00m
[95m## Using tool:[00m [92mget_sections[00m
[95m## Tool Input:[00m [92m
"{\"title\": \"LLMZip: Lossless Text Compression using Large Language Models\"}"[00m
[95m## Tool Output:[00m [92m
{"title":"LLMZip: Lossless Text Compression using Large Language Models","sections":[]}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mThought: The `get_sections` tool returned an empty list for the title "LLMZip: Lossless Text Compression using Large Language Models". This might be because the tool expects a Wikipedia article title, not an arXiv paper title. Since I have already downloaded the paper and have its full content, I can directly read the content and extract the sections and other relevant information from the markdown text. I will now read the paper content and manually identify the sections and key information to create the synopsis.[00m
[95m## Using tool:[00m [92mread_paper[00m
[95m## Tool Input:[00m [92m
"{\"paper_id\": \"2306.04050\"}"[00m
[95m## Tool Output:[00m [92m
{"status": "success", "paper_id": "2306.04050", "content": "1\n\n# LLMZip: Lossless Text Compression using Large Language Models\n\n\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\n\n\nAbstract\n\n\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\n\n\nI. I NTRODUCTION\n\n\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\n\n\nII. I NTUITIVE EXPLANATION OF THE MAIN IDEA\n\n\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\n\nMy first attempt at writing a book\n\n\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032, \u2032 a \u2032, \u2032 book \u2032\n\n\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n\n\n2\n\n\nFigure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be\nwords such as [\u2032] reading [\u2032], [\u2032] writing [\u2032], [\u2032] driving [\u2032], [\u2032] cooking [\u2032] etc. The main idea is to compute the rank of the actual word in\nour sentence ( [\u2032] writing [\u2032] ) in this list and call it R 5 . We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for [\u2032] writing [\u2032] is R 5 = 1.\n\n\nTokens\n\n\n\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x5|x4 1)<br>5|Rank<br>computation|R5 = 1|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nreading 0.3\nwriting 0.2\ncycling 0.1\ndriving 0.05\n\n... ...\n\n\n\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\n\n\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, [\u2032] a [\u2032], and hence, the rank R 6 would be 0.\n\n\nTokens\n\n\n\n\n\n\u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x6|x5 2)<br>6|Rank<br>computation|R6 = 0|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nan 0.2\n\nthe 0.05\n\n... ...\n\n\n\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\n\n\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n\n\n1, 0, 0, . . .\n\n\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\n\n\nSequence of ranks\n\n|e of ranks|Col2|Col3|\n|---|---|---|\n|. ., rNT|Compression (zip)|Nb|\n||||\n\n\n\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\n\n\nWhen we wish to reconstruct the sequence, we first decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank R i during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\n\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n\n\n3\n\n\nIII. C OMPRESSION USING LLM S\n\n\nLet s denote a sentence from the English language composed of N c letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We first parse s into a sequence of N T tokens\ndenoted by x = x 1, x 2, . . ., x i\u22121, x i, x i+1, . . . x N T, where x i \u2208X . There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. x i \u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter X i .\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens x i\u2212M, x i\u2212M+1, . . ., x i\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nq i (x i ) := Pr(X i = x i |x i\u22121, x i\u22122, . . ., x i\u2212M ), \u2200x i \u2208X . The PMF vector q i := [q i (1), q i (2), . . ., q i (D)] [T] is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcq i . Let \u03b3 i : X \u2192X be a permutation on the integers\nfrom 1 to D such that\n\n\u02dc\nq i (\u03b3 i (j)) = q i (j), \u2200j \u2208X .\n\n\nThat is, \u03b3 i (j) is the rank of the token j at epoch i. We define the rank of the input sequence at epoch i as the rank of the\ntoken x i at epoch i, r i := \u03b3 i (x i ). The sequence {r i } [N] i=1 [T] [is compressed by a lossless compression algorithm (such as zlib) to]\nproduce N b bits which are the final bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors q i \u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 defined as\n\n\u03c1 := [N] [b] bits/character.\n\nN c\n\n\n\nLosseless\n\nCompression\n\n\n\nTokens . . ., x i\u2212M, . . ., x i\u22121\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n\nInput sentence s with N c characters\n\n\n\ni i i\u22121 Rank\n\nLLM\ncomputation\n\n\n\nq i = Pr(X i = j|x [i] i [\u2212] \u2212 [M] 1 [)] Rank r i Losseless N b bits\n\n\n\nFig. 4. Schematic showing the prediction at epoch i.\n\n\nA. Entropy bounds\n\n\nLet S \u2208S [\u221e] be a random process that represents language input. The nth character in the sequence is denoted by S n,\nwhereas the string of characters from the beginning to the nth character is expressed as S n . The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X 1, X 2, . . . using a variable-length mapping. In this sequence, X i is the ith\ntoken. The number of characters employed to generate X i depends on the realization of the random process and, as such,\nwe introduce random variable B i to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which B i \u2265 1 and B i is uniformly bounded, with B i < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded N T \u2192\u221e, the number of characters must also approach infinity N c \u2192\u221e. Formally, consider the tokenizer\nfunction T : S [N] \u2192X [N] operating on infinite symbol sequences; that is, T (s) = x where s is an infinite sequence in S [\u221e] . For\nnatural number, i \u2208 N, define m i : S [N] \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Specifically, suppose s is given, then\n\n\nm i (s) = min n [{][length (][T][ (][s] [n] [))][ \u2265] [i][}][ .] (1)\n\n\nWe note that, by construction, lim n\u2192\u221e length (T (s n )) = \u221e and, as such, m i (\u00b7) is well-defined. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple finite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (s n ) = T (s n+1 ). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (finite) series of input symbols in the restricted\nset for which T (y n ) = x i is s m i (s) . Given a fixed sequence s, we can express the number of characters contained in a token\n\nas\nb i = m i (s) \u2212 m i\u22121 (s)\n\n\n4\n\n\nwith initial condition m \u22121 = 0. Consequently, the number of characters embedded in the first N T tokens for a random input\nbecomes N c = [\ufffd] i [N] =1 [T] [B] [i] [.]\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {S k } [\u221e] k=1 [,]\n{B i } [\u221e] i=1 [, and][ {][X] [i] [}] [\u221e] i=1 [are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that]\n\n\n\n\u2212 [1]\n\n\n\n\n[1]\n\nn [log] [2] [ p] [S] [n] [(][S] [1] [, . . ., S] [n] [) =][ \u2212] n [1]\n\n\n\nn [log] [2] [ p] [S] [n] [(][S] [n] [)][ \u2192] [H][(][S][)] almost surely. (2)\n\n\n\nLet \u2126 S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\n\n\n\n[1]\n\ni [log] [2] [ p] [X] [i] [(][X] [1] [, . . ., X] [i] [) =][ \u2212] [1] i\n\n\n\n\u2212 [1]\n\n\n\ni [log] [2] [ p] [X] [i] [(][X] [i] [)][ \u2192] [H][(][X][)] almost surely. (3)\n\n\n\nDefine \u2126 X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\n\n\nm i (S)\nlim = E [B] almost surely. (4)\ni\u2192\u221e i\n\n\nSet \u2126 B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126 S \u2229 \u2126 X \u2229 \u2126 B, we deduce that\n\n\nH(S) = lim\nk\u2192\u221e [\u2212] k [1] [log] [2] [ p] [S] [k] [(][S] [k] [(][\u03c9][))]\n\n= lim i\u2192\u221e [\u2212] l [1] i log 2 p S li (S l i (\u03c9))\n\n\n= lim log 2 Pr (X i = T (S l i (\u03c9)))\ni\u2192\u221e [\u2212] l [1] i\n\n\n\n1\n\ni [log] [2] [ Pr (][X] [i] [ =][ x] [i] [) =][ H] E [(] B [X][)]\n\n\n\n1 1\n\n= \u2212\nE[B] i [lim] \u2192\u221e i\n\n\n\nE[B] [.]\n\n\n\nThe first equality follows from (2). The second equality is a consequence of the fact that {l i = m i (S(\u03c9))|i \u2208 N} is an infinite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\n\ntwo events,\n{\u03c9 \u2208 \u2126|X i (\u03c9) = x i } = {\u03c9 \u2208 \u2126|T (S m i (S(\u03c9)) ) = x i }.\n\n\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126 B . The sets \u2126 S, \u2126 X, and \u2126 B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\n\nH(S) = [H][(][X][)] almost surely.\n\nE[B]\n\n\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\n\n\n\nPr\n\n\ufffd\n\n\n\n\ufffd\n\n\n\nH(X) \u2264 lim\nN T \u2192\u221e [\u2212] N [1] T\n\n\n\nN T\n\ufffd\n\n\n\n\ufffd log 2 q i (X i )\n\n\ni=1\n\n\n\n= 1, (5)\n\n\n\nwhere q i (\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\n\n\n\n\n[1] N T\n\nN T \ufffd i=1 [log] 2 [q] [i] [(][X] [i] [)]\n\n\n\nH(S) \u2264 lim N T \u2192\u221e \u2212 N [1]\n\n\n\n. (6)\nE[B]\n\n\n\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by H ub . The\nnumerator in (6) represents the average number of bits required to represent the tokens X N T and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\n\n\nB. Encoding schemes\n\n\nWe consider three schemes for the lossless compression block in Fig. 3.\n\n\n5\n\n\n1) Compressing the ranks using zlib: The first scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1 LLaMA+zlib .\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token x i at epoch i by using a prefix-free code assuming q i to be the true distribution of\nthe tokens. A natural choice for a prefix-free code is a Huffman code. Instead, for simplicity, we use a prefix-free code where\n1\nthe codeword for the token x i is of length l i = \u2308log 2 q i (x i ) [\u2309][. A prefix-free code with this length for][ x] [i] [ is guaranteed to exist]\nsince this choice of lengths satisfies the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1 LLaMA+TbyT,\nis given by\n\n\n\nN T\n\ufffd\n\n\ni=1\n\n\n\n1\nlog 2\n\ufffd q i (x i )\n\n\n\n\ufffd\n\n\n\n\u03c1 LLaMA+TbyT =\n\n\n\n\ufffd Ni=1 T [b] [i] .\n\n\n\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use q i (x i ) as the probability of token x i at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1 LLM+AC . It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\n\n\n\u03c1 LLM+AC \u2248\n\n\n\nN T 1\n\ufffd i=1 log 2 q i (x i )\n\n\ufffd Ni=1 T [b] [i] . (7)\n\n\n\nClearly, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and \u03c1 LLM+AC also provide upper bounds on H(S). H ub, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and\n\u03c1 LLM+AC are estimated using a finite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\n\n\nIV. R ESULTS\n\n\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it difficult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\n[Nevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of](http://mattmahoney.net/dc/text8.zip)\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M ) is shown in Table II. As expected, the\ncompression performance improves with increasing M . We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n\n\n6\n\n\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\n\n\nV. A CKNOWLEDGEMENT\n\n\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\n\n\nTABLE I\n\nR ESULTS FOR 1MB OF TEXT FROM TEXT 8 DATASET\n\n\n\nBatch\nN c N T\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC ZPAQ pq8h\n\n\n\nNo. (bpc) file size (bits) (bpc) (bpc) (bpc) (bpc)\n\n\n1 466, 650 100, 000 0.6882 1.0513 0.8215 0.689\n\n\n2 461, 477 100, 000 0.6893 1.0558 0.8242 0.6901\n\n\n3 454, 599 100, 000 0.699 1.0681 0.8357 0.6999\n\n\n4 462, 755 100, 000 0.6748 1.0346 0.8093 0.6757\n\n\n5 453, 847 100, 000 0.7481 1.1265 0.8831 0.749\n\n\n6 458, 252 100, 000 0.7218 1.0957 0.8567 0.7227\n\n\n7 451, 036 100, 000 0.6959 1.0729 0.8353 0.6968\n\n\n8 447, 953 100, 000 0.7092 1.0896 0.8489 0.7101\n\n\n9 462, 665 100, 000 0.7394 1.1126 0.8713 0.7402\n\n\n10 449, 621 100, 000 0.7269 1.1046 0.8643 0.7277\n\n\nTotal 9, 137, 710 2, 000, 000 0.7093 1.0812 0.845 0.7101 1.4 [1] 1.2 [2]\n\n\nTABLE II\n\nC OMPRESSION PERFORMANCE OF THE LLM ON THE TEXT 8 DATASET, AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\n\n\n(bpc) file size (bits) (bpc) (bpc)\n\n\n\n31 4, 568, 855 1, 000, 000 0.9139 1.3159 1.0425 0.9145\n\n\n127 4, 568, 855 1, 000, 000 0.7511 1.1303 0.8847 0.752\n\n\n255 4, 568, 855 1, 000, 000 0.7242 1.0985 0.859 0.725\n\n\n511 4, 568, 855 1, 000, 000 0.7093 1.0812 0.845 0.7101\n\n\n1 This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2 This result is taken from [12] and it corresponds to the full 100MB dataset text8\n\n\n7\n\n\n\nTABLE III\n\nM EAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\nM\n\n(bpc) (bpc) (bpc) (bpc)\n\n\n31 0.9139 \u00b1 0.0263 1.3159 \u00b1 0.0329 1.0425 \u00b1 0.0262 0.9145 \u00b1 0.0263\n\n\n127 0.7511 \u00b1 0.0233 1.1303 \u00b1 0.0292 0.8847 \u00b1 0.0231 0.752 \u00b1 0.0233\n\n\n255 0.7242 \u00b1 0.0234 1.0985 \u00b1 0.0289 0.859 \u00b1 0.0232 0.725 \u00b1 0.0234\n\n\n511 0.7093 \u00b1 0.0228 1.0812 \u00b1 0.028 0.845 \u00b1 0.0226 0.7101 \u00b1 0.0228\n\n\nTABLE IV\n\nC OMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN P ROJECT G UTENBERG [7], AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC Standalone Zlib\n\n\n(bpc) (bpc) (bpc) (bpc) (bpc)\n\n\n\n31 508, 463 115, 000 1.0919 1.5316 1.2152 1.0924 2.80\n\n\n127 508, 463 115, 000 0.8973 1.3128 1.0235 0.8982 2.80\n\n\n255 508, 463 115, 000 0.8618 1.2684 0.9899 0.8627 2.80\n\n\n511 508, 463 115, 000 0.8417 1.2465 0.9711 0.8426 2.80\n\n\n8\n\n\nR EFERENCES\n\n\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR filters),\u201d 2023.\n\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and efficient foundation language models,\u201d\n2023.\n\n[7] J. Frank Dobie, Legends of Texas, United States, Texas Folk-Lore Society, 1924; Project Gutenberg, May 25, 2023, 2023,\n[https://www.gutenberg.org/ebooks/70859.](https://www.gutenberg.org/ebooks/70859)\n\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n\n\n"}


You ONLY have access to the following tools, and should NEVER make up tools that are not listed here:

Tool Name: download_paper
Tool Arguments: {'properties': {'check_status': {'anyOf': [], 'default': False, 'description': 'Set to true to only check the status of an ongoing or completed conversion without starting a new download. Use this to monitor long-running conversions or verify if a paper is already available.', 'enum': None, 'items': None, 'properties': {}, 'title': ''}, 'paper_id': {'anyOf': [], 'description': "The arXiv identifier of the paper to download (e.g., '2301.07041', '1706.03762', 'cs.AI/0301001'). This can be found in search results or arXiv URLs. The paper must exist on arXiv.", 'enum': None, 'items': None, 'properties': {}, 'title': '', 'type': 'string'}}, 'required': ['paper_id'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Download and convert an arXiv paper to readable markdown format for analysis and reading. This tool fetches the PDF from arXiv, converts it to markdown using advanced text extraction, and stores it locally for immediate access. Use this tool when you need to read, analyze, or work with the full text content of a specific paper. The conversion process extracts text, preserves formatting, and handles mathematical equations. Returns the full paper content directly upon successful completion.
Tool Name: extract_key_facts
Tool Arguments: {'properties': {'count': {'anyOf': [], 'default': 5, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Count'}, 'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}, 'topic_within_article': {'anyOf': [], 'default': '', 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Topic Within Article'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Extract key facts from a Wikipedia article, optionally focused on a topic.
Tool Name: fetch_content
Tool Arguments: {'properties': {'url': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Url', 'type': 'string'}}, 'required': ['url'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: 
    Fetch and parse content from a webpage URL.

    Args:
        url: The webpage URL to fetch content from
        ctx: MCP context for logging
    
Tool Name: get_article
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get the full content of a Wikipedia article.
Tool Name: get_coordinates
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get the coordinates of a Wikipedia article.
Tool Name: get_links
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get the links contained within a Wikipedia article.
Tool Name: get_related_topics
Tool Arguments: {'properties': {'limit': {'anyOf': [], 'default': 10, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Limit'}, 'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get topics related to a Wikipedia article based on links and categories.
Tool Name: get_sections
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get the sections of a Wikipedia article.
Tool Name: get_summary
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get a summary of a Wikipedia article.
Tool Name: list_papers
Tool Arguments: {'properties': {}, 'title': 'DynamicModel', 'type': 'object'}
Tool Description: List all previously downloaded and converted papers that are available in local storage for immediate reading and analysis. This tool shows you what papers you already have access to without needing to download them again. Each paper in the list includes metadata like title, authors, abstract, and direct links. Use this tool to see your paper library, check if a specific paper is already downloaded, or browse previously acquired research papers before downloading new ones.
Tool Name: read_paper
Tool Arguments: {'properties': {'paper_id': {'anyOf': [], 'description': "The arXiv identifier of the paper to read (e.g., '2301.07041', '1706.03762'). This must be a paper that has been previously downloaded and converted to markdown format. Use list_papers to see available papers.", 'enum': None, 'items': None, 'properties': {}, 'title': '', 'type': 'string'}}, 'required': ['paper_id'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Read the full text content of a previously downloaded and converted research paper in clean markdown format. This tool retrieves the complete paper content including abstract, introduction, methodology, results, conclusions, and references. The content is formatted for easy reading and analysis, with preserved mathematical equations and structured sections. Use this tool when you need to access the full text of a paper for detailed study, quotation, analysis, or research. The paper must have been previously downloaded using the download_paper tool.
Tool Name: search
Tool Arguments: {'properties': {'max_results': {'anyOf': [], 'default': 10, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Max Results'}, 'query': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Query', 'type': 'string'}}, 'required': ['query'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: 
    Search DuckDuckGo and return formatted results.

    Args:
        query: The search query string
        max_results: Maximum number of results to return (default: 10)
        ctx: MCP context for logging
    
Tool Name: search_papers
Tool Arguments: {'properties': {'categories': {'anyOf': [], 'default': None, 'description': "Strongly recommended: arXiv categories to focus search (e.g., ['cs.AI', 'cs.MA'] for agent research, ['cs.LG'] for ML, ['cs.CL'] for NLP, ['cs.CV'] for vision). Greatly improves relevance.", 'enum': None, 'items': {'type': 'string'}, 'properties': {}, 'title': ''}, 'date_from': {'anyOf': [], 'default': None, 'description': "Start date for papers (YYYY-MM-DD format). Use to find recent work, e.g., '2023-01-01' for last 2 years.", 'enum': None, 'items': None, 'properties': {}, 'title': ''}, 'date_to': {'anyOf': [], 'default': None, 'description': "End date for papers (YYYY-MM-DD format). Use with date_from to find historical work, e.g., '2020-12-31' for older research.", 'enum': None, 'items': None, 'properties': {}, 'title': ''}, 'max_results': {'anyOf': [], 'default': None, 'description': 'Maximum number of results to return (default: 10, max: 50). Use 15-20 for comprehensive searches.', 'enum': None, 'items': None, 'properties': {}, 'title': ''}, 'query': {'anyOf': [], 'description': 'Search query using quoted phrases for exact matches (e.g., \'"machine learning" OR "deep learning"\') or specific technical terms. Avoid overly broad or generic terms.', 'enum': None, 'items': None, 'properties': {}, 'title': '', 'type': 'string'}, 'sort_by': {'anyOf': [], 'default': None, 'description': "Sort results by 'relevance' (most relevant first, default) or 'date' (newest first). Use 'relevance' for focused searches, 'date' for recent developments.", 'enum': ['relevance', 'date'], 'items': None, 'properties': {}, 'title': ''}}, 'required': ['query'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Search for papers on arXiv with advanced filtering and query optimization.

QUERY CONSTRUCTION GUIDELINES:
- Use QUOTED PHRASES for exact matches: "multi-agent systems", "neural networks", "machine learning"
- Combine related concepts with OR: "AI agents" OR "software agents" OR "intelligent agents"  
- Use field-specific searches for precision:
  - ti:"exact title phrase" - search in titles only
  - au:"author name" - search by author
  - abs:"keyword" - search in abstracts only
- Use ANDNOT to exclude unwanted results: "machine learning" ANDNOT "survey"
- For best results, use 2-4 core concepts rather than long keyword lists

ADVANCED SEARCH PATTERNS:
- Field + phrase: ti:"transformer architecture" for papers with exact title phrase
- Multiple fields: au:"Smith" AND ti:"quantum" for author Smith's quantum papers  
- Exclusions: "deep learning" ANDNOT ("survey" OR "review") to exclude survey papers
- Broad + narrow: "artificial intelligence" AND (robotics OR "computer vision")

CATEGORY FILTERING (highly recommended for relevance):
- cs.AI: Artificial Intelligence
- cs.MA: Multi-Agent Systems  
- cs.LG: Machine Learning
- cs.CL: Computation and Language (NLP)
- cs.CV: Computer Vision
- cs.RO: Robotics
- cs.HC: Human-Computer Interaction
- cs.CR: Cryptography and Security
- cs.DB: Databases

EXAMPLES OF EFFECTIVE QUERIES:
- ti:"reinforcement learning" with categories: ["cs.LG", "cs.AI"] - for RL papers by title
- au:"Hinton" AND "deep learning" with categories: ["cs.LG"] - for Hinton's deep learning work
- "multi-agent" ANDNOT "survey" with categories: ["cs.MA"] - exclude survey papers
- abs:"transformer" AND ti:"attention" with categories: ["cs.CL"] - attention papers with transformer abstracts

DATE FILTERING: Use YYYY-MM-DD format for historical research:
- date_to: "2015-12-31" - for foundational/classic work (pre-2016)
- date_from: "2020-01-01" - for recent developments (post-2020)
- Both together for specific time periods

RESULT QUALITY: Results sorted by RELEVANCE (most relevant papers first), not just newest papers.
This ensures you get the most pertinent results regardless of publication date.

TIPS FOR FOUNDATIONAL RESEARCH:
- Use date_to: "2010-12-31" to find classic papers on BDI, SOAR, ACT-R
- Combine with field searches: ti:"BDI" AND abs:"belief desire intention"  
- Try author searches: au:"Rao" AND "BDI" for Anand Rao's foundational BDI work
Tool Name: search_wikipedia
Tool Arguments: {'properties': {'limit': {'anyOf': [], 'default': 10, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Limit'}, 'query': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Query', 'type': 'string'}}, 'required': ['query'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Search Wikipedia for articles matching a query.
Tool Name: summarize_article_for_query
Tool Arguments: {'properties': {'max_length': {'anyOf': [], 'default': 250, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Max Length'}, 'query': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Query', 'type': 'string'}, 'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['query', 'title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get a summary of a Wikipedia article tailored to a specific query.
Tool Name: summarize_article_section
Tool Arguments: {'properties': {'max_length': {'anyOf': [], 'default': 150, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Max Length'}, 'section_title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Section Title', 'type': 'string'}, 'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['section_title', 'title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get a summary of a specific section of a Wikipedia article.

IMPORTANT: Use the following format in your response:

```
Thought: you should always think about what to do
Action: the action to take, only one name of [download_paper, extract_key_facts, fetch_content, get_article, get_coordinates, get_links, get_related_topics, get_sections, get_summary, list_papers, read_paper, search, search_papers, search_wikipedia, summarize_article_for_query, summarize_article_section], just the name, exactly as it's written.
Action Input: the input to the action, just a simple JSON object, enclosed in curly braces, using " to wrap keys and values.
Observation: the result of the action
```

Once all necessary information is gathered, return the following format:

```
Thought: I now know the final answer
Final Answer: the final answer to the original input question
```[00m
üöÄ Crew: crew
‚îú‚îÄ‚îÄ üìã Task: 0fb61fec-de9f-4d83-82e4-798b1c45aadd
‚îÇ   Status: Executing Task...
‚îÇ   ‚îú‚îÄ‚îÄ ‚úÖ Reasoning Completed
‚îÇ   ‚îú‚îÄ‚îÄ üîß Used download_paper (1)
‚îÇ   ‚îú‚îÄ‚îÄ üîß Used get_sections (1)
‚îÇ   ‚îú‚îÄ‚îÄ üîß Used read_paper (1)
‚îÇ   ‚îî‚îÄ‚îÄ ‚ùå LLM Failed
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îú‚îÄ‚îÄ üîÑ Tool Usage Started
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îî‚îÄ‚îÄ ‚úÖ Tool Usage Completed‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ LLM Error ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚ùå LLM Call Failed                                                         ‚îÇ
‚îÇ  Error: list index out of range                                             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

[91m Error during LLM call: list index out of range[00m
[91m An unknown error occurred. Please check the details below.[00m
[91m Error details: list index out of range[00m
[91m An unknown error occurred. Please check the details below.[00m
[91m Error details: list index out of range[00m
üöÄ Crew: crew
‚îú‚îÄ‚îÄ üìã Task: 0fb61fec-de9f-4d83-82e4-798b1c45aadd
‚îÇ   Status: Executing Task...
‚îÇ   ‚îú‚îÄ‚îÄ ‚úÖ Reasoning Completed
‚îÇ   ‚îú‚îÄ‚îÄ üîß Used download_paper (1)
‚îÇ   ‚îú‚îÄ‚îÄ üîß Used get_sections (1)
‚îÇ   ‚îú‚îÄ‚îÄ üîß Used read_paper (1)
‚îÇ   ‚îú‚îÄ‚îÄ ‚ùå LLM Failed
‚îÇ   ‚îî‚îÄ‚îÄ ‚úÖ Reasoning Completed
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îú‚îÄ‚îÄ üîÑ Tool Usage Started
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îú‚îÄ‚îÄ ‚úÖ Tool Usage Completed
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îú‚îÄ‚îÄ üîÑ Tool Usage Started
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îî‚îÄ‚îÄ ‚úÖ Tool Usage Completed‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üß† Reasoning Plan ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ  1. **Initial Information Extraction from Prefix:** Carefully read the      ‚îÇ
‚îÇ  provided document prefix to identify the paper's title, authors,           ‚îÇ
‚îÇ  affiliation, arXiv ID, and the main claims and findings presented in the   ‚îÇ
‚îÇ  abstract and introduction. This includes the paper's objective (using      ‚îÇ
‚îÇ  LLaMA-7B for better compression and entropy estimation), the specific      ‚îÇ
‚îÇ  entropy estimates (0.709 bits/character for 1MB text8, 0.85                ‚îÇ
‚îÇ  bits/character for 100KB from [7]), and compression ratios (0.7101         ‚îÇ
‚îÇ  bits/character for 1MB text8, 0.8426 bits/character for 100KB from [7]),   ‚îÇ
‚îÇ  and the claim of outperforming BSC, ZPAQ, and paq8h. Note the mention of   ‚îÇ
‚îÇ  Shannon's work and the connection between prediction and compression.      ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  2. **Retrieve the Full Document:** Use the `read_paper` tool with the      ‚îÇ
‚îÇ  arXiv ID "2306.04050" to access the complete content of the paper.         ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  3. **Analyze Document Structure and Content:**                             ‚îÇ
‚îÇ      *   Use `get_sections()` on the retrieved paper to understand its      ‚îÇ
‚îÇ  overall structure (e.g., Introduction, Methodology, Experimental Setup,    ‚îÇ
‚îÇ  Results, Discussion, Conclusion, References).                              ‚îÇ
‚îÇ      *   Use `extract_key_facts()` on the entire document to pull out       ‚îÇ
‚îÇ  specific details, including:                                               ‚îÇ
‚îÇ          *   **Methodology:** Detailed explanation of how LLaMA-7B is used  ‚îÇ
‚îÇ  as a predictor, its integration with arithmetic coding, and any specific   ‚îÇ
‚îÇ  algorithms or techniques employed.                                         ‚îÇ
‚îÇ          *   **Experimental Setup:** Description of datasets used (e.g.,    ‚îÇ
‚îÇ  text8, the dataset from [7]), evaluation metrics, and the baselines (BSC,  ‚îÇ
‚îÇ  ZPAQ, paq8h) against which LLMZip is compared.                             ‚îÇ
‚îÇ          *   **Results:** Specific numerical results, tables, or figures    ‚îÇ
‚îÇ  presenting the entropy estimates and compression ratios for different      ‚îÇ
‚îÇ  datasets and models.                                                       ‚îÇ
‚îÇ          *   **Definitions/Equations:** Any formal definitions or           ‚îÇ
‚îÇ  mathematical equations introduced.                                         ‚îÇ
‚îÇ          *   **Discussion:** Interpretation of results, comparison with     ‚îÇ
‚îÇ  prior work, and insights.                                                  ‚îÇ
‚îÇ          *   **Limitations and Future Work:** Acknowledged limitations and  ‚îÇ
‚îÇ  proposed future research directions.                                       ‚îÇ
‚îÇ          *   **References:** A list of cited works.                         ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  4. **Synthesize the Synopsis:** Construct a concise textual synopsis       ‚îÇ
‚îÇ  based on the extracted sections and key facts. The synopsis will detail:   ‚îÇ
‚îÇ      *   The detailed methodology of LLMZip, including the role of          ‚îÇ
‚îÇ  LLaMA-7B and arithmetic coding.                                            ‚îÇ
‚îÇ      *   The specific experimental design, including datasets (text8,       ‚îÇ
‚îÇ  dataset from [7]), evaluation metrics, and comparison with                 ‚îÇ
‚îÇ  state-of-the-art compressors (BSC, ZPAQ, paq8h).                           ‚îÇ
‚îÇ      *   A comprehensive presentation of results, including refined         ‚îÇ
‚îÇ  entropy estimates and compression ratios, potentially with references to   ‚îÇ
‚îÇ  tables or figures.                                                         ‚îÇ
‚îÇ      *   A discussion of the significance of these results, comparing them  ‚îÇ
‚îÇ  to existing literature.                                                    ‚îÇ
‚îÇ      *   Any theoretical underpinnings, definitions, or equations.          ‚îÇ
‚îÇ      *   Concluding remarks, limitations, and future research directions.   ‚îÇ
‚îÇ      *   A mention of the references section.                               ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  5. **Review and Refine:** Review the generated synopsis to ensure it is    ‚îÇ
‚îÇ  concise, accurate, maximally relevant to the document, non-hallucinatory,  ‚îÇ
‚îÇ  and suitable for conditioning a language model compressor.                 ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Task:[00m [92mYou act as a deterministic research agent. Given the observed prefix of a document, use available MCP tools to retrieve information that is likely to appear later in the same document. Prefer authoritative sources. Produce a concise textual synopsis of the likely future content (facts, equations, definitions, datasets, references), suitable for conditioning a language model compressor. Maximize relevance to the document. Do not hallucinate; only include content supported by the tools.

Document prefix (UTF-8 text):
# LLMZip: Lossless Text Compression using Large Language Models

**Authors:** Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas Shakkottai  
**Affiliation:** Department of Electrical and Computer Engineering, Texas A&M University  
**Email:** {vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu  
**arXiv:** 2306.04050v2 [cs.IT] 26 Jun 2023

## Abstract

We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.

## I. INTRODUCTION

There are close connections between learning, prediction, and compression. The success of ChatGPT has captured the fascination of general public and brought the connection between learning and prediction to the fore. The main advance brought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in a paragraph based on knowing the past several words (tokens).

The connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the entropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past values can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many algorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of lossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very effective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor and every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression performance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved results for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ for text compression.

It is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the entropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this paper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the entropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates provided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of the text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for compression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression ratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio obtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.

## II. INTUITIVE EXPLANATION OF THE MAIN IDEA

We will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3] for estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable length and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence that reads as

My first attempt at writing a book

Our goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can be reconstructed from the sequence of bits. This sentence can first be split into a sequence of

Output: A concise synopsis (plain text).

Reasoning Plan:
1. Identify key information from the document prefix: Extract the paper title, authors, affiliation, arXiv ID, and core claims from the abstract and introduction. This includes the paper's main goal (better compression results and sharper entropy estimates using LLaMA-7B), specific entropy estimates (0.709 bits/character for 1MB text8, 0.85 bits/character for 100KB from [7]), and compression ratios (0.7101 bits/character for 1MB text8, 0.8426 bits/character for 100KB from [7]), and the claim of outperforming BSC, ZPAQ, and paq8h.
2. Search for the full paper using its arXiv ID: Use the `read_paper` tool with the arXiv ID "2306.04050" to access the complete document content.
3. Extract the document structure and key facts: Utilize `get_sections` to obtain a list of all sections in the paper. Additionally, use `extract_key_facts` to pull out specific data, definitions, equations, and methodologies mentioned throughout the paper.
4. Synthesize the likely future content: Based on the extracted sections and key facts, formulate a concise textual synopsis. This synopsis will detail the methodology (e.g., how LLaMA-7B is integrated with arithmetic coding), the experimental setup (e.g., specific datasets like text8, evaluation metrics, and baselines for comparison), detailed results (e.g., tables or figures presenting entropy estimates and compression ratios), a discussion of these results, potential limitations, and future research directions. It will also include a mention of the references.
5. Ensure conciseness and accuracy: Review the synopsis to ensure it is concise, directly relevant to the document, and only includes information explicitly supported by the retrieved content, avoiding any hallucination. The synopsis will be formatted to be suitable for conditioning a language model compressor.

Reasoning Plan:
1. **Initial Information Extraction from Prefix:** Carefully read the provided document prefix to identify the paper's title, authors, affiliation, arXiv ID, and the main claims and findings presented in the abstract and introduction. This includes the paper's objective (using LLaMA-7B for better compression and entropy estimation), the specific entropy estimates (0.709 bits/character for 1MB text8, 0.85 bits/character for 100KB from [7]), and compression ratios (0.7101 bits/character for 1MB text8, 0.8426 bits/character for 100KB from [7]), and the claim of outperforming BSC, ZPAQ, and paq8h. Note the mention of Shannon's work and the connection between prediction and compression.

2. **Retrieve the Full Document:** Use the `read_paper` tool with the arXiv ID "2306.04050" to access the complete content of the paper.

3. **Analyze Document Structure and Content:**
    *   Use `get_sections()` on the retrieved paper to understand its overall structure (e.g., Introduction, Methodology, Experimental Setup, Results, Discussion, Conclusion, References).
    *   Use `extract_key_facts()` on the entire document to pull out specific details, including:
        *   **Methodology:** Detailed explanation of how LLaMA-7B is used as a predictor, its integration with arithmetic coding, and any specific algorithms or techniques employed.
        *   **Experimental Setup:** Description of datasets used (e.g., text8, the dataset from [7]), evaluation metrics, and the baselines (BSC, ZPAQ, paq8h) against which LLMZip is compared.
        *   **Results:** Specific numerical results, tables, or figures presenting the entropy estimates and compression ratios for different datasets and models.
        *   **Definitions/Equations:** Any formal definitions or mathematical equations introduced.
        *   **Discussion:** Interpretation of results, comparison with prior work, and insights.
        *   **Limitations and Future Work:** Acknowledged limitations and proposed future research directions.
        *   **References:** A list of cited works.

4. **Synthesize the Synopsis:** Construct a concise textual synopsis based on the extracted sections and key facts. The synopsis will detail:
    *   The detailed methodology of LLMZip, including the role of LLaMA-7B and arithmetic coding.
    *   The specific experimental design, including datasets (text8, dataset from [7]), evaluation metrics, and comparison with state-of-the-art compressors (BSC, ZPAQ, paq8h).
    *   A comprehensive presentation of results, including refined entropy estimates and compression ratios, potentially with references to tables or figures.
    *   A discussion of the significance of these results, comparing them to existing literature.
    *   Any theoretical underpinnings, definitions, or equations.
    *   Concluding remarks, limitations, and future research directions.
    *   A mention of the references section.

5. **Review and Refine:** Review the generated synopsis to ensure it is concise, accurate, maximally relevant to the document, non-hallucinatory, and suitable for conditioning a language model compressor.[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mAction: download_paper[00m
[95m## Using tool:[00m [92mdownload_paper[00m
[95m## Tool Input:[00m [92m
"{\"paper_id\": \"2306.04050\"}"[00m
[95m## Tool Output:[00m [92m
  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool extract_key_facts with arguments: {"count":20,"title":"LLMZip: Lossless Text Compression using Large Language Models","topic_within_article":""}
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
  > Calling tool extract_key_facts took: 1.5330672s
  - Scanning tool call response for secrets...
  > No secret found in response.
{"status": "success", "message": "Paper already available", "resource_uri": "file:///app/papers/2306.04050.md", "pdf_uri": "https://arxiv.org/pdf/2306.04050.pdf", "content": "1\n\n# LLMZip: Lossless Text Compression using Large Language Models\n\n\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\n\n\nAbstract\n\n\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\n\n\nI. I NTRODUCTION\n\n\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\n\n\nII. I NTUITIVE EXPLANATION OF THE MAIN IDEA\n\n\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\n\nMy first attempt at writing a book\n\n\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032, \u2032 a \u2032, \u2032 book \u2032\n\n\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n\n\n2\n\n\nFigure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be\nwords such as [\u2032] reading [\u2032], [\u2032] writing [\u2032], [\u2032] driving [\u2032], [\u2032] cooking [\u2032] etc. The main idea is to compute the rank of the actual word in\nour sentence ( [\u2032] writing [\u2032] ) in this list and call it R 5 . We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for [\u2032] writing [\u2032] is R 5 = 1.\n\n\nTokens\n\n\n\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x5|x4 1)<br>5|Rank<br>computation|R5 = 1|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nreading 0.3\nwriting 0.2\ncycling 0.1\ndriving 0.05\n\n... ...\n\n\n\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\n\n\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, [\u2032] a [\u2032], and hence, the rank R 6 would be 0.\n\n\nTokens\n\n\n\n\n\n\u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x6|x5 2)<br>6|Rank<br>computation|R6 = 0|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nan 0.2\n\nthe 0.05\n\n... ...\n\n\n\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\n\n\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n\n\n1, 0, 0, . . .\n\n\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\n\n\nSequence of ranks\n\n|e of ranks|Col2|Col3|\n|---|---|---|\n|. ., rNT|Compression (zip)|Nb|\n||||\n\n\n\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\n\n\nWhen we wish to reconstruct the sequence, we first decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank R i during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\n\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n\n\n3\n\n\nIII. C OMPRESSION USING LLM S\n\n\nLet s denote a sentence from the English language composed of N c letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We first parse s into a sequence of N T tokens\ndenoted by x = x 1, x 2, . . ., x i\u22121, x i, x i+1, . . . x N T, where x i \u2208X . There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. x i \u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter X i .\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens x i\u2212M, x i\u2212M+1, . . ., x i\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nq i (x i ) := Pr(X i = x i |x i\u22121, x i\u22122, . . ., x i\u2212M ), \u2200x i \u2208X . The PMF vector q i := [q i (1), q i (2), . . ., q i (D)] [T] is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcq i . Let \u03b3 i : X \u2192X be a permutation on the integers\nfrom 1 to D such that\n\n\u02dc\nq i (\u03b3 i (j)) = q i (j), \u2200j \u2208X .\n\n\nThat is, \u03b3 i (j) is the rank of the token j at epoch i. We define the rank of the input sequence at epoch i as the rank of the\ntoken x i at epoch i, r i := \u03b3 i (x i ). The sequence {r i } [N] i=1 [T] [is compressed by a lossless compression algorithm (such as zlib) to]\nproduce N b bits which are the final bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors q i \u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 defined as\n\n\u03c1 := [N] [b] bits/character.\n\nN c\n\n\n\nLosseless\n\nCompression\n\n\n\nTokens . . ., x i\u2212M, . . ., x i\u22121\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n\nInput sentence s with N c characters\n\n\n\ni i i\u22121 Rank\n\nLLM\ncomputation\n\n\n\nq i = Pr(X i = j|x [i] i [\u2212] \u2212 [M] 1 [)] Rank r i Losseless N b bits\n\n\n\nFig. 4. Schematic showing the prediction at epoch i.\n\n\nA. Entropy bounds\n\n\nLet S \u2208S [\u221e] be a random process that represents language input. The nth character in the sequence is denoted by S n,\nwhereas the string of characters from the beginning to the nth character is expressed as S n . The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X 1, X 2, . . . using a variable-length mapping. In this sequence, X i is the ith\ntoken. The number of characters employed to generate X i depends on the realization of the random process and, as such,\nwe introduce random variable B i to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which B i \u2265 1 and B i is uniformly bounded, with B i < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded N T \u2192\u221e, the number of characters must also approach infinity N c \u2192\u221e. Formally, consider the tokenizer\nfunction T : S [N] \u2192X [N] operating on infinite symbol sequences; that is, T (s) = x where s is an infinite sequence in S [\u221e] . For\nnatural number, i \u2208 N, define m i : S [N] \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Specifically, suppose s is given, then\n\n\nm i (s) = min n [{][length (][T][ (][s] [n] [))][ \u2265] [i][}][ .] (1)\n\n\nWe note that, by construction, lim n\u2192\u221e length (T (s n )) = \u221e and, as such, m i (\u00b7) is well-defined. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple finite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (s n ) = T (s n+1 ). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (finite) series of input symbols in the restricted\nset for which T (y n ) = x i is s m i (s) . Given a fixed sequence s, we can express the number of characters contained in a token\n\nas\nb i = m i (s) \u2212 m i\u22121 (s)\n\n\n4\n\n\nwith initial condition m \u22121 = 0. Consequently, the number of characters embedded in the first N T tokens for a random input\nbecomes N c = [\ufffd] i [N] =1 [T] [B] [i] [.]\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {S k } [\u221e] k=1 [,]\n{B i } [\u221e] i=1 [, and][ {][X] [i] [}] [\u221e] i=1 [are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that]\n\n\n\n\u2212 [1]\n\n\n\n\n[1]\n\nn [log] [2] [ p] [S] [n] [(][S] [1] [, . . ., S] [n] [) =][ \u2212] n [1]\n\n\n\nn [log] [2] [ p] [S] [n] [(][S] [n] [)][ \u2192] [H][(][S][)] almost surely. (2)\n\n\n\nLet \u2126 S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\n\n\n\n[1]\n\ni [log] [2] [ p] [X] [i] [(][X] [1] [, . . ., X] [i] [) =][ \u2212] [1] i\n\n\n\n\u2212 [1]\n\n\n\ni [log] [2] [ p] [X] [i] [(][X] [i] [)][ \u2192] [H][(][X][)] almost surely. (3)\n\n\n\nDefine \u2126 X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\n\n\nm i (S)\nlim = E [B] almost surely. (4)\ni\u2192\u221e i\n\n\nSet \u2126 B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126 S \u2229 \u2126 X \u2229 \u2126 B, we deduce that\n\n\nH(S) = lim\nk\u2192\u221e [\u2212] k [1] [log] [2] [ p] [S] [k] [(][S] [k] [(][\u03c9][))]\n\n= lim i\u2192\u221e [\u2212] l [1] i log 2 p S li (S l i (\u03c9))\n\n\n= lim log 2 Pr (X i = T (S l i (\u03c9)))\ni\u2192\u221e [\u2212] l [1] i\n\n\n\n1\n\ni [log] [2] [ Pr (][X] [i] [ =][ x] [i] [) =][ H] E [(] B [X][)]\n\n\n\n1 1\n\n= \u2212\nE[B] i [lim] \u2192\u221e i\n\n\n\nE[B] [.]\n\n\n\nThe first equality follows from (2). The second equality is a consequence of the fact that {l i = m i (S(\u03c9))|i \u2208 N} is an infinite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\n\ntwo events,\n{\u03c9 \u2208 \u2126|X i (\u03c9) = x i } = {\u03c9 \u2208 \u2126|T (S m i (S(\u03c9)) ) = x i }.\n\n\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126 B . The sets \u2126 S, \u2126 X, and \u2126 B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\n\nH(S) = [H][(][X][)] almost surely.\n\nE[B]\n\n\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\n\n\n\nPr\n\n\ufffd\n\n\n\n\ufffd\n\n\n\nH(X) \u2264 lim\nN T \u2192\u221e [\u2212] N [1] T\n\n\n\nN T\n\ufffd\n\n\n\n\ufffd log 2 q i (X i )\n\n\ni=1\n\n\n\n= 1, (5)\n\n\n\nwhere q i (\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\n\n\n\n\n[1] N T\n\nN T \ufffd i=1 [log] 2 [q] [i] [(][X] [i] [)]\n\n\n\nH(S) \u2264 lim N T \u2192\u221e \u2212 N [1]\n\n\n\n. (6)\nE[B]\n\n\n\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by H ub . The\nnumerator in (6) represents the average number of bits required to represent the tokens X N T and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\n\n\nB. Encoding schemes\n\n\nWe consider three schemes for the lossless compression block in Fig. 3.\n\n\n5\n\n\n1) Compressing the ranks using zlib: The first scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1 LLaMA+zlib .\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token x i at epoch i by using a prefix-free code assuming q i to be the true distribution of\nthe tokens. A natural choice for a prefix-free code is a Huffman code. Instead, for simplicity, we use a prefix-free code where\n1\nthe codeword for the token x i is of length l i = \u2308log 2 q i (x i ) [\u2309][. A prefix-free code with this length for][ x] [i] [ is guaranteed to exist]\nsince this choice of lengths satisfies the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1 LLaMA+TbyT,\nis given by\n\n\n\nN T\n\ufffd\n\n\ni=1\n\n\n\n1\nlog 2\n\ufffd q i (x i )\n\n\n\n\ufffd\n\n\n\n\u03c1 LLaMA+TbyT =\n\n\n\n\ufffd Ni=1 T [b] [i] .\n\n\n\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use q i (x i ) as the probability of token x i at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1 LLM+AC . It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\n\n\n\u03c1 LLM+AC \u2248\n\n\n\nN T 1\n\ufffd i=1 log 2 q i (x i )\n\n\ufffd Ni=1 T [b] [i] . (7)\n\n\n\nClearly, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and \u03c1 LLM+AC also provide upper bounds on H(S). H ub, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and\n\u03c1 LLM+AC are estimated using a finite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\n\n\nIV. R ESULTS\n\n\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it difficult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\n[Nevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of](http://mattmahoney.net/dc/text8.zip)\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M ) is shown in Table II. As expected, the\ncompression performance improves with increasing M . We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n\n\n6\n\n\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\n\n\nV. A CKNOWLEDGEMENT\n\n\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\n\n\nTABLE I\n\nR ESULTS FOR 1MB OF TEXT FROM TEXT 8 DATASET\n\n\n\nBatch\nN c N T\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC ZPAQ pq8h\n\n\n\nNo. (bpc) file size (bits) (bpc) (bpc) (bpc) (bpc)\n\n\n1 466, 650 100, 000 0.6882 1.0513 0.8215 0.689\n\n\n2 461, 477 100, 000 0.6893 1.0558 0.8242 0.6901\n\n\n3 454, 599 100, 000 0.699 1.0681 0.8357 0.6999\n\n\n4 462, 755 100, 000 0.6748 1.0346 0.8093 0.6757\n\n\n5 453, 847 100, 000 0.7481 1.1265 0.8831 0.749\n\n\n6 458, 252 100, 000 0.7218 1.0957 0.8567 0.7227\n\n\n7 451, 036 100, 000 0.6959 1.0729 0.8353 0.6968\n\n\n8 447, 953 100, 000 0.7092 1.0896 0.8489 0.7101\n\n\n9 462, 665 100, 000 0.7394 1.1126 0.8713 0.7402\n\n\n10 449, 621 100, 000 0.7269 1.1046 0.8643 0.7277\n\n\nTotal 9, 137, 710 2, 000, 000 0.7093 1.0812 0.845 0.7101 1.4 [1] 1.2 [2]\n\n\nTABLE II\n\nC OMPRESSION PERFORMANCE OF THE LLM ON THE TEXT 8 DATASET, AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\n\n\n(bpc) file size (bits) (bpc) (bpc)\n\n\n\n31 4, 568, 855 1, 000, 000 0.9139 1.3159 1.0425 0.9145\n\n\n127 4, 568, 855 1, 000, 000 0.7511 1.1303 0.8847 0.752\n\n\n255 4, 568, 855 1, 000, 000 0.7242 1.0985 0.859 0.725\n\n\n511 4, 568, 855 1, 000, 000 0.7093 1.0812 0.845 0.7101\n\n\n1 This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2 This result is taken from [12] and it corresponds to the full 100MB dataset text8\n\n\n7\n\n\n\nTABLE III\n\nM EAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\nM\n\n(bpc) (bpc) (bpc) (bpc)\n\n\n31 0.9139 \u00b1 0.0263 1.3159 \u00b1 0.0329 1.0425 \u00b1 0.0262 0.9145 \u00b1 0.0263\n\n\n127 0.7511 \u00b1 0.0233 1.1303 \u00b1 0.0292 0.8847 \u00b1 0.0231 0.752 \u00b1 0.0233\n\n\n255 0.7242 \u00b1 0.0234 1.0985 \u00b1 0.0289 0.859 \u00b1 0.0232 0.725 \u00b1 0.0234\n\n\n511 0.7093 \u00b1 0.0228 1.0812 \u00b1 0.028 0.845 \u00b1 0.0226 0.7101 \u00b1 0.0228\n\n\nTABLE IV\n\nC OMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN P ROJECT G UTENBERG [7], AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC Standalone Zlib\n\n\n(bpc) (bpc) (bpc) (bpc) (bpc)\n\n\n\n31 508, 463 115, 000 1.0919 1.5316 1.2152 1.0924 2.80\n\n\n127 508, 463 115, 000 0.8973 1.3128 1.0235 0.8982 2.80\n\n\n255 508, 463 115, 000 0.8618 1.2684 0.9899 0.8627 2.80\n\n\n511 508, 463 115, 000 0.8417 1.2465 0.9711 0.8426 2.80\n\n\n8\n\n\nR EFERENCES\n\n\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR filters),\u201d 2023.\n\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and efficient foundation language models,\u201d\n2023.\n\n[7] J. Frank Dobie, Legends of Texas, United States, Texas Folk-Lore Society, 1924; Project Gutenberg, May 25, 2023, 2023,\n[https://www.gutenberg.org/ebooks/70859.](https://www.gutenberg.org/ebooks/70859)\n\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n\n\n"}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mThought: The full paper has been downloaded. Now I need to extract the sections and key facts to create the synopsis. I will use `get_sections` to get the structure and then `extract_key_facts` to get the details.[00m
[95m## Using tool:[00m [92mget_sections[00m
[95m## Tool Input:[00m [92m
"{\"title\": \"LLMZip: Lossless Text Compression using Large Language Models\"}"[00m
[95m## Tool Output:[00m [92m
{"title":"LLMZip: Lossless Text Compression using Large Language Models","sections":[]}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mThought: The `get_sections` tool returned an empty list, which indicates that the tool might not be able to parse sections from the markdown content directly. I will proceed by extracting key facts from the entire document content that was returned by `download_paper`. I will focus on the methodology, experimental setup, results, and any definitions or equations.[00m
[95m## Using tool:[00m [92mextract_key_facts[00m
[95m## Tool Input:[00m [92m
"{\"title\": \"LLMZip: Lossless Text Compression using Large Language Models\", \"count\": 20}"[00m
[95m## Tool Output:[00m [92m
{"title":"LLMZip: Lossless Text Compression using Large Language Models","topic_within_article":"","facts":["No Wikipedia article found for 'LLMZip: Lossless Text Compression using Large Language Models'."]}


You ONLY have access to the following tools, and should NEVER make up tools that are not listed here:

Tool Name: download_paper
Tool Arguments: {'properties': {'check_status': {'anyOf': [], 'default': False, 'description': 'Set to true to only check the status of an ongoing or completed conversion without starting a new download. Use this to monitor long-running conversions or verify if a paper is already available.', 'enum': None, 'items': None, 'properties': {}, 'title': ''}, 'paper_id': {'anyOf': [], 'description': "The arXiv identifier of the paper to download (e.g., '2301.07041', '1706.03762', 'cs.AI/0301001'). This can be found in search results or arXiv URLs. The paper must exist on arXiv.", 'enum': None, 'items': None, 'properties': {}, 'title': '', 'type': 'string'}}, 'required': ['paper_id'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Download and convert an arXiv paper to readable markdown format for analysis and reading. This tool fetches the PDF from arXiv, converts it to markdown using advanced text extraction, and stores it locally for immediate access. Use this tool when you need to read, analyze, or work with the full text content of a specific paper. The conversion process extracts text, preserves formatting, and handles mathematical equations. Returns the full paper content directly upon successful completion.
Tool Name: extract_key_facts
Tool Arguments: {'properties': {'count': {'anyOf': [], 'default': 5, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Count'}, 'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}, 'topic_within_article': {'anyOf': [], 'default': '', 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Topic Within Article'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Extract key facts from a Wikipedia article, optionally focused on a topic.
Tool Name: fetch_content
Tool Arguments: {'properties': {'url': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Url', 'type': 'string'}}, 'required': ['url'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: 
    Fetch and parse content from a webpage URL.

    Args:
        url: The webpage URL to fetch content from
        ctx: MCP context for logging
    
Tool Name: get_article
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get the full content of a Wikipedia article.
Tool Name: get_coordinates
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get the coordinates of a Wikipedia article.
Tool Name: get_links
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get the links contained within a Wikipedia article.
Tool Name: get_related_topics
Tool Arguments: {'properties': {'limit': {'anyOf': [], 'default': 10, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Limit'}, 'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get topics related to a Wikipedia article based on links and categories.
Tool Name: get_sections
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get the sections of a Wikipedia article.
Tool Name: get_summary
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get a summary of a Wikipedia article.
Tool Name: list_papers
Tool Arguments: {'properties': {}, 'title': 'DynamicModel', 'type': 'object'}
Tool Description: List all previously downloaded and converted papers that are available in local storage for immediate reading and analysis. This tool shows you what papers you already have access to without needing to download them again. Each paper in the list includes metadata like title, authors, abstract, and direct links. Use this tool to see your paper library, check if a specific paper is already downloaded, or browse previously acquired research papers before downloading new ones.
Tool Name: read_paper
Tool Arguments: {'properties': {'paper_id': {'anyOf': [], 'description': "The arXiv identifier of the paper to read (e.g., '2301.07041', '1706.03762'). This must be a paper that has been previously downloaded and converted to markdown format. Use list_papers to see available papers.", 'enum': None, 'items': None, 'properties': {}, 'title': '', 'type': 'string'}}, 'required': ['paper_id'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Read the full text content of a previously downloaded and converted research paper in clean markdown format. This tool retrieves the complete paper content including abstract, introduction, methodology, results, conclusions, and references. The content is formatted for easy reading and analysis, with preserved mathematical equations and structured sections. Use this tool when you need to access the full text of a paper for detailed study, quotation, analysis, or research. The paper must have been previously downloaded using the download_paper tool.
Tool Name: search
Tool Arguments: {'properties': {'max_results': {'anyOf': [], 'default': 10, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Max Results'}, 'query': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Query', 'type': 'string'}}, 'required': ['query'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: 
    Search DuckDuckGo and return formatted results.

    Args:
        query: The search query string
        max_results: Maximum number of results to return (default: 10)
        ctx: MCP context for logging
    
Tool Name: search_papers
Tool Arguments: {'properties': {'categories': {'anyOf': [], 'default': None, 'description': "Strongly recommended: arXiv categories to focus search (e.g., ['cs.AI', 'cs.MA'] for agent research, ['cs.LG'] for ML, ['cs.CL'] for NLP, ['cs.CV'] for vision). Greatly improves relevance.", 'enum': None, 'items': {'type': 'string'}, 'properties': {}, 'title': ''}, 'date_from': {'anyOf': [], 'default': None, 'description': "Start date for papers (YYYY-MM-DD format). Use to find recent work, e.g., '2023-01-01' for last 2 years.", 'enum': None, 'items': None, 'properties': {}, 'title': ''}, 'date_to': {'anyOf': [], 'default': None, 'description': "End date for papers (YYYY-MM-DD format). Use with date_from to find historical work, e.g., '2020-12-31' for older research.", 'enum': None, 'items': None, 'properties': {}, 'title': ''}, 'max_results': {'anyOf': [], 'default': None, 'description': 'Maximum number of results to return (default: 10, max: 50). Use 15-20 for comprehensive searches.', 'enum': None, 'items': None, 'properties': {}, 'title': ''}, 'query': {'anyOf': [], 'description': 'Search query using quoted phrases for exact matches (e.g., \'"machine learning" OR "deep learning"\') or specific technical terms. Avoid overly broad or generic terms.', 'enum': None, 'items': None, 'properties': {}, 'title': '', 'type': 'string'}, 'sort_by': {'anyOf': [], 'default': None, 'description': "Sort results by 'relevance' (most relevant first, default) or 'date' (newest first). Use 'relevance' for focused searches, 'date' for recent developments.", 'enum': ['relevance', 'date'], 'items': None, 'properties': {}, 'title': ''}}, 'required': ['query'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Search for papers on arXiv with advanced filtering and query optimization.

QUERY CONSTRUCTION GUIDELINES:
- Use QUOTED PHRASES for exact matches: "multi-agent systems", "neural networks", "machine learning"
- Combine related concepts with OR: "AI agents" OR "software agents" OR "intelligent agents"  
- Use field-specific searches for precision:
  - ti:"exact title phrase" - search in titles only
  - au:"author name" - search by author
  - abs:"keyword" - search in abstracts only
- Use ANDNOT to exclude unwanted results: "machine learning" ANDNOT "survey"
- For best results, use 2-4 core concepts rather than long keyword lists

ADVANCED SEARCH PATTERNS:
- Field + phrase: ti:"transformer architecture" for papers with exact title phrase
- Multiple fields: au:"Smith" AND ti:"quantum" for author Smith's quantum papers  
- Exclusions: "deep learning" ANDNOT ("survey" OR "review") to exclude survey papers
- Broad + narrow: "artificial intelligence" AND (robotics OR "computer vision")

CATEGORY FILTERING (highly recommended for relevance):
- cs.AI: Artificial Intelligence
- cs.MA: Multi-Agent Systems  
- cs.LG: Machine Learning
- cs.CL: Computation and Language (NLP)
- cs.CV: Computer Vision
- cs.RO: Robotics
- cs.HC: Human-Computer Interaction
- cs.CR: Cryptography and Security
- cs.DB: Databases

EXAMPLES OF EFFECTIVE QUERIES:
- ti:"reinforcement learning" with categories: ["cs.LG", "cs.AI"] - for RL papers by title
- au:"Hinton" AND "deep learning" with categories: ["cs.LG"] - for Hinton's deep learning work
- "multi-agent" ANDNOT "survey" with categories: ["cs.MA"] - exclude survey papers
- abs:"transformer" AND ti:"attention" with categories: ["cs.CL"] - attention papers with transformer abstracts

DATE FILTERING: Use YYYY-MM-DD format for historical research:
- date_to: "2015-12-31" - for foundational/classic work (pre-2016)
- date_from: "2020-01-01" - for recent developments (post-2020)
- Both together for specific time periods

RESULT QUALITY: Results sorted by RELEVANCE (most relevant papers first), not just newest papers.
This ensures you get the most pertinent results regardless of publication date.

TIPS FOR FOUNDATIONAL RESEARCH:
- Use date_to: "2010-12-31" to find classic papers on BDI, SOAR, ACT-R
- Combine with field searches: ti:"BDI" AND abs:"belief desire intention"  
- Try author searches: au:"Rao" AND "BDI" for Anand Rao's foundational BDI work
Tool Name: search_wikipedia
Tool Arguments: {'properties': {'limit': {'anyOf': [], 'default': 10, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Limit'}, 'query': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Query', 'type': 'string'}}, 'required': ['query'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Search Wikipedia for articles matching a query.
Tool Name: summarize_article_for_query
Tool Arguments: {'properties': {'max_length': {'anyOf': [], 'default': 250, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Max Length'}, 'query': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Query', 'type': 'string'}, 'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['query', 'title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get a summary of a Wikipedia article tailored to a specific query.
Tool Name: summarize_article_section
Tool Arguments: {'properties': {'max_length': {'anyOf': [], 'default': 150, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Max Length'}, 'section_title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Section Title', 'type': 'string'}, 'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['section_title', 'title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get a summary of a specific section of a Wikipedia article.

IMPORTANT: Use the following format in your response:

```
Thought: you should always think about what to do
Action: the action to take, only one name of [download_paper, extract_key_facts, fetch_content, get_article, get_coordinates, get_links, get_related_topics, get_sections, get_summary, list_papers, read_paper, search, search_papers, search_wikipedia, summarize_article_for_query, summarize_article_section], just the name, exactly as it's written.
Action Input: the input to the action, just a simple JSON object, enclosed in curly braces, using " to wrap keys and values.
Observation: the result of the action
```

Once all necessary information is gathered, return the following format:

```
Thought: I now know the final answer
Final Answer: the final answer to the original input question
```[00mC:\tools\Anaconda3\Lib\site-packages\paramiko\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "cipher": algorithms.TripleDES,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.
  "class": algorithms.Blowfish,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "class": algorithms.TripleDES,
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\_internal\_generate_schema.py:628: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.
  warn(
- Reading configuration...
  - Reading registry from registry.yaml
  - Reading catalog from docker-mcp.yaml
  - Reading config from config.yaml
- Configuration read in 19.4203ms
- Watching registry at C:\Users\Noah\.docker\mcp\registry.yaml
- Watching config at C:\Users\Noah\.docker\mcp\config.yaml
- Those servers are enabled: arxiv-mcp-server, duckduckgo, wikipedia-mcp
- Using images:
  - mcp/arxiv-mcp-server@sha256:6dc6bba6dfed97f4ad6eb8d23a5c98ef5b7fa6184937d54b2d675801cd9dd29e
  - mcp/duckduckgo@sha256:68eb20db6109f5c312a695fc5ec3386ad15d93ffb765a0b4eb1baf4328dec14f
  - mcp/wikipedia-mcp@sha256:e90e3f6e3bb795f10d3d8d8ac1523f6d46d4f2ab5ac3b3a4720e1be879466472
> Images pulled in 58.9317ms
- Listing MCP tools...
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers:ro]
  - Running mcp/duckduckgo with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=duckduckgo -l docker-mcp-transport=stdio]
  > duckduckgo: (2 tools)
  > wikipedia-mcp: (10 tools) (9 resourceTemplates)
  > arxiv-mcp-server: (4 tools) (1 prompts)
> 16 tools listed in 2.2347691s
- Watching for configuration updates...
> Initialized in 2.3146662s
> Start stdio server
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\fields.py:1093: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'items', 'anyOf', 'enum', 'properties'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
  warn(
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Crew Execution Started ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ  Crew Execution Started                                                     ‚îÇ
‚îÇ  Name: crew                                                                 ‚îÇ
‚îÇ  ID: 37b74096-e692-4136-a194-9eb70149cc5f                                   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üöÄ Crew: crew
‚îú‚îÄ‚îÄ üìã Task: 91b7630c-bea9-4b6d-93c1-621145f3eda1
‚îÇ   Status: Executing Task...
‚îÇ   ‚îî‚îÄ‚îÄ ‚úÖ Reasoning Completed
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îú‚îÄ‚îÄ üîÑ Tool Usage Started
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îî‚îÄ‚îÄ ‚úÖ Tool Usage Completed‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üß† Reasoning Plan ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ  1. Identify key information from the document prefix: Extract the paper    ‚îÇ
‚îÇ  title, authors, affiliation, arXiv ID, and core claims from the abstract   ‚îÇ
‚îÇ  and introduction. This includes the paper's main goal (better compression  ‚îÇ
‚îÇ  results and sharper entropy estimates using LLaMA-7B), specific entropy    ‚îÇ
‚îÇ  estimates (0.709 bits/character for 1MB text8, 0.85 bits/character for     ‚îÇ
‚îÇ  100KB from [7]), and compression ratios (0.7101 bits/character for 1MB     ‚îÇ
‚îÇ  text8, 0.8426 bits/character for 100KB from [7]), and the claim of         ‚îÇ
‚îÇ  outperforming BSC, ZPAQ, and paq8h.                                        ‚îÇ
‚îÇ  2. Search for the full paper using its arXiv ID: Use the `read_paper`      ‚îÇ
‚îÇ  tool with the arXiv ID "2306.04050" to access the complete document        ‚îÇ
‚îÇ  content.                                                                   ‚îÇ
‚îÇ  3. Extract the document structure and key facts: Utilize `get_sections`    ‚îÇ
‚îÇ  to obtain a list of all sections in the paper. Additionally, use           ‚îÇ
‚îÇ  `extract_key_facts` to pull out specific data, definitions, equations,     ‚îÇ
‚îÇ  and methodologies mentioned throughout the paper.                          ‚îÇ
‚îÇ  4. Synthesize the likely future content: Based on the extracted sections   ‚îÇ
‚îÇ  and key facts, formulate a concise textual synopsis. This synopsis will    ‚îÇ
‚îÇ  detail the methodology (e.g., how LLaMA-7B is integrated with arithmetic   ‚îÇ
‚îÇ  coding), the experimental setup (e.g., specific datasets like text8,       ‚îÇ
‚îÇ  evaluation metrics, and baselines for comparison), detailed results        ‚îÇ
‚îÇ  (e.g., tables or figures presenting entropy estimates and compression      ‚îÇ
‚îÇ  ratios), a discussion of these results, potential limitations, and future  ‚îÇ
‚îÇ  research directions. It will also include a mention of the references.     ‚îÇ
‚îÇ  5. Ensure conciseness and accuracy: Review the synopsis to ensure it is    ‚îÇ
‚îÇ  concise, directly relevant to the document, and only includes information  ‚îÇ
‚îÇ  explicitly supported by the retrieved content, avoiding any                ‚îÇ
‚îÇ  hallucination. The synopsis will be formatted to be suitable for           ‚îÇ
‚îÇ  conditioning a language model compressor.                                  ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool download_paper with arguments: {"check_status":false,"paper_id":"2306.04050"}
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
  > Calling tool download_paper took: 1.5623696s
  - Scanning tool call response for secrets...
  > No secret found in response.
[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Task:[00m [92mYou act as a deterministic research agent. Given the observed prefix of a document, use available MCP tools to retrieve information that is likely to appear later in the same document. Prefer authoritative sources. Produce a concise textual synopsis of the likely future content (facts, equations, definitions, datasets, references), suitable for conditioning a language model compressor. Maximize relevance to the document. Do not hallucinate; only include content supported by the tools.

Document prefix (UTF-8 text):
# LLMZip: Lossless Text Compression using Large Language Models

**Authors:** Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas Shakkottai  
**Affiliation:** Department of Electrical and Computer Engineering, Texas A&M University  
**Email:** {vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu  
**arXiv:** 2306.04050v2 [cs.IT] 26 Jun 2023

## Abstract

We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.

## I. INTRODUCTION

There are close connections between learning, prediction, and compression. The success of ChatGPT has captured the fascination of general public and brought the connection between learning and prediction to the fore. The main advance brought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in a paragraph based on knowing the past several words (tokens).

The connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the entropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past values can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many algorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of lossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very effective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor and every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression performance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved results for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ for text compression.

It is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the entropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this paper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the entropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates provided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of the text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for compression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression ratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio obtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.

## II. INTUITIVE EXPLANATION OF THE MAIN IDEA

We will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3] for estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable length and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence that reads as

My first attempt at writing a book

Our goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can be reconstructed from the sequence of bits. This sentence can first be split into a sequence of

Output: A concise synopsis (plain text).

Reasoning Plan:
1. Identify key information from the document prefix: Extract the paper title, authors, affiliation, arXiv ID, and core claims from the abstract and introduction. This includes the paper's main goal (better compression results and sharper entropy estimates using LLaMA-7B), specific entropy estimates (0.709 bits/character for 1MB text8, 0.85 bits/character for 100KB from [7]), and compression ratios (0.7101 bits/character for 1MB text8, 0.8426 bits/character for 100KB from [7]), and the claim of outperforming BSC, ZPAQ, and paq8h.
2. Search for the full paper using its arXiv ID: Use the `read_paper` tool with the arXiv ID "2306.04050" to access the complete document content.
3. Extract the document structure and key facts: Utilize `get_sections` to obtain a list of all sections in the paper. Additionally, use `extract_key_facts` to pull out specific data, definitions, equations, and methodologies mentioned throughout the paper.
4. Synthesize the likely future content: Based on the extracted sections and key facts, formulate a concise textual synopsis. This synopsis will detail the methodology (e.g., how LLaMA-7B is integrated with arithmetic coding), the experimental setup (e.g., specific datasets like text8, evaluation metrics, and baselines for comparison), detailed results (e.g., tables or figures presenting entropy estimates and compression ratios), a discussion of these results, potential limitations, and future research directions. It will also include a mention of the references.
5. Ensure conciseness and accuracy: Review the synopsis to ensure it is concise, directly relevant to the document, and only includes information explicitly supported by the retrieved content, avoiding any hallucination. The synopsis will be formatted to be suitable for conditioning a language model compressor.[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mAction: download_paper[00m
[95m## Using tool:[00m [92mdownload_paper[00m
[95m## Tool Input:[00m [92m
"{\"paper_id\": \"2306.04050\"}"[00m
[95m## Tool Output:[00m [92m
  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool get_sections with arguments: {"title":"LLMZip: Lossless Text Compression using Large Language Models"}
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
  > Calling tool get_sections took: 1.6151931s
  - Scanning tool call response for secrets...
  > No secret found in response.
  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool read_paper with arguments: {"paper_id":"2306.04050"}
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
  > Calling tool read_paper took: 1.7295115s
  - Scanning tool call response for secrets...
  > No secret found in response.
{"status": "success", "message": "Paper already available", "resource_uri": "file:///app/papers/2306.04050.md", "pdf_uri": "https://arxiv.org/pdf/2306.04050.pdf", "content": "1\n\n# LLMZip: Lossless Text Compression using Large Language Models\n\n\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\n\n\nAbstract\n\n\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\n\n\nI. I NTRODUCTION\n\n\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\n\n\nII. I NTUITIVE EXPLANATION OF THE MAIN IDEA\n\n\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\n\nMy first attempt at writing a book\n\n\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032, \u2032 a \u2032, \u2032 book \u2032\n\n\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n\n\n2\n\n\nFigure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be\nwords such as [\u2032] reading [\u2032], [\u2032] writing [\u2032], [\u2032] driving [\u2032], [\u2032] cooking [\u2032] etc. The main idea is to compute the rank of the actual word in\nour sentence ( [\u2032] writing [\u2032] ) in this list and call it R 5 . We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for [\u2032] writing [\u2032] is R 5 = 1.\n\n\nTokens\n\n\n\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x5|x4 1)<br>5|Rank<br>computation|R5 = 1|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nreading 0.3\nwriting 0.2\ncycling 0.1\ndriving 0.05\n\n... ...\n\n\n\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\n\n\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, [\u2032] a [\u2032], and hence, the rank R 6 would be 0.\n\n\nTokens\n\n\n\n\n\n\u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x6|x5 2)<br>6|Rank<br>computation|R6 = 0|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nan 0.2\n\nthe 0.05\n\n... ...\n\n\n\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\n\n\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n\n\n1, 0, 0, . . .\n\n\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\n\n\nSequence of ranks\n\n|e of ranks|Col2|Col3|\n|---|---|---|\n|. ., rNT|Compression (zip)|Nb|\n||||\n\n\n\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\n\n\nWhen we wish to reconstruct the sequence, we first decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank R i during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\n\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n\n\n3\n\n\nIII. C OMPRESSION USING LLM S\n\n\nLet s denote a sentence from the English language composed of N c letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We first parse s into a sequence of N T tokens\ndenoted by x = x 1, x 2, . . ., x i\u22121, x i, x i+1, . . . x N T, where x i \u2208X . There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. x i \u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter X i .\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens x i\u2212M, x i\u2212M+1, . . ., x i\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nq i (x i ) := Pr(X i = x i |x i\u22121, x i\u22122, . . ., x i\u2212M ), \u2200x i \u2208X . The PMF vector q i := [q i (1), q i (2), . . ., q i (D)] [T] is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcq i . Let \u03b3 i : X \u2192X be a permutation on the integers\nfrom 1 to D such that\n\n\u02dc\nq i (\u03b3 i (j)) = q i (j), \u2200j \u2208X .\n\n\nThat is, \u03b3 i (j) is the rank of the token j at epoch i. We define the rank of the input sequence at epoch i as the rank of the\ntoken x i at epoch i, r i := \u03b3 i (x i ). The sequence {r i } [N] i=1 [T] [is compressed by a lossless compression algorithm (such as zlib) to]\nproduce N b bits which are the final bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors q i \u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 defined as\n\n\u03c1 := [N] [b] bits/character.\n\nN c\n\n\n\nLosseless\n\nCompression\n\n\n\nTokens . . ., x i\u2212M, . . ., x i\u22121\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n\nInput sentence s with N c characters\n\n\n\ni i i\u22121 Rank\n\nLLM\ncomputation\n\n\n\nq i = Pr(X i = j|x [i] i [\u2212] \u2212 [M] 1 [)] Rank r i Losseless N b bits\n\n\n\nFig. 4. Schematic showing the prediction at epoch i.\n\n\nA. Entropy bounds\n\n\nLet S \u2208S [\u221e] be a random process that represents language input. The nth character in the sequence is denoted by S n,\nwhereas the string of characters from the beginning to the nth character is expressed as S n . The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X 1, X 2, . . . using a variable-length mapping. In this sequence, X i is the ith\ntoken. The number of characters employed to generate X i depends on the realization of the random process and, as such,\nwe introduce random variable B i to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which B i \u2265 1 and B i is uniformly bounded, with B i < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded N T \u2192\u221e, the number of characters must also approach infinity N c \u2192\u221e. Formally, consider the tokenizer\nfunction T : S [N] \u2192X [N] operating on infinite symbol sequences; that is, T (s) = x where s is an infinite sequence in S [\u221e] . For\nnatural number, i \u2208 N, define m i : S [N] \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Specifically, suppose s is given, then\n\n\nm i (s) = min n [{][length (][T][ (][s] [n] [))][ \u2265] [i][}][ .] (1)\n\n\nWe note that, by construction, lim n\u2192\u221e length (T (s n )) = \u221e and, as such, m i (\u00b7) is well-defined. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple finite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (s n ) = T (s n+1 ). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (finite) series of input symbols in the restricted\nset for which T (y n ) = x i is s m i (s) . Given a fixed sequence s, we can express the number of characters contained in a token\n\nas\nb i = m i (s) \u2212 m i\u22121 (s)\n\n\n4\n\n\nwith initial condition m \u22121 = 0. Consequently, the number of characters embedded in the first N T tokens for a random input\nbecomes N c = [\ufffd] i [N] =1 [T] [B] [i] [.]\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {S k } [\u221e] k=1 [,]\n{B i } [\u221e] i=1 [, and][ {][X] [i] [}] [\u221e] i=1 [are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that]\n\n\n\n\u2212 [1]\n\n\n\n\n[1]\n\nn [log] [2] [ p] [S] [n] [(][S] [1] [, . . ., S] [n] [) =][ \u2212] n [1]\n\n\n\nn [log] [2] [ p] [S] [n] [(][S] [n] [)][ \u2192] [H][(][S][)] almost surely. (2)\n\n\n\nLet \u2126 S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\n\n\n\n[1]\n\ni [log] [2] [ p] [X] [i] [(][X] [1] [, . . ., X] [i] [) =][ \u2212] [1] i\n\n\n\n\u2212 [1]\n\n\n\ni [log] [2] [ p] [X] [i] [(][X] [i] [)][ \u2192] [H][(][X][)] almost surely. (3)\n\n\n\nDefine \u2126 X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\n\n\nm i (S)\nlim = E [B] almost surely. (4)\ni\u2192\u221e i\n\n\nSet \u2126 B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126 S \u2229 \u2126 X \u2229 \u2126 B, we deduce that\n\n\nH(S) = lim\nk\u2192\u221e [\u2212] k [1] [log] [2] [ p] [S] [k] [(][S] [k] [(][\u03c9][))]\n\n= lim i\u2192\u221e [\u2212] l [1] i log 2 p S li (S l i (\u03c9))\n\n\n= lim log 2 Pr (X i = T (S l i (\u03c9)))\ni\u2192\u221e [\u2212] l [1] i\n\n\n\n1\n\ni [log] [2] [ Pr (][X] [i] [ =][ x] [i] [) =][ H] E [(] B [X][)]\n\n\n\n1 1\n\n= \u2212\nE[B] i [lim] \u2192\u221e i\n\n\n\nE[B] [.]\n\n\n\nThe first equality follows from (2). The second equality is a consequence of the fact that {l i = m i (S(\u03c9))|i \u2208 N} is an infinite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\n\ntwo events,\n{\u03c9 \u2208 \u2126|X i (\u03c9) = x i } = {\u03c9 \u2208 \u2126|T (S m i (S(\u03c9)) ) = x i }.\n\n\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126 B . The sets \u2126 S, \u2126 X, and \u2126 B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\n\nH(S) = [H][(][X][)] almost surely.\n\nE[B]\n\n\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\n\n\n\nPr\n\n\ufffd\n\n\n\n\ufffd\n\n\n\nH(X) \u2264 lim\nN T \u2192\u221e [\u2212] N [1] T\n\n\n\nN T\n\ufffd\n\n\n\n\ufffd log 2 q i (X i )\n\n\ni=1\n\n\n\n= 1, (5)\n\n\n\nwhere q i (\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\n\n\n\n\n[1] N T\n\nN T \ufffd i=1 [log] 2 [q] [i] [(][X] [i] [)]\n\n\n\nH(S) \u2264 lim N T \u2192\u221e \u2212 N [1]\n\n\n\n. (6)\nE[B]\n\n\n\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by H ub . The\nnumerator in (6) represents the average number of bits required to represent the tokens X N T and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\n\n\nB. Encoding schemes\n\n\nWe consider three schemes for the lossless compression block in Fig. 3.\n\n\n5\n\n\n1) Compressing the ranks using zlib: The first scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1 LLaMA+zlib .\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token x i at epoch i by using a prefix-free code assuming q i to be the true distribution of\nthe tokens. A natural choice for a prefix-free code is a Huffman code. Instead, for simplicity, we use a prefix-free code where\n1\nthe codeword for the token x i is of length l i = \u2308log 2 q i (x i ) [\u2309][. A prefix-free code with this length for][ x] [i] [ is guaranteed to exist]\nsince this choice of lengths satisfies the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1 LLaMA+TbyT,\nis given by\n\n\n\nN T\n\ufffd\n\n\ni=1\n\n\n\n1\nlog 2\n\ufffd q i (x i )\n\n\n\n\ufffd\n\n\n\n\u03c1 LLaMA+TbyT =\n\n\n\n\ufffd Ni=1 T [b] [i] .\n\n\n\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use q i (x i ) as the probability of token x i at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1 LLM+AC . It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\n\n\n\u03c1 LLM+AC \u2248\n\n\n\nN T 1\n\ufffd i=1 log 2 q i (x i )\n\n\ufffd Ni=1 T [b] [i] . (7)\n\n\n\nClearly, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and \u03c1 LLM+AC also provide upper bounds on H(S). H ub, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and\n\u03c1 LLM+AC are estimated using a finite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\n\n\nIV. R ESULTS\n\n\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it difficult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\n[Nevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of](http://mattmahoney.net/dc/text8.zip)\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M ) is shown in Table II. As expected, the\ncompression performance improves with increasing M . We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n\n\n6\n\n\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\n\n\nV. A CKNOWLEDGEMENT\n\n\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\n\n\nTABLE I\n\nR ESULTS FOR 1MB OF TEXT FROM TEXT 8 DATASET\n\n\n\nBatch\nN c N T\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC ZPAQ pq8h\n\n\n\nNo. (bpc) file size (bits) (bpc) (bpc) (bpc) (bpc)\n\n\n1 466, 650 100, 000 0.6882 1.0513 0.8215 0.689\n\n\n2 461, 477 100, 000 0.6893 1.0558 0.8242 0.6901\n\n\n3 454, 599 100, 000 0.699 1.0681 0.8357 0.6999\n\n\n4 462, 755 100, 000 0.6748 1.0346 0.8093 0.6757\n\n\n5 453, 847 100, 000 0.7481 1.1265 0.8831 0.749\n\n\n6 458, 252 100, 000 0.7218 1.0957 0.8567 0.7227\n\n\n7 451, 036 100, 000 0.6959 1.0729 0.8353 0.6968\n\n\n8 447, 953 100, 000 0.7092 1.0896 0.8489 0.7101\n\n\n9 462, 665 100, 000 0.7394 1.1126 0.8713 0.7402\n\n\n10 449, 621 100, 000 0.7269 1.1046 0.8643 0.7277\n\n\nTotal 9, 137, 710 2, 000, 000 0.7093 1.0812 0.845 0.7101 1.4 [1] 1.2 [2]\n\n\nTABLE II\n\nC OMPRESSION PERFORMANCE OF THE LLM ON THE TEXT 8 DATASET, AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\n\n\n(bpc) file size (bits) (bpc) (bpc)\n\n\n\n31 4, 568, 855 1, 000, 000 0.9139 1.3159 1.0425 0.9145\n\n\n127 4, 568, 855 1, 000, 000 0.7511 1.1303 0.8847 0.752\n\n\n255 4, 568, 855 1, 000, 000 0.7242 1.0985 0.859 0.725\n\n\n511 4, 568, 855 1, 000, 000 0.7093 1.0812 0.845 0.7101\n\n\n1 This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2 This result is taken from [12] and it corresponds to the full 100MB dataset text8\n\n\n7\n\n\n\nTABLE III\n\nM EAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\nM\n\n(bpc) (bpc) (bpc) (bpc)\n\n\n31 0.9139 \u00b1 0.0263 1.3159 \u00b1 0.0329 1.0425 \u00b1 0.0262 0.9145 \u00b1 0.0263\n\n\n127 0.7511 \u00b1 0.0233 1.1303 \u00b1 0.0292 0.8847 \u00b1 0.0231 0.752 \u00b1 0.0233\n\n\n255 0.7242 \u00b1 0.0234 1.0985 \u00b1 0.0289 0.859 \u00b1 0.0232 0.725 \u00b1 0.0234\n\n\n511 0.7093 \u00b1 0.0228 1.0812 \u00b1 0.028 0.845 \u00b1 0.0226 0.7101 \u00b1 0.0228\n\n\nTABLE IV\n\nC OMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN P ROJECT G UTENBERG [7], AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC Standalone Zlib\n\n\n(bpc) (bpc) (bpc) (bpc) (bpc)\n\n\n\n31 508, 463 115, 000 1.0919 1.5316 1.2152 1.0924 2.80\n\n\n127 508, 463 115, 000 0.8973 1.3128 1.0235 0.8982 2.80\n\n\n255 508, 463 115, 000 0.8618 1.2684 0.9899 0.8627 2.80\n\n\n511 508, 463 115, 000 0.8417 1.2465 0.9711 0.8426 2.80\n\n\n8\n\n\nR EFERENCES\n\n\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR filters),\u201d 2023.\n\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and efficient foundation language models,\u201d\n2023.\n\n[7] J. Frank Dobie, Legends of Texas, United States, Texas Folk-Lore Society, 1924; Project Gutenberg, May 25, 2023, 2023,\n[https://www.gutenberg.org/ebooks/70859.](https://www.gutenberg.org/ebooks/70859)\n\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n\n\n"}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mThought: The paper "LLMZip: Lossless Text Compression using Large Language Models" (arXiv:2306.04050) has been successfully downloaded. I now have access to its full content. The next step is to extract the document structure (sections) and key facts to create a synopsis of its likely future content. I will use `get_sections` to get the section titles and then synthesize the information.[00m
[95m## Using tool:[00m [92mget_sections[00m
[95m## Tool Input:[00m [92m
"{\"title\": \"LLMZip: Lossless Text Compression using Large Language Models\"}"[00m
[95m## Tool Output:[00m [92m
{"title":"LLMZip: Lossless Text Compression using Large Language Models","sections":[]}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mThought: The `get_sections` tool returned an empty list for the title "LLMZip: Lossless Text Compression using Large Language Models". This might be because the tool expects a Wikipedia article title, not an arXiv paper title. Since I have already downloaded the paper and have its full content, I can directly read the content and extract the sections and other relevant information from the markdown text. I will now read the paper content and manually identify the sections and key information to create the synopsis.[00m
[95m## Using tool:[00m [92mread_paper[00m
[95m## Tool Input:[00m [92m
"{\"paper_id\": \"2306.04050\"}"[00m
[95m## Tool Output:[00m [92m
{"status": "success", "paper_id": "2306.04050", "content": "1\n\n# LLMZip: Lossless Text Compression using Large Language Models\n\n\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\n\n\nAbstract\n\n\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\n\n\nI. I NTRODUCTION\n\n\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\n\n\nII. I NTUITIVE EXPLANATION OF THE MAIN IDEA\n\n\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\n\nMy first attempt at writing a book\n\n\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032, \u2032 a \u2032, \u2032 book \u2032\n\n\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n\n\n2\n\n\nFigure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be\nwords such as [\u2032] reading [\u2032], [\u2032] writing [\u2032], [\u2032] driving [\u2032], [\u2032] cooking [\u2032] etc. The main idea is to compute the rank of the actual word in\nour sentence ( [\u2032] writing [\u2032] ) in this list and call it R 5 . We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for [\u2032] writing [\u2032] is R 5 = 1.\n\n\nTokens\n\n\n\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x5|x4 1)<br>5|Rank<br>computation|R5 = 1|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nreading 0.3\nwriting 0.2\ncycling 0.1\ndriving 0.05\n\n... ...\n\n\n\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\n\n\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, [\u2032] a [\u2032], and hence, the rank R 6 would be 0.\n\n\nTokens\n\n\n\n\n\n\u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x6|x5 2)<br>6|Rank<br>computation|R6 = 0|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nan 0.2\n\nthe 0.05\n\n... ...\n\n\n\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\n\n\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n\n\n1, 0, 0, . . .\n\n\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\n\n\nSequence of ranks\n\n|e of ranks|Col2|Col3|\n|---|---|---|\n|. ., rNT|Compression (zip)|Nb|\n||||\n\n\n\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\n\n\nWhen we wish to reconstruct the sequence, we first decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank R i during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\n\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n\n\n3\n\n\nIII. C OMPRESSION USING LLM S\n\n\nLet s denote a sentence from the English language composed of N c letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We first parse s into a sequence of N T tokens\ndenoted by x = x 1, x 2, . . ., x i\u22121, x i, x i+1, . . . x N T, where x i \u2208X . There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. x i \u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter X i .\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens x i\u2212M, x i\u2212M+1, . . ., x i\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nq i (x i ) := Pr(X i = x i |x i\u22121, x i\u22122, . . ., x i\u2212M ), \u2200x i \u2208X . The PMF vector q i := [q i (1), q i (2), . . ., q i (D)] [T] is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcq i . Let \u03b3 i : X \u2192X be a permutation on the integers\nfrom 1 to D such that\n\n\u02dc\nq i (\u03b3 i (j)) = q i (j), \u2200j \u2208X .\n\n\nThat is, \u03b3 i (j) is the rank of the token j at epoch i. We define the rank of the input sequence at epoch i as the rank of the\ntoken x i at epoch i, r i := \u03b3 i (x i ). The sequence {r i } [N] i=1 [T] [is compressed by a lossless compression algorithm (such as zlib) to]\nproduce N b bits which are the final bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors q i \u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 defined as\n\n\u03c1 := [N] [b] bits/character.\n\nN c\n\n\n\nLosseless\n\nCompression\n\n\n\nTokens . . ., x i\u2212M, . . ., x i\u22121\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n\nInput sentence s with N c characters\n\n\n\ni i i\u22121 Rank\n\nLLM\ncomputation\n\n\n\nq i = Pr(X i = j|x [i] i [\u2212] \u2212 [M] 1 [)] Rank r i Losseless N b bits\n\n\n\nFig. 4. Schematic showing the prediction at epoch i.\n\n\nA. Entropy bounds\n\n\nLet S \u2208S [\u221e] be a random process that represents language input. The nth character in the sequence is denoted by S n,\nwhereas the string of characters from the beginning to the nth character is expressed as S n . The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X 1, X 2, . . . using a variable-length mapping. In this sequence, X i is the ith\ntoken. The number of characters employed to generate X i depends on the realization of the random process and, as such,\nwe introduce random variable B i to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which B i \u2265 1 and B i is uniformly bounded, with B i < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded N T \u2192\u221e, the number of characters must also approach infinity N c \u2192\u221e. Formally, consider the tokenizer\nfunction T : S [N] \u2192X [N] operating on infinite symbol sequences; that is, T (s) = x where s is an infinite sequence in S [\u221e] . For\nnatural number, i \u2208 N, define m i : S [N] \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Specifically, suppose s is given, then\n\n\nm i (s) = min n [{][length (][T][ (][s] [n] [))][ \u2265] [i][}][ .] (1)\n\n\nWe note that, by construction, lim n\u2192\u221e length (T (s n )) = \u221e and, as such, m i (\u00b7) is well-defined. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple finite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (s n ) = T (s n+1 ). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (finite) series of input symbols in the restricted\nset for which T (y n ) = x i is s m i (s) . Given a fixed sequence s, we can express the number of characters contained in a token\n\nas\nb i = m i (s) \u2212 m i\u22121 (s)\n\n\n4\n\n\nwith initial condition m \u22121 = 0. Consequently, the number of characters embedded in the first N T tokens for a random input\nbecomes N c = [\ufffd] i [N] =1 [T] [B] [i] [.]\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {S k } [\u221e] k=1 [,]\n{B i } [\u221e] i=1 [, and][ {][X] [i] [}] [\u221e] i=1 [are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that]\n\n\n\n\u2212 [1]\n\n\n\n\n[1]\n\nn [log] [2] [ p] [S] [n] [(][S] [1] [, . . ., S] [n] [) =][ \u2212] n [1]\n\n\n\nn [log] [2] [ p] [S] [n] [(][S] [n] [)][ \u2192] [H][(][S][)] almost surely. (2)\n\n\n\nLet \u2126 S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\n\n\n\n[1]\n\ni [log] [2] [ p] [X] [i] [(][X] [1] [, . . ., X] [i] [) =][ \u2212] [1] i\n\n\n\n\u2212 [1]\n\n\n\ni [log] [2] [ p] [X] [i] [(][X] [i] [)][ \u2192] [H][(][X][)] almost surely. (3)\n\n\n\nDefine \u2126 X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\n\n\nm i (S)\nlim = E [B] almost surely. (4)\ni\u2192\u221e i\n\n\nSet \u2126 B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126 S \u2229 \u2126 X \u2229 \u2126 B, we deduce that\n\n\nH(S) = lim\nk\u2192\u221e [\u2212] k [1] [log] [2] [ p] [S] [k] [(][S] [k] [(][\u03c9][))]\n\n= lim i\u2192\u221e [\u2212] l [1] i log 2 p S li (S l i (\u03c9))\n\n\n= lim log 2 Pr (X i = T (S l i (\u03c9)))\ni\u2192\u221e [\u2212] l [1] i\n\n\n\n1\n\ni [log] [2] [ Pr (][X] [i] [ =][ x] [i] [) =][ H] E [(] B [X][)]\n\n\n\n1 1\n\n= \u2212\nE[B] i [lim] \u2192\u221e i\n\n\n\nE[B] [.]\n\n\n\nThe first equality follows from (2). The second equality is a consequence of the fact that {l i = m i (S(\u03c9))|i \u2208 N} is an infinite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\n\ntwo events,\n{\u03c9 \u2208 \u2126|X i (\u03c9) = x i } = {\u03c9 \u2208 \u2126|T (S m i (S(\u03c9)) ) = x i }.\n\n\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126 B . The sets \u2126 S, \u2126 X, and \u2126 B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\n\nH(S) = [H][(][X][)] almost surely.\n\nE[B]\n\n\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\n\n\n\nPr\n\n\ufffd\n\n\n\n\ufffd\n\n\n\nH(X) \u2264 lim\nN T \u2192\u221e [\u2212] N [1] T\n\n\n\nN T\n\ufffd\n\n\n\n\ufffd log 2 q i (X i )\n\n\ni=1\n\n\n\n= 1, (5)\n\n\n\nwhere q i (\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\n\n\n\n\n[1] N T\n\nN T \ufffd i=1 [log] 2 [q] [i] [(][X] [i] [)]\n\n\n\nH(S) \u2264 lim N T \u2192\u221e \u2212 N [1]\n\n\n\n. (6)\nE[B]\n\n\n\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by H ub . The\nnumerator in (6) represents the average number of bits required to represent the tokens X N T and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\n\n\nB. Encoding schemes\n\n\nWe consider three schemes for the lossless compression block in Fig. 3.\n\n\n5\n\n\n1) Compressing the ranks using zlib: The first scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1 LLaMA+zlib .\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token x i at epoch i by using a prefix-free code assuming q i to be the true distribution of\nthe tokens. A natural choice for a prefix-free code is a Huffman code. Instead, for simplicity, we use a prefix-free code where\n1\nthe codeword for the token x i is of length l i = \u2308log 2 q i (x i ) [\u2309][. A prefix-free code with this length for][ x] [i] [ is guaranteed to exist]\nsince this choice of lengths satisfies the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1 LLaMA+TbyT,\nis given by\n\n\n\nN T\n\ufffd\n\n\ni=1\n\n\n\n1\nlog 2\n\ufffd q i (x i )\n\n\n\n\ufffd\n\n\n\n\u03c1 LLaMA+TbyT =\n\n\n\n\ufffd Ni=1 T [b] [i] .\n\n\n\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use q i (x i ) as the probability of token x i at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1 LLM+AC . It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\n\n\n\u03c1 LLM+AC \u2248\n\n\n\nN T 1\n\ufffd i=1 log 2 q i (x i )\n\n\ufffd Ni=1 T [b] [i] . (7)\n\n\n\nClearly, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and \u03c1 LLM+AC also provide upper bounds on H(S). H ub, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and\n\u03c1 LLM+AC are estimated using a finite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\n\n\nIV. R ESULTS\n\n\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it difficult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\n[Nevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of](http://mattmahoney.net/dc/text8.zip)\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M ) is shown in Table II. As expected, the\ncompression performance improves with increasing M . We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n\n\n6\n\n\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\n\n\nV. A CKNOWLEDGEMENT\n\n\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\n\n\nTABLE I\n\nR ESULTS FOR 1MB OF TEXT FROM TEXT 8 DATASET\n\n\n\nBatch\nN c N T\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC ZPAQ pq8h\n\n\n\nNo. (bpc) file size (bits) (bpc) (bpc) (bpc) (bpc)\n\n\n1 466, 650 100, 000 0.6882 1.0513 0.8215 0.689\n\n\n2 461, 477 100, 000 0.6893 1.0558 0.8242 0.6901\n\n\n3 454, 599 100, 000 0.699 1.0681 0.8357 0.6999\n\n\n4 462, 755 100, 000 0.6748 1.0346 0.8093 0.6757\n\n\n5 453, 847 100, 000 0.7481 1.1265 0.8831 0.749\n\n\n6 458, 252 100, 000 0.7218 1.0957 0.8567 0.7227\n\n\n7 451, 036 100, 000 0.6959 1.0729 0.8353 0.6968\n\n\n8 447, 953 100, 000 0.7092 1.0896 0.8489 0.7101\n\n\n9 462, 665 100, 000 0.7394 1.1126 0.8713 0.7402\n\n\n10 449, 621 100, 000 0.7269 1.1046 0.8643 0.7277\n\n\nTotal 9, 137, 710 2, 000, 000 0.7093 1.0812 0.845 0.7101 1.4 [1] 1.2 [2]\n\n\nTABLE II\n\nC OMPRESSION PERFORMANCE OF THE LLM ON THE TEXT 8 DATASET, AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\n\n\n(bpc) file size (bits) (bpc) (bpc)\n\n\n\n31 4, 568, 855 1, 000, 000 0.9139 1.3159 1.0425 0.9145\n\n\n127 4, 568, 855 1, 000, 000 0.7511 1.1303 0.8847 0.752\n\n\n255 4, 568, 855 1, 000, 000 0.7242 1.0985 0.859 0.725\n\n\n511 4, 568, 855 1, 000, 000 0.7093 1.0812 0.845 0.7101\n\n\n1 This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2 This result is taken from [12] and it corresponds to the full 100MB dataset text8\n\n\n7\n\n\n\nTABLE III\n\nM EAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\nM\n\n(bpc) (bpc) (bpc) (bpc)\n\n\n31 0.9139 \u00b1 0.0263 1.3159 \u00b1 0.0329 1.0425 \u00b1 0.0262 0.9145 \u00b1 0.0263\n\n\n127 0.7511 \u00b1 0.0233 1.1303 \u00b1 0.0292 0.8847 \u00b1 0.0231 0.752 \u00b1 0.0233\n\n\n255 0.7242 \u00b1 0.0234 1.0985 \u00b1 0.0289 0.859 \u00b1 0.0232 0.725 \u00b1 0.0234\n\n\n511 0.7093 \u00b1 0.0228 1.0812 \u00b1 0.028 0.845 \u00b1 0.0226 0.7101 \u00b1 0.0228\n\n\nTABLE IV\n\nC OMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN P ROJECT G UTENBERG [7], AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC Standalone Zlib\n\n\n(bpc) (bpc) (bpc) (bpc) (bpc)\n\n\n\n31 508, 463 115, 000 1.0919 1.5316 1.2152 1.0924 2.80\n\n\n127 508, 463 115, 000 0.8973 1.3128 1.0235 0.8982 2.80\n\n\n255 508, 463 115, 000 0.8618 1.2684 0.9899 0.8627 2.80\n\n\n511 508, 463 115, 000 0.8417 1.2465 0.9711 0.8426 2.80\n\n\n8\n\n\nR EFERENCES\n\n\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR filters),\u201d 2023.\n\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and efficient foundation language models,\u201d\n2023.\n\n[7] J. Frank Dobie, Legends of Texas, United States, Texas Folk-Lore Society, 1924; Project Gutenberg, May 25, 2023, 2023,\n[https://www.gutenberg.org/ebooks/70859.](https://www.gutenberg.org/ebooks/70859)\n\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n\n\n"}


You ONLY have access to the following tools, and should NEVER make up tools that are not listed here:

Tool Name: download_paper
Tool Arguments: {'properties': {'check_status': {'anyOf': [], 'default': False, 'description': 'Set to true to only check the status of an ongoing or completed conversion without starting a new download. Use this to monitor long-running conversions or verify if a paper is already available.', 'enum': None, 'items': None, 'properties': {}, 'title': ''}, 'paper_id': {'anyOf': [], 'description': "The arXiv identifier of the paper to download (e.g., '2301.07041', '1706.03762', 'cs.AI/0301001'). This can be found in search results or arXiv URLs. The paper must exist on arXiv.", 'enum': None, 'items': None, 'properties': {}, 'title': '', 'type': 'string'}}, 'required': ['paper_id'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Download and convert an arXiv paper to readable markdown format for analysis and reading. This tool fetches the PDF from arXiv, converts it to markdown using advanced text extraction, and stores it locally for immediate access. Use this tool when you need to read, analyze, or work with the full text content of a specific paper. The conversion process extracts text, preserves formatting, and handles mathematical equations. Returns the full paper content directly upon successful completion.
Tool Name: extract_key_facts
Tool Arguments: {'properties': {'count': {'anyOf': [], 'default': 5, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Count'}, 'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}, 'topic_within_article': {'anyOf': [], 'default': '', 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Topic Within Article'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Extract key facts from a Wikipedia article, optionally focused on a topic.
Tool Name: fetch_content
Tool Arguments: {'properties': {'url': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Url', 'type': 'string'}}, 'required': ['url'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: 
    Fetch and parse content from a webpage URL.

    Args:
        url: The webpage URL to fetch content from
        ctx: MCP context for logging
    
Tool Name: get_article
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get the full content of a Wikipedia article.
Tool Name: get_coordinates
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get the coordinates of a Wikipedia article.
Tool Name: get_links
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get the links contained within a Wikipedia article.
Tool Name: get_related_topics
Tool Arguments: {'properties': {'limit': {'anyOf': [], 'default': 10, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Limit'}, 'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get topics related to a Wikipedia article based on links and categories.
Tool Name: get_sections
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get the sections of a Wikipedia article.
Tool Name: get_summary
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get a summary of a Wikipedia article.
Tool Name: list_papers
Tool Arguments: {'properties': {}, 'title': 'DynamicModel', 'type': 'object'}
Tool Description: List all previously downloaded and converted papers that are available in local storage for immediate reading and analysis. This tool shows you what papers you already have access to without needing to download them again. Each paper in the list includes metadata like title, authors, abstract, and direct links. Use this tool to see your paper library, check if a specific paper is already downloaded, or browse previously acquired research papers before downloading new ones.
Tool Name: read_paper
Tool Arguments: {'properties': {'paper_id': {'anyOf': [], 'description': "The arXiv identifier of the paper to read (e.g., '2301.07041', '1706.03762'). This must be a paper that has been previously downloaded and converted to markdown format. Use list_papers to see available papers.", 'enum': None, 'items': None, 'properties': {}, 'title': '', 'type': 'string'}}, 'required': ['paper_id'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Read the full text content of a previously downloaded and converted research paper in clean markdown format. This tool retrieves the complete paper content including abstract, introduction, methodology, results, conclusions, and references. The content is formatted for easy reading and analysis, with preserved mathematical equations and structured sections. Use this tool when you need to access the full text of a paper for detailed study, quotation, analysis, or research. The paper must have been previously downloaded using the download_paper tool.
Tool Name: search
Tool Arguments: {'properties': {'max_results': {'anyOf': [], 'default': 10, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Max Results'}, 'query': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Query', 'type': 'string'}}, 'required': ['query'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: 
    Search DuckDuckGo and return formatted results.

    Args:
        query: The search query string
        max_results: Maximum number of results to return (default: 10)
        ctx: MCP context for logging
    
Tool Name: search_papers
Tool Arguments: {'properties': {'categories': {'anyOf': [], 'default': None, 'description': "Strongly recommended: arXiv categories to focus search (e.g., ['cs.AI', 'cs.MA'] for agent research, ['cs.LG'] for ML, ['cs.CL'] for NLP, ['cs.CV'] for vision). Greatly improves relevance.", 'enum': None, 'items': {'type': 'string'}, 'properties': {}, 'title': ''}, 'date_from': {'anyOf': [], 'default': None, 'description': "Start date for papers (YYYY-MM-DD format). Use to find recent work, e.g., '2023-01-01' for last 2 years.", 'enum': None, 'items': None, 'properties': {}, 'title': ''}, 'date_to': {'anyOf': [], 'default': None, 'description': "End date for papers (YYYY-MM-DD format). Use with date_from to find historical work, e.g., '2020-12-31' for older research.", 'enum': None, 'items': None, 'properties': {}, 'title': ''}, 'max_results': {'anyOf': [], 'default': None, 'description': 'Maximum number of results to return (default: 10, max: 50). Use 15-20 for comprehensive searches.', 'enum': None, 'items': None, 'properties': {}, 'title': ''}, 'query': {'anyOf': [], 'description': 'Search query using quoted phrases for exact matches (e.g., \'"machine learning" OR "deep learning"\') or specific technical terms. Avoid overly broad or generic terms.', 'enum': None, 'items': None, 'properties': {}, 'title': '', 'type': 'string'}, 'sort_by': {'anyOf': [], 'default': None, 'description': "Sort results by 'relevance' (most relevant first, default) or 'date' (newest first). Use 'relevance' for focused searches, 'date' for recent developments.", 'enum': ['relevance', 'date'], 'items': None, 'properties': {}, 'title': ''}}, 'required': ['query'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Search for papers on arXiv with advanced filtering and query optimization.

QUERY CONSTRUCTION GUIDELINES:
- Use QUOTED PHRASES for exact matches: "multi-agent systems", "neural networks", "machine learning"
- Combine related concepts with OR: "AI agents" OR "software agents" OR "intelligent agents"  
- Use field-specific searches for precision:
  - ti:"exact title phrase" - search in titles only
  - au:"author name" - search by author
  - abs:"keyword" - search in abstracts only
- Use ANDNOT to exclude unwanted results: "machine learning" ANDNOT "survey"
- For best results, use 2-4 core concepts rather than long keyword lists

ADVANCED SEARCH PATTERNS:
- Field + phrase: ti:"transformer architecture" for papers with exact title phrase
- Multiple fields: au:"Smith" AND ti:"quantum" for author Smith's quantum papers  
- Exclusions: "deep learning" ANDNOT ("survey" OR "review") to exclude survey papers
- Broad + narrow: "artificial intelligence" AND (robotics OR "computer vision")

CATEGORY FILTERING (highly recommended for relevance):
- cs.AI: Artificial Intelligence
- cs.MA: Multi-Agent Systems  
- cs.LG: Machine Learning
- cs.CL: Computation and Language (NLP)
- cs.CV: Computer Vision
- cs.RO: Robotics
- cs.HC: Human-Computer Interaction
- cs.CR: Cryptography and Security
- cs.DB: Databases

EXAMPLES OF EFFECTIVE QUERIES:
- ti:"reinforcement learning" with categories: ["cs.LG", "cs.AI"] - for RL papers by title
- au:"Hinton" AND "deep learning" with categories: ["cs.LG"] - for Hinton's deep learning work
- "multi-agent" ANDNOT "survey" with categories: ["cs.MA"] - exclude survey papers
- abs:"transformer" AND ti:"attention" with categories: ["cs.CL"] - attention papers with transformer abstracts

DATE FILTERING: Use YYYY-MM-DD format for historical research:
- date_to: "2015-12-31" - for foundational/classic work (pre-2016)
- date_from: "2020-01-01" - for recent developments (post-2020)
- Both together for specific time periods

RESULT QUALITY: Results sorted by RELEVANCE (most relevant papers first), not just newest papers.
This ensures you get the most pertinent results regardless of publication date.

TIPS FOR FOUNDATIONAL RESEARCH:
- Use date_to: "2010-12-31" to find classic papers on BDI, SOAR, ACT-R
- Combine with field searches: ti:"BDI" AND abs:"belief desire intention"  
- Try author searches: au:"Rao" AND "BDI" for Anand Rao's foundational BDI work
Tool Name: search_wikipedia
Tool Arguments: {'properties': {'limit': {'anyOf': [], 'default': 10, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Limit'}, 'query': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Query', 'type': 'string'}}, 'required': ['query'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Search Wikipedia for articles matching a query.
Tool Name: summarize_article_for_query
Tool Arguments: {'properties': {'max_length': {'anyOf': [], 'default': 250, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Max Length'}, 'query': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Query', 'type': 'string'}, 'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['query', 'title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get a summary of a Wikipedia article tailored to a specific query.
Tool Name: summarize_article_section
Tool Arguments: {'properties': {'max_length': {'anyOf': [], 'default': 150, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Max Length'}, 'section_title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Section Title', 'type': 'string'}, 'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['section_title', 'title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get a summary of a specific section of a Wikipedia article.

IMPORTANT: Use the following format in your response:

```
Thought: you should always think about what to do
Action: the action to take, only one name of [download_paper, extract_key_facts, fetch_content, get_article, get_coordinates, get_links, get_related_topics, get_sections, get_summary, list_papers, read_paper, search, search_papers, search_wikipedia, summarize_article_for_query, summarize_article_section], just the name, exactly as it's written.
Action Input: the input to the action, just a simple JSON object, enclosed in curly braces, using " to wrap keys and values.
Observation: the result of the action
```

Once all necessary information is gathered, return the following format:

```
Thought: I now know the final answer
Final Answer: the final answer to the original input question
```[00m
üöÄ Crew: crew
‚îú‚îÄ‚îÄ üìã Task: 91b7630c-bea9-4b6d-93c1-621145f3eda1
‚îÇ   Status: Executing Task...
‚îÇ   ‚îú‚îÄ‚îÄ ‚úÖ Reasoning Completed
‚îÇ   ‚îú‚îÄ‚îÄ üîß Used download_paper (1)
‚îÇ   ‚îú‚îÄ‚îÄ üîß Used get_sections (1)
‚îÇ   ‚îú‚îÄ‚îÄ üîß Used read_paper (1)
‚îÇ   ‚îî‚îÄ‚îÄ ‚ùå LLM Failed
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îú‚îÄ‚îÄ üîÑ Tool Usage Started
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îî‚îÄ‚îÄ ‚úÖ Tool Usage Completed‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ LLM Error ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚ùå LLM Call Failed                                                         ‚îÇ
‚îÇ  Error: list index out of range                                             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

[91m Error during LLM call: list index out of range[00m
[91m An unknown error occurred. Please check the details below.[00m
[91m Error details: list index out of range[00m
[91m An unknown error occurred. Please check the details below.[00m
[91m Error details: list index out of range[00m




üöÄ Crew: crew
‚îú‚îÄ‚îÄ üìã Task: 91b7630c-bea9-4b6d-93c1-621145f3eda1
‚îÇ   Status: Executing Task...
‚îÇ   ‚îú‚îÄ‚îÄ ‚úÖ Reasoning Completed
‚îÇ   ‚îú‚îÄ‚îÄ üîß Used download_paper (1)
‚îÇ   ‚îú‚îÄ‚îÄ üîß Used get_sections (1)
‚îÇ   ‚îú‚îÄ‚îÄ üîß Used read_paper (1)
‚îÇ   ‚îú‚îÄ‚îÄ ‚ùå LLM Failed
‚îÇ   ‚îî‚îÄ‚îÄ üß† Reasoning...
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îú‚îÄ‚îÄ üîÑ Tool Usage Started
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îî‚îÄ‚îÄ ‚úÖ Tool Usage Completed‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ LLM Error ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚ùå LLM Call Failed                                                         ‚îÇ
‚îÇ  Error:           ‚îÇ
‚îÇ  VertexAIException - {                                                      ‚îÇ
‚îÇ    "error": {                                                               ‚îÇ
‚îÇ      "code": 503,                                                           ‚îÇ
‚îÇ      "message": "The model is overloaded. Please try again later.",         ‚îÇ
‚îÇ      "status": "UNAVAILABLE"                                                ‚îÇ
‚îÇ    }                                                                        ‚îÇ
‚îÇ  }                                                                          ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

C:\tools\Anaconda3\Lib\site-packages\paramiko\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "cipher": algorithms.TripleDES,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.
  "class": algorithms.Blowfish,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "class": algorithms.TripleDES,
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\_internal\_generate_schema.py:628: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.
  warn(
- Reading configuration...
  - Reading registry from registry.yaml
  - Reading catalog from docker-mcp.yaml
  - Reading config from config.yaml
- Configuration read in 21.7238ms
- Watching registry at C:\Users\Noah\.docker\mcp\registry.yaml
- Watching config at C:\Users\Noah\.docker\mcp\config.yaml
- Those servers are enabled: arxiv-mcp-server, duckduckgo, wikipedia-mcp
- Using images:
  - mcp/arxiv-mcp-server@sha256:6dc6bba6dfed97f4ad6eb8d23a5c98ef5b7fa6184937d54b2d675801cd9dd29e
  - mcp/duckduckgo@sha256:68eb20db6109f5c312a695fc5ec3386ad15d93ffb765a0b4eb1baf4328dec14f
  - mcp/wikipedia-mcp@sha256:e90e3f6e3bb795f10d3d8d8ac1523f6d46d4f2ab5ac3b3a4720e1be879466472
> Images pulled in 63.2469ms
- Listing MCP tools...
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
  - Running mcp/duckduckgo with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=duckduckgo -l docker-mcp-transport=stdio]
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers:ro]
  > duckduckgo: (2 tools)
  > wikipedia-mcp: (10 tools) (9 resourceTemplates)
  > arxiv-mcp-server: (4 tools) (1 prompts)
> 16 tools listed in 2.6671056s
- Watching for configuration updates...
> Initialized in 2.7531503s
> Start stdio server
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\fields.py:1093: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'items', 'anyOf', 'enum', 'properties'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
  warn(
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Crew Execution Started ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ  Crew Execution Started                                                     ‚îÇ
‚îÇ  Name: crew                                                                 ‚îÇ
‚îÇ  ID: 59b0e1c7-6440-450d-b210-bbeade0e48a8                                   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üöÄ Crew: crew
‚îú‚îÄ‚îÄ üìã Task: 30c57814-c36d-412e-95b9-229da4e91c6d
‚îÇ   Status: Executing Task...
‚îÇ   ‚îî‚îÄ‚îÄ ‚úÖ Reasoning Completed
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îú‚îÄ‚îÄ üîÑ Tool Usage Started
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îî‚îÄ‚îÄ ‚úÖ Tool Usage Completed‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üß† Reasoning Plan ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ  1. Analyze the document prefix: Carefully read the provided text to        ‚îÇ
‚îÇ  understand the paper's core topic, stated goals, methodologies mentioned,  ‚îÇ
‚îÇ  preliminary results, and the structure implied by the section headings.    ‚îÇ
‚îÇ  2. Identify explicit and implicit cues for future content:                 ‚îÇ
‚îÇ      *   Abstract: Clearly states the main contributions (new entropy       ‚îÇ
‚îÇ  estimates, lossless compression algorithm, performance comparison).        ‚îÇ
‚îÇ      *   Introduction: Elaborates on the motivation, historical context,    ‚îÇ
‚îÇ  and explicitly states the main goal ("This is the main goal of this        ‚îÇ
‚îÇ  paper. We show that when the LLaMA-7B large language model is used as the  ‚îÇ
‚îÇ  predictor, the asymptotic upper bound on the entropy is 0.709              ‚îÇ
‚îÇ  bits/character..."). It also mentions specific datasets (text8, text from  ‚îÇ
‚îÇ  [7]) and compression schemes (BSC, ZPAQ, pq8h).                            ‚îÇ
‚îÇ      *   Section II. INTUITIVE EXPLANATION OF THE MAIN IDEA: Indicates a    ‚îÇ
‚îÇ  detailed explanation of their proposed method, likely involving examples   ‚îÇ
‚îÇ  and a breakdown of how LLaMA-7B is integrated.                             ‚îÇ
‚îÇ      *   References: [1], [2], [3], [4], [5], [6], [7] are already cited,   ‚îÇ
‚îÇ  implying a "References" section will follow.                               ‚îÇ
‚îÇ  3. Formulate search queries: Based on the identified cues, formulate       ‚îÇ
‚îÇ  targeted search queries to find the full paper or related information      ‚îÇ
‚îÇ  that can confirm or elaborate on the expected future content. The arXiv    ‚îÇ
‚îÇ  ID "2306.04050v2" is a strong identifier.                                  ‚îÇ
‚îÇ  4. Use tools to retrieve information: Prioritize tools that can access     ‚îÇ
‚îÇ  academic papers or provide summaries of them.                              ‚îÇ
‚îÇ      *   Use `search_papers(query="LLMZip: Lossless Text Compression using  ‚îÇ
‚îÇ  Large Language Models arXiv: 2306.04050")` to find the paper.              ‚îÇ
‚îÇ      *   If a paper ID is returned, use `download_paper(paper_id=...)` to   ‚îÇ
‚îÇ  get the full document.                                                     ‚îÇ
‚îÇ      *   Then, use `get_sections(paper_id=...)` to understand the paper's   ‚îÇ
‚îÇ  structure.                                                                 ‚îÇ
‚îÇ      *   Subsequently, use `extract_key_facts(paper_id=...,                 ‚îÇ
‚îÇ  section_name=...)` or `summarize_article_section(paper_id=...,             ‚îÇ
‚îÇ  section_name=..., query="facts, equations, definitions, datasets,          ‚îÇ
‚îÇ  references")` on relevant sections (e.g., "Methodology," "Results,"        ‚îÇ
‚îÇ  "Experiments," "Conclusion") to extract the specific types of information  ‚îÇ
‚îÇ  requested.                                                                 ‚îÇ
‚îÇ  5. Synthesize information into a synopsis: Combine the retrieved           ‚îÇ
‚îÇ  information into a concise synopsis, ensuring it covers facts, equations   ‚îÇ
‚îÇ  (if found), definitions, datasets, and references, as requested. Adhere    ‚îÇ
‚îÇ  strictly to the "no hallucination" rule.                                   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  Expected outcome: A concise textual synopsis of the likely future content  ‚îÇ
‚îÇ  of the "LLMZip" paper. This synopsis will include:                         ‚îÇ
‚îÇ  *   Details on the methodology of combining LLaMA-7B with an arithmetic    ‚îÇ
‚îÇ  coder.                                                                     ‚îÇ
‚îÇ  *   Specific experimental setups, including datasets (text8, text from     ‚îÇ
‚îÇ  [7]) and their sizes (1MB, 100KB).                                         ‚îÇ
‚îÇ  *   Detailed results, including the asymptotic upper bound on entropy      ‚îÇ
‚îÇ  (0.709 bits/character for text8, 0.85 bits/character for text from [7])    ‚îÇ
‚îÇ  and compression ratios (0.7101 bits/character for text8, 0.8426            ‚îÇ
‚îÇ  bits/character for text from [7]).                                         ‚îÇ
‚îÇ  *   Comparisons with state-of-the-art compression schemes (BSC, ZPAQ,      ‚îÇ
‚îÇ  paq8h).                                                                    ‚îÇ
‚îÇ  *   Further discussion on the implications of these results for English    ‚îÇ
‚îÇ  entropy estimation and lossless text compression.                          ‚îÇ
‚îÇ  *   A "References" section listing the cited works.                        ‚îÇ
‚îÇ  *   Potentially, equations related to entropy estimation or the            ‚îÇ
‚îÇ  compression algorithm, if found.                                           ‚îÇ
‚îÇ  *   Definitions of key terms if they are formally introduced later in the  ‚îÇ
‚îÇ  paper.                                                                     ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  This outcome directly aligns with my goal of completing the task           ‚îÇ
‚îÇ  accurately and efficiently by providing a factual, relevant, and concise   ‚îÇ
‚îÇ  summary of the paper's expected future content, based on authoritative     ‚îÇ
‚îÇ  sources.                                                                   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Task:[00m [92mYou act as a deterministic research agent. Given the observed prefix of a document, use available MCP tools to retrieve information that is likely to appear later in the same document. Prefer authoritative sources. Produce a concise textual synopsis of the likely future content (facts, equations, definitions, datasets, references), suitable for conditioning a language model compressor. Maximize relevance to the document. Do not hallucinate; only include content supported by the tools.

Document prefix (UTF-8 text):
# LLMZip: Lossless Text Compression using Large Language Models

**Authors:** Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas Shakkottai  
**Affiliation:** Department of Electrical and Computer Engineering, Texas A&M University  
**Email:** {vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu  
**arXiv:** 2306.04050v2 [cs.IT] 26 Jun 2023

## Abstract

We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.

## I. INTRODUCTION

There are close connections between learning, prediction, and compression. The success of ChatGPT has captured the fascination of general public and brought the connection between learning and prediction to the fore. The main advance brought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in a paragraph based on knowing the past several words (tokens).

The connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the entropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past values can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many algorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of lossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very effective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor and every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression performance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved results for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ for text compression.

It is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the entropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this paper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the entropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates provided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of the text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for compression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression ratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio obtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.

## II. INTUITIVE EXPLANATION OF THE MAIN IDEA

We will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3] for estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable length and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence that reads as

My first attempt at writing a book

Our goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can be reconstructed from the sequence of bits. This sentence can first be split into a sequence of

Output: A concise synopsis (plain text).

Reasoning Plan:
1. Analyze the document prefix: Carefully read the provided text to understand the paper's core topic, stated goals, methodologies mentioned, preliminary results, and the structure implied by the section headings.
2. Identify explicit and implicit cues for future content:
    *   Abstract: Clearly states the main contributions (new entropy estimates, lossless compression algorithm, performance comparison).
    *   Introduction: Elaborates on the motivation, historical context, and explicitly states the main goal ("This is the main goal of this paper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the entropy is 0.709 bits/character..."). It also mentions specific datasets (text8, text from [7]) and compression schemes (BSC, ZPAQ, pq8h).
    *   Section II. INTUITIVE EXPLANATION OF THE MAIN IDEA: Indicates a detailed explanation of their proposed method, likely involving examples and a breakdown of how LLaMA-7B is integrated.
    *   References: [1], [2], [3], [4], [5], [6], [7] are already cited, implying a "References" section will follow.
3. Formulate search queries: Based on the identified cues, formulate targeted search queries to find the full paper or related information that can confirm or elaborate on the expected future content. The arXiv ID "2306.04050v2" is a strong identifier.
4. Use tools to retrieve information: Prioritize tools that can access academic papers or provide summaries of them.
    *   Use `search_papers(query="LLMZip: Lossless Text Compression using Large Language Models arXiv: 2306.04050")` to find the paper.
    *   If a paper ID is returned, use `download_paper(paper_id=...)` to get the full document.
    *   Then, use `get_sections(paper_id=...)` to understand the paper's structure.
    *   Subsequently, use `extract_key_facts(paper_id=..., section_name=...)` or `summarize_article_section(paper_id=..., section_name=..., query="facts, equations, definitions, datasets, references")` on relevant sections (e.g., "Methodology," "Results," "Experiments," "Conclusion") to extract the specific types of information requested.
5. Synthesize information into a synopsis: Combine the retrieved information into a concise synopsis, ensuring it covers facts, equations (if found), definitions, datasets, and references, as requested. Adhere strictly to the "no hallucination" rule.

Expected outcome: A concise textual synopsis of the likely future content of the "LLMZip" paper. This synopsis will include:
*   Details on the methodology of combining LLaMA-7B with an arithmetic coder.
*   Specific experimental setups, including datasets (text8, text from [7]) and their sizes (1MB, 100KB).
*   Detailed results, including the asymptotic upper bound on entropy (0.709 bits/character for text8, 0.85 bits/character for text from [7]) and compression ratios (0.7101 bits/character for text8, 0.8426 bits/character for text from [7]).
*   Comparisons with state-of-the-art compression schemes (BSC, ZPAQ, paq8h).
*   Further discussion on the implications of these results for English entropy estimation and lossless text compression.
*   A "References" section listing the cited works.
*   Potentially, equations related to entropy estimation or the compression algorithm, if found.
*   Definitions of key terms if they are formally introduced later in the paper.

  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool download_paper with arguments: {"check_status":false,"paper_id":"2306.04050"}
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
  > Calling tool download_paper took: 1.6872549s
  - Scanning tool call response for secrets...
  > No secret found in response.
This outcome directly aligns with my goal of completing the task accurately and efficiently by providing a factual, relevant, and concise summary of the paper's expected future content, based on authoritative sources.[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mAction: download_paper[00m
[95m## Using tool:[00m [92mdownload_paper[00m
[95m## Tool Input:[00m [92m
"{\"paper_id\": \"2306.04050\"}"[00m
[95m## Tool Output:[00m [92m
{"status": "success", "message": "Paper already available", "resource_uri": "file:///app/papers/2306.04050.md", "pdf_uri": "https://arxiv.org/pdf/2306.04050.pdf", "content": "1\n\n# LLMZip: Lossless Text Compression using Large Language Models\n\n\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\n\n\nAbstract\n\n\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\n\n\nI. I NTRODUCTION\n\n\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\n\n\nII. I NTUITIVE EXPLANATION OF THE MAIN IDEA\n\n\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\n\nMy first attempt at writing a book\n\n\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032, \u2032 a \u2032, \u2032 book \u2032\n\n\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n\n\n2\n\n\nFigure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be\nwords such as [\u2032] reading [\u2032], [\u2032] writing [\u2032], [\u2032] driving [\u2032], [\u2032] cooking [\u2032] etc. The main idea is to compute the rank of the actual word in\nour sentence ( [\u2032] writing [\u2032] ) in this list and call it R 5 . We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for [\u2032] writing [\u2032] is R 5 = 1.\n\n\nTokens\n\n\n\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x5|x4 1)<br>5|Rank<br>computation|R5 = 1|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nreading 0.3\nwriting 0.2\ncycling 0.1\ndriving 0.05\n\n... ...\n\n\n\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\n\n\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, [\u2032] a [\u2032], and hence, the rank R 6 would be 0.\n\n\nTokens\n\n\n\n\n\n\u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x6|x5 2)<br>6|Rank<br>computation|R6 = 0|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nan 0.2\n\nthe 0.05\n\n... ...\n\n\n\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\n\n\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n\n\n1, 0, 0, . . .\n\n\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\n\n\nSequence of ranks\n\n|e of ranks|Col2|Col3|\n|---|---|---|\n|. ., rNT|Compression (zip)|Nb|\n||||\n\n\n\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\n\n\nWhen we wish to reconstruct the sequence, we first decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank R i during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\n\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n\n\n3\n\n\nIII. C OMPRESSION USING LLM S\n\n\nLet s denote a sentence from the English language composed of N c letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We first parse s into a sequence of N T tokens\ndenoted by x = x 1, x 2, . . ., x i\u22121, x i, x i+1, . . . x N T, where x i \u2208X . There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. x i \u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter X i .\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens x i\u2212M, x i\u2212M+1, . . ., x i\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nq i (x i ) := Pr(X i = x i |x i\u22121, x i\u22122, . . ., x i\u2212M ), \u2200x i \u2208X . The PMF vector q i := [q i (1), q i (2), . . ., q i (D)] [T] is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcq i . Let \u03b3 i : X \u2192X be a permutation on the integers\nfrom 1 to D such that\n\n\u02dc\nq i (\u03b3 i (j)) = q i (j), \u2200j \u2208X .\n\n\nThat is, \u03b3 i (j) is the rank of the token j at epoch i. We define the rank of the input sequence at epoch i as the rank of the\ntoken x i at epoch i, r i := \u03b3 i (x i ). The sequence {r i } [N] i=1 [T] [is compressed by a lossless compression algorithm (such as zlib) to]\nproduce N b bits which are the final bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors q i \u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 defined as\n\n\u03c1 := [N] [b] bits/character.\n\nN c\n\n\n\nLosseless\n\nCompression\n\n\n\nTokens . . ., x i\u2212M, . . ., x i\u22121\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n\nInput sentence s with N c characters\n\n\n\ni i i\u22121 Rank\n\nLLM\ncomputation\n\n\n\nq i = Pr(X i = j|x [i] i [\u2212] \u2212 [M] 1 [)] Rank r i Losseless N b bits\n\n\n\nFig. 4. Schematic showing the prediction at epoch i.\n\n\nA. Entropy bounds\n\n\nLet S \u2208S [\u221e] be a random process that represents language input. The nth character in the sequence is denoted by S n,\nwhereas the string of characters from the beginning to the nth character is expressed as S n . The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X 1, X 2, . . . using a variable-length mapping. In this sequence, X i is the ith\ntoken. The number of characters employed to generate X i depends on the realization of the random process and, as such,\nwe introduce random variable B i to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which B i \u2265 1 and B i is uniformly bounded, with B i < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded N T \u2192\u221e, the number of characters must also approach infinity N c \u2192\u221e. Formally, consider the tokenizer\nfunction T : S [N] \u2192X [N] operating on infinite symbol sequences; that is, T (s) = x where s is an infinite sequence in S [\u221e] . For\nnatural number, i \u2208 N, define m i : S [N] \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Specifically, suppose s is given, then\n\n\nm i (s) = min n [{][length (][T][ (][s] [n] [))][ \u2265] [i][}][ .] (1)\n\n\nWe note that, by construction, lim n\u2192\u221e length (T (s n )) = \u221e and, as such, m i (\u00b7) is well-defined. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple finite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (s n ) = T (s n+1 ). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (finite) series of input symbols in the restricted\nset for which T (y n ) = x i is s m i (s) . Given a fixed sequence s, we can express the number of characters contained in a token\n\nas\nb i = m i (s) \u2212 m i\u22121 (s)\n\n\n4\n\n\nwith initial condition m \u22121 = 0. Consequently, the number of characters embedded in the first N T tokens for a random input\nbecomes N c = [\ufffd] i [N] =1 [T] [B] [i] [.]\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {S k } [\u221e] k=1 [,]\n{B i } [\u221e] i=1 [, and][ {][X] [i] [}] [\u221e] i=1 [are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that]\n\n\n\n\u2212 [1]\n\n\n\n\n[1]\n\nn [log] [2] [ p] [S] [n] [(][S] [1] [, . . ., S] [n] [) =][ \u2212] n [1]\n\n\n\nn [log] [2] [ p] [S] [n] [(][S] [n] [)][ \u2192] [H][(][S][)] almost surely. (2)\n\n\n\nLet \u2126 S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\n\n\n\n[1]\n\ni [log] [2] [ p] [X] [i] [(][X] [1] [, . . ., X] [i] [) =][ \u2212] [1] i\n\n\n\n\u2212 [1]\n\n\n\ni [log] [2] [ p] [X] [i] [(][X] [i] [)][ \u2192] [H][(][X][)] almost surely. (3)\n\n\n\nDefine \u2126 X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\n\n\nm i (S)\nlim = E [B] almost surely. (4)\ni\u2192\u221e i\n\n\nSet \u2126 B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126 S \u2229 \u2126 X \u2229 \u2126 B, we deduce that\n\n\nH(S) = lim\nk\u2192\u221e [\u2212] k [1] [log] [2] [ p] [S] [k] [(][S] [k] [(][\u03c9][))]\n\n= lim i\u2192\u221e [\u2212] l [1] i log 2 p S li (S l i (\u03c9))\n\n\n= lim log 2 Pr (X i = T (S l i (\u03c9)))\ni\u2192\u221e [\u2212] l [1] i\n\n\n\n1\n\ni [log] [2] [ Pr (][X] [i] [ =][ x] [i] [) =][ H] E [(] B [X][)]\n\n\n\n1 1\n\n= \u2212\nE[B] i [lim] \u2192\u221e i\n\n\n\nE[B] [.]\n\n\n\nThe first equality follows from (2). The second equality is a consequence of the fact that {l i = m i (S(\u03c9))|i \u2208 N} is an infinite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\n\ntwo events,\n{\u03c9 \u2208 \u2126|X i (\u03c9) = x i } = {\u03c9 \u2208 \u2126|T (S m i (S(\u03c9)) ) = x i }.\n\n\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126 B . The sets \u2126 S, \u2126 X, and \u2126 B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\n\nH(S) = [H][(][X][)] almost surely.\n\nE[B]\n\n\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\n\n\n\nPr\n\n\ufffd\n\n\n\n\ufffd\n\n\n\nH(X) \u2264 lim\nN T \u2192\u221e [\u2212] N [1] T\n\n\n\nN T\n\ufffd\n\n\n\n\ufffd log 2 q i (X i )\n\n\ni=1\n\n\n\n= 1, (5)\n\n\n\nwhere q i (\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\n\n\n\n\n[1] N T\n\nN T \ufffd i=1 [log] 2 [q] [i] [(][X] [i] [)]\n\n\n\nH(S) \u2264 lim N T \u2192\u221e \u2212 N [1]\n\n\n\n. (6)\nE[B]\n\n\n\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by H ub . The\nnumerator in (6) represents the average number of bits required to represent the tokens X N T and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\n\n\nB. Encoding schemes\n\n\nWe consider three schemes for the lossless compression block in Fig. 3.\n\n\n5\n\n\n1) Compressing the ranks using zlib: The first scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1 LLaMA+zlib .\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token x i at epoch i by using a prefix-free code assuming q i to be the true distribution of\nthe tokens. A natural choice for a prefix-free code is a Huffman code. Instead, for simplicity, we use a prefix-free code where\n1\nthe codeword for the token x i is of length l i = \u2308log 2 q i (x i ) [\u2309][. A prefix-free code with this length for][ x] [i] [ is guaranteed to exist]\nsince this choice of lengths satisfies the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1 LLaMA+TbyT,\nis given by\n\n\n\nN T\n\ufffd\n\n\ni=1\n\n\n\n1\nlog 2\n\ufffd q i (x i )\n\n\n\n\ufffd\n\n\n\n\u03c1 LLaMA+TbyT =\n\n\n\n\ufffd Ni=1 T [b] [i] .\n\n\n\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use q i (x i ) as the probability of token x i at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1 LLM+AC . It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\n\n\n\u03c1 LLM+AC \u2248\n\n\n\nN T 1\n\ufffd i=1 log 2 q i (x i )\n\n\ufffd Ni=1 T [b] [i] . (7)\n\n\n\nClearly, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and \u03c1 LLM+AC also provide upper bounds on H(S). H ub, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and\n\u03c1 LLM+AC are estimated using a finite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\n\n\nIV. R ESULTS\n\n\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it difficult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\n[Nevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of](http://mattmahoney.net/dc/text8.zip)\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M ) is shown in Table II. As expected, the\ncompression performance improves with increasing M . We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n\n\n6\n\n\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\n\n\nV. A CKNOWLEDGEMENT\n\n\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\n\n\nTABLE I\n\nR ESULTS FOR 1MB OF TEXT FROM TEXT 8 DATASET\n\n\n\nBatch\nN c N T\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC ZPAQ pq8h\n\n\n\nNo. (bpc) file size (bits) (bpc) (bpc) (bpc) (bpc)\n\n\n1 466, 650 100, 000 0.6882 1.0513 0.8215 0.689\n\n\n2 461, 477 100, 000 0.6893 1.0558 0.8242 0.6901\n\n\n3 454, 599 100, 000 0.699 1.0681 0.8357 0.6999\n\n\n4 462, 755 100, 000 0.6748 1.0346 0.8093 0.6757\n\n\n5 453, 847 100, 000 0.7481 1.1265 0.8831 0.749\n\n\n6 458, 252 100, 000 0.7218 1.0957 0.8567 0.7227\n\n\n7 451, 036 100, 000 0.6959 1.0729 0.8353 0.6968\n\n\n8 447, 953 100, 000 0.7092 1.0896 0.8489 0.7101\n\n\n9 462, 665 100, 000 0.7394 1.1126 0.8713 0.7402\n\n\n10 449, 621 100, 000 0.7269 1.1046 0.8643 0.7277\n\n\nTotal 9, 137, 710 2, 000, 000 0.7093 1.0812 0.845 0.7101 1.4 [1] 1.2 [2]\n\n\nTABLE II\n\nC OMPRESSION PERFORMANCE OF THE LLM ON THE TEXT 8 DATASET, AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\n\n\n(bpc) file size (bits) (bpc) (bpc)\n\n\n\n31 4, 568, 855 1, 000, 000 0.9139 1.3159 1.0425 0.9145\n\n\n127 4, 568, 855 1, 000, 000 0.7511 1.1303 0.8847 0.752\n\n\n255 4, 568, 855 1, 000, 000 0.7242 1.0985 0.859 0.725\n\n\n511 4, 568, 855 1, 000, 000 0.7093 1.0812 0.845 0.7101\n\n\n1 This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2 This result is taken from [12] and it corresponds to the full 100MB dataset text8\n\n\n7\n\n\n\nTABLE III\n\nM EAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\nM\n\n(bpc) (bpc) (bpc) (bpc)\n\n\n31 0.9139 \u00b1 0.0263 1.3159 \u00b1 0.0329 1.0425 \u00b1 0.0262 0.9145 \u00b1 0.0263\n\n\n127 0.7511 \u00b1 0.0233 1.1303 \u00b1 0.0292 0.8847 \u00b1 0.0231 0.752 \u00b1 0.0233\n\n\n255 0.7242 \u00b1 0.0234 1.0985 \u00b1 0.0289 0.859 \u00b1 0.0232 0.725 \u00b1 0.0234\n\n\n511 0.7093 \u00b1 0.0228 1.0812 \u00b1 0.028 0.845 \u00b1 0.0226 0.7101 \u00b1 0.0228\n\n\nTABLE IV\n\nC OMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN P ROJECT G UTENBERG [7], AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC Standalone Zlib\n\n\n(bpc) (bpc) (bpc) (bpc) (bpc)\n\n\n\n31 508, 463 115, 000 1.0919 1.5316 1.2152 1.0924 2.80\n\n\n127 508, 463 115, 000 0.8973 1.3128 1.0235 0.8982 2.80\n\n\n255 508, 463 115, 000 0.8618 1.2684 0.9899 0.8627 2.80\n\n\n511 508, 463 115, 000 0.8417 1.2465 0.9711 0.8426 2.80\n\n\n8\n\n\nR EFERENCES\n\n\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR filters),\u201d 2023.\n\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and efficient foundation language models,\u201d\n2023.\n\n[7] J. Frank Dobie, Legends of Texas, United States, Texas Folk-Lore Society, 1924; Project Gutenberg, May 25, 2023, 2023,\n[https://www.gutenberg.org/ebooks/70859.](https://www.gutenberg.org/ebooks/70859)\n\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n\n\n"}[00mC:\tools\Anaconda3\Lib\site-packages\paramiko\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "cipher": algorithms.TripleDES,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.
  "class": algorithms.Blowfish,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "class": algorithms.TripleDES,
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\_internal\_generate_schema.py:628: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.
  warn(
- Reading configuration...
  - Reading registry from registry.yaml
  - Reading catalog from docker-mcp.yaml
  - Reading config from config.yaml
- Configuration read in 20.0213ms
- Watching registry at C:\Users\Noah\.docker\mcp\registry.yaml
- Watching config at C:\Users\Noah\.docker\mcp\config.yaml
- Those servers are enabled: arxiv-mcp-server, duckduckgo, wikipedia-mcp
- Using images:
  - mcp/arxiv-mcp-server@sha256:6dc6bba6dfed97f4ad6eb8d23a5c98ef5b7fa6184937d54b2d675801cd9dd29e
  - mcp/duckduckgo@sha256:68eb20db6109f5c312a695fc5ec3386ad15d93ffb765a0b4eb1baf4328dec14f
  - mcp/wikipedia-mcp@sha256:e90e3f6e3bb795f10d3d8d8ac1523f6d46d4f2ab5ac3b3a4720e1be879466472
> Images pulled in 65.312ms
- Listing MCP tools...
  - Running mcp/duckduckgo with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=duckduckgo -l docker-mcp-transport=stdio]
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers:ro]
  > duckduckgo: (2 tools)
  > wikipedia-mcp: (10 tools) (9 resourceTemplates)
  > arxiv-mcp-server: (4 tools) (1 prompts)
> 16 tools listed in 3.1106374s
- Watching for configuration updates...
> Initialized in 3.1970321s
> Start stdio server
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\fields.py:1093: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'items', 'anyOf', 'enum', 'properties'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
  warn(
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Crew Execution Started ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ  Crew Execution Started                                                     ‚îÇ
‚îÇ  Name: crew                                                                 ‚îÇ
‚îÇ  ID: 00aaf669-5a79-40dc-9242-76268973947e                                   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üöÄ Crew: crew
‚îú‚îÄ‚îÄ üìã Task: c8ca34b1-9034-4e11-8900-1d06ebc68bed
‚îÇ   Status: Executing Task...
‚îÇ   ‚îî‚îÄ‚îÄ ‚úÖ Reasoning Completed
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îú‚îÄ‚îÄ üîÑ Tool Usage Started
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îî‚îÄ‚îÄ ‚úÖ Tool Usage Completed‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üß† Reasoning Plan ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ  1. Analyze the document prefix: Carefully read the provided text to        ‚îÇ
‚îÇ  understand the paper's core topic, stated goals, methodologies mentioned,  ‚îÇ
‚îÇ  preliminary results, and the structure implied by the section headings.    ‚îÇ
‚îÇ  2. Identify explicit and implicit cues for future content:                 ‚îÇ
‚îÇ      *   Abstract: Clearly states the main contributions (new entropy       ‚îÇ
‚îÇ  estimates, lossless compression algorithm, performance comparison).        ‚îÇ
‚îÇ      *   Introduction: Elaborates on the motivation, historical context,    ‚îÇ
‚îÇ  and explicitly states the main goal ("This is the main goal of this        ‚îÇ
‚îÇ  paper. We show that when the LLaMA-7B large language model is used as the  ‚îÇ
‚îÇ  predictor, the asymptotic upper bound on the entropy is 0.709              ‚îÇ
‚îÇ  bits/character..."). It also mentions specific datasets (text8, text from  ‚îÇ
‚îÇ  [7]) and compression schemes (BSC, ZPAQ, pq8h).                            ‚îÇ
‚îÇ      *   Section II. INTUITIVE EXPLANATION OF THE MAIN IDEA: Indicates a    ‚îÇ
‚îÇ  detailed explanation of their proposed method, likely involving examples   ‚îÇ
‚îÇ  and a breakdown of how LLaMA-7B is integrated.                             ‚îÇ
‚îÇ      *   References: [1], [2], [3], [4], [5], [6], [7] are already cited,   ‚îÇ
‚îÇ  implying a "References" section will follow.                               ‚îÇ
‚îÇ  3. Formulate search queries: Based on the identified cues, formulate       ‚îÇ
‚îÇ  targeted search queries to find the full paper or related information      ‚îÇ
‚îÇ  that can confirm or elaborate on the expected future content. The arXiv    ‚îÇ
‚îÇ  ID "2306.04050v2" is a strong identifier.                                  ‚îÇ
‚îÇ  4. Use tools to retrieve information: Prioritize tools that can access     ‚îÇ
‚îÇ  academic papers or provide summaries of them.                              ‚îÇ
‚îÇ      *   Use `search_papers(query="LLMZip: Lossless Text Compression using  ‚îÇ
‚îÇ  Large Language Models arXiv: 2306.04050")` to find the paper.              ‚îÇ
‚îÇ      *   If a paper ID is returned, use `download_paper(paper_id=...)` to   ‚îÇ
‚îÇ  get the full document.                                                     ‚îÇ
‚îÇ      *   Then, use `get_sections(paper_id=...)` to understand the paper's   ‚îÇ
‚îÇ  structure.                                                                 ‚îÇ
‚îÇ      *   Subsequently, use `extract_key_facts(paper_id=...,                 ‚îÇ
‚îÇ  section_name=...)` or `summarize_article_section(paper_id=...,             ‚îÇ
‚îÇ  section_name=..., query="facts, equations, definitions, datasets,          ‚îÇ
‚îÇ  references")` on relevant sections (e.g., "Methodology," "Results,"        ‚îÇ
‚îÇ  "Experiments," "Conclusion") to extract the specific types of information  ‚îÇ
‚îÇ  requested.                                                                 ‚îÇ
‚îÇ  5. Synthesize information into a synopsis: Combine the retrieved           ‚îÇ
‚îÇ  information into a concise synopsis, ensuring it covers facts, equations   ‚îÇ
‚îÇ  (if found), definitions, datasets, and references, as requested. Adhere    ‚îÇ
‚îÇ  strictly to the "no hallucination" rule.                                   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  Expected outcome: A concise textual synopsis of the likely future content  ‚îÇ
‚îÇ  of the "LLMZip" paper. This synopsis will include:                         ‚îÇ
‚îÇ  *   Details on the methodology of combining LLaMA-7B with an arithmetic    ‚îÇ
‚îÇ  coder.                                                                     ‚îÇ
‚îÇ  *   Specific experimental setups, including datasets (text8, text from     ‚îÇ
‚îÇ  [7]) and their sizes (1MB, 100KB).                                         ‚îÇ
‚îÇ  *   Detailed results, including the asymptotic upper bound on entropy      ‚îÇ
‚îÇ  (0.709 bits/character for text8, 0.85 bits/character for text from [7])    ‚îÇ
‚îÇ  and compression ratios (0.7101 bits/character for text8, 0.8426            ‚îÇ
‚îÇ  bits/character for text from [7]).                                         ‚îÇ
‚îÇ  *   Comparisons with state-of-the-art compression schemes (BSC, ZPAQ,      ‚îÇ
‚îÇ  paq8h).                                                                    ‚îÇ
‚îÇ  *   Further discussion on the implications of these results for English    ‚îÇ
‚îÇ  entropy estimation and lossless text compression.                          ‚îÇ
‚îÇ  *   A "References" section listing the cited works.                        ‚îÇ
‚îÇ  *   Potentially, equations related to entropy estimation or the            ‚îÇ
‚îÇ  compression algorithm, if found.                                           ‚îÇ
‚îÇ  *   Definitions of key terms if they are formally introduced later in the  ‚îÇ
‚îÇ  paper.                                                                     ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  This outcome directly aligns with my goal of completing the task           ‚îÇ
‚îÇ  accurately and efficiently by providing a factual, relevant, and concise   ‚îÇ
‚îÇ  summary of the paper's expected future content, based on authoritative     ‚îÇ
‚îÇ  sources.                                                                   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Task:[00m [92mYou act as a deterministic research agent. Given the observed prefix of a document, use available MCP tools to retrieve information that is likely to appear later in the same document. Prefer authoritative sources. Produce a concise textual synopsis of the likely future content (facts, equations, definitions, datasets, references), suitable for conditioning a language model compressor. Maximize relevance to the document. Do not hallucinate; only include content supported by the tools.

Document prefix (UTF-8 text):
# LLMZip: Lossless Text Compression using Large Language Models

**Authors:** Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas Shakkottai  
**Affiliation:** Department of Electrical and Computer Engineering, Texas A&M University  
**Email:** {vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu  
**arXiv:** 2306.04050v2 [cs.IT] 26 Jun 2023

## Abstract

We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.

## I. INTRODUCTION

There are close connections between learning, prediction, and compression. The success of ChatGPT has captured the fascination of general public and brought the connection between learning and prediction to the fore. The main advance brought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in a paragraph based on knowing the past several words (tokens).

The connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the entropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past values can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many algorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of lossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very effective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor and every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression performance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved results for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ for text compression.

It is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the entropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this paper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the entropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates provided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of the text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for compression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression ratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio obtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.

## II. INTUITIVE EXPLANATION OF THE MAIN IDEA

We will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3] for estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable length and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence that reads as

My first attempt at writing a book

Our goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can be reconstructed from the sequence of bits. This sentence can first be split into a sequence of

Output: A concise synopsis (plain text).

Reasoning Plan:
1. Analyze the document prefix: Carefully read the provided text to understand the paper's core topic, stated goals, methodologies mentioned, preliminary results, and the structure implied by the section headings.
2. Identify explicit and implicit cues for future content:
    *   Abstract: Clearly states the main contributions (new entropy estimates, lossless compression algorithm, performance comparison).
    *   Introduction: Elaborates on the motivation, historical context, and explicitly states the main goal ("This is the main goal of this paper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the entropy is 0.709 bits/character..."). It also mentions specific datasets (text8, text from [7]) and compression schemes (BSC, ZPAQ, pq8h).
    *   Section II. INTUITIVE EXPLANATION OF THE MAIN IDEA: Indicates a detailed explanation of their proposed method, likely involving examples and a breakdown of how LLaMA-7B is integrated.
    *   References: [1], [2], [3], [4], [5], [6], [7] are already cited, implying a "References" section will follow.
3. Formulate search queries: Based on the identified cues, formulate targeted search queries to find the full paper or related information that can confirm or elaborate on the expected future content. The arXiv ID "2306.04050v2" is a strong identifier.
4. Use tools to retrieve information: Prioritize tools that can access academic papers or provide summaries of them.
    *   Use `search_papers(query="LLMZip: Lossless Text Compression using Large Language Models arXiv: 2306.04050")` to find the paper.
    *   If a paper ID is returned, use `download_paper(paper_id=...)` to get the full document.
    *   Then, use `get_sections(paper_id=...)` to understand the paper's structure.
    *   Subsequently, use `extract_key_facts(paper_id=..., section_name=...)` or `summarize_article_section(paper_id=..., section_name=..., query="facts, equations, definitions, datasets, references")` on relevant sections (e.g., "Methodology," "Results," "Experiments," "Conclusion") to extract the specific types of information requested.
5. Synthesize information into a synopsis: Combine the retrieved information into a concise synopsis, ensuring it covers facts, equations (if found), definitions, datasets, and references, as requested. Adhere strictly to the "no hallucination" rule.

Expected outcome: A concise textual synopsis of the likely future content of the "LLMZip" paper. This synopsis will include:
*   Details on the methodology of combining LLaMA-7B with an arithmetic coder.
*   Specific experimental setups, including datasets (text8, text from [7]) and their sizes (1MB, 100KB).
*   Detailed results, including the asymptotic upper bound on entropy (0.709 bits/character for text8, 0.85 bits/character for text from [7]) and compression ratios (0.7101 bits/character for text8, 0.8426 bits/character for text from [7]).
*   Comparisons with state-of-the-art compression schemes (BSC, ZPAQ, paq8h).
*   Further discussion on the implications of these results for English entropy estimation and lossless text compression.
*   A "References" section listing the cited works.
*   Potentially, equations related to entropy estimation or the compression algorithm, if found.
*   Definitions of key terms if they are formally introduced later in the paper.

  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool download_paper with arguments: {"check_status":false,"paper_id":"2306.04050"}
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
  > Calling tool download_paper took: 1.5925999s
  - Scanning tool call response for secrets...
  > No secret found in response.
This outcome directly aligns with my goal of completing the task accurately and efficiently by providing a factual, relevant, and concise summary of the paper's expected future content, based on authoritative sources.[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mAction: download_paper[00m
[95m## Using tool:[00m [92mdownload_paper[00m
[95m## Tool Input:[00m [92m
"{\"paper_id\": \"2306.04050\"}"[00m
[95m## Tool Output:[00m [92m
{"status": "success", "message": "Paper already available", "resource_uri": "file:///app/papers/2306.04050.md", "pdf_uri": "https://arxiv.org/pdf/2306.04050.pdf", "content": "1\n\n# LLMZip: Lossless Text Compression using Large Language Models\n\n\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\n\n\nAbstract\n\n\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\n\n\nI. I NTRODUCTION\n\n\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\n\n\nII. I NTUITIVE EXPLANATION OF THE MAIN IDEA\n\n\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\n\nMy first attempt at writing a book\n\n\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032, \u2032 a \u2032, \u2032 book \u2032\n\n\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n\n\n2\n\n\nFigure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be\nwords such as [\u2032] reading [\u2032], [\u2032] writing [\u2032], [\u2032] driving [\u2032], [\u2032] cooking [\u2032] etc. The main idea is to compute the rank of the actual word in\nour sentence ( [\u2032] writing [\u2032] ) in this list and call it R 5 . We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for [\u2032] writing [\u2032] is R 5 = 1.\n\n\nTokens\n\n\n\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x5|x4 1)<br>5|Rank<br>computation|R5 = 1|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nreading 0.3\nwriting 0.2\ncycling 0.1\ndriving 0.05\n\n... ...\n\n\n\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\n\n\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, [\u2032] a [\u2032], and hence, the rank R 6 would be 0.\n\n\nTokens\n\n\n\n\n\n\u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x6|x5 2)<br>6|Rank<br>computation|R6 = 0|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nan 0.2\n\nthe 0.05\n\n... ...\n\n\n\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\n\n\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n\n\n1, 0, 0, . . .\n\n\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\n\n\nSequence of ranks\n\n|e of ranks|Col2|Col3|\n|---|---|---|\n|. ., rNT|Compression (zip)|Nb|\n||||\n\n\n\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\n\n\nWhen we wish to reconstruct the sequence, we first decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank R i during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\n\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n\n\n3\n\n\nIII. C OMPRESSION USING LLM S\n\n\nLet s denote a sentence from the English language composed of N c letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We first parse s into a sequence of N T tokens\ndenoted by x = x 1, x 2, . . ., x i\u22121, x i, x i+1, . . . x N T, where x i \u2208X . There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. x i \u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter X i .\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens x i\u2212M, x i\u2212M+1, . . ., x i\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nq i (x i ) := Pr(X i = x i |x i\u22121, x i\u22122, . . ., x i\u2212M ), \u2200x i \u2208X . The PMF vector q i := [q i (1), q i (2), . . ., q i (D)] [T] is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcq i . Let \u03b3 i : X \u2192X be a permutation on the integers\nfrom 1 to D such that\n\n\u02dc\nq i (\u03b3 i (j)) = q i (j), \u2200j \u2208X .\n\n\nThat is, \u03b3 i (j) is the rank of the token j at epoch i. We define the rank of the input sequence at epoch i as the rank of the\ntoken x i at epoch i, r i := \u03b3 i (x i ). The sequence {r i } [N] i=1 [T] [is compressed by a lossless compression algorithm (such as zlib) to]\nproduce N b bits which are the final bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors q i \u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 defined as\n\n\u03c1 := [N] [b] bits/character.\n\nN c\n\n\n\nLosseless\n\nCompression\n\n\n\nTokens . . ., x i\u2212M, . . ., x i\u22121\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n\nInput sentence s with N c characters\n\n\n\ni i i\u22121 Rank\n\nLLM\ncomputation\n\n\n\nq i = Pr(X i = j|x [i] i [\u2212] \u2212 [M] 1 [)] Rank r i Losseless N b bits\n\n\n\nFig. 4. Schematic showing the prediction at epoch i.\n\n\nA. Entropy bounds\n\n\nLet S \u2208S [\u221e] be a random process that represents language input. The nth character in the sequence is denoted by S n,\nwhereas the string of characters from the beginning to the nth character is expressed as S n . The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X 1, X 2, . . . using a variable-length mapping. In this sequence, X i is the ith\ntoken. The number of characters employed to generate X i depends on the realization of the random process and, as such,\nwe introduce random variable B i to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which B i \u2265 1 and B i is uniformly bounded, with B i < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded N T \u2192\u221e, the number of characters must also approach infinity N c \u2192\u221e. Formally, consider the tokenizer\nfunction T : S [N] \u2192X [N] operating on infinite symbol sequences; that is, T (s) = x where s is an infinite sequence in S [\u221e] . For\nnatural number, i \u2208 N, define m i : S [N] \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Specifically, suppose s is given, then\n\n\nm i (s) = min n [{][length (][T][ (][s] [n] [))][ \u2265] [i][}][ .] (1)\n\n\nWe note that, by construction, lim n\u2192\u221e length (T (s n )) = \u221e and, as such, m i (\u00b7) is well-defined. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple finite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (s n ) = T (s n+1 ). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (finite) series of input symbols in the restricted\nset for which T (y n ) = x i is s m i (s) . Given a fixed sequence s, we can express the number of characters contained in a token\n\nas\nb i = m i (s) \u2212 m i\u22121 (s)\n\n\n4\n\n\nwith initial condition m \u22121 = 0. Consequently, the number of characters embedded in the first N T tokens for a random input\nbecomes N c = [\ufffd] i [N] =1 [T] [B] [i] [.]\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {S k } [\u221e] k=1 [,]\n{B i } [\u221e] i=1 [, and][ {][X] [i] [}] [\u221e] i=1 [are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that]\n\n\n\n\u2212 [1]\n\n\n\n\n[1]\n\nn [log] [2] [ p] [S] [n] [(][S] [1] [, . . ., S] [n] [) =][ \u2212] n [1]\n\n\n\nn [log] [2] [ p] [S] [n] [(][S] [n] [)][ \u2192] [H][(][S][)] almost surely. (2)\n\n\n\nLet \u2126 S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\n\n\n\n[1]\n\ni [log] [2] [ p] [X] [i] [(][X] [1] [, . . ., X] [i] [) =][ \u2212] [1] i\n\n\n\n\u2212 [1]\n\n\n\ni [log] [2] [ p] [X] [i] [(][X] [i] [)][ \u2192] [H][(][X][)] almost surely. (3)\n\n\n\nDefine \u2126 X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\n\n\nm i (S)\nlim = E [B] almost surely. (4)\ni\u2192\u221e i\n\n\nSet \u2126 B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126 S \u2229 \u2126 X \u2229 \u2126 B, we deduce that\n\n\nH(S) = lim\nk\u2192\u221e [\u2212] k [1] [log] [2] [ p] [S] [k] [(][S] [k] [(][\u03c9][))]\n\n= lim i\u2192\u221e [\u2212] l [1] i log 2 p S li (S l i (\u03c9))\n\n\n= lim log 2 Pr (X i = T (S l i (\u03c9)))\ni\u2192\u221e [\u2212] l [1] i\n\n\n\n1\n\ni [log] [2] [ Pr (][X] [i] [ =][ x] [i] [) =][ H] E [(] B [X][)]\n\n\n\n1 1\n\n= \u2212\nE[B] i [lim] \u2192\u221e i\n\n\n\nE[B] [.]\n\n\n\nThe first equality follows from (2). The second equality is a consequence of the fact that {l i = m i (S(\u03c9))|i \u2208 N} is an infinite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\n\ntwo events,\n{\u03c9 \u2208 \u2126|X i (\u03c9) = x i } = {\u03c9 \u2208 \u2126|T (S m i (S(\u03c9)) ) = x i }.\n\n\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126 B . The sets \u2126 S, \u2126 X, and \u2126 B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\n\nH(S) = [H][(][X][)] almost surely.\n\nE[B]\n\n\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\n\n\n\nPr\n\n\ufffd\n\n\n\n\ufffd\n\n\n\nH(X) \u2264 lim\nN T \u2192\u221e [\u2212] N [1] T\n\n\n\nN T\n\ufffd\n\n\n\n\ufffd log 2 q i (X i )\n\n\ni=1\n\n\n\n= 1, (5)\n\n\n\nwhere q i (\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\n\n\n\n\n[1] N T\n\nN T \ufffd i=1 [log] 2 [q] [i] [(][X] [i] [)]\n\n\n\nH(S) \u2264 lim N T \u2192\u221e \u2212 N [1]\n\n\n\n. (6)\nE[B]\n\n\n\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by H ub . The\nnumerator in (6) represents the average number of bits required to represent the tokens X N T and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\n\n\nB. Encoding schemes\n\n\nWe consider three schemes for the lossless compression block in Fig. 3.\n\n\n5\n\n\n1) Compressing the ranks using zlib: The first scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1 LLaMA+zlib .\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token x i at epoch i by using a prefix-free code assuming q i to be the true distribution of\nthe tokens. A natural choice for a prefix-free code is a Huffman code. Instead, for simplicity, we use a prefix-free code where\n1\nthe codeword for the token x i is of length l i = \u2308log 2 q i (x i ) [\u2309][. A prefix-free code with this length for][ x] [i] [ is guaranteed to exist]\nsince this choice of lengths satisfies the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1 LLaMA+TbyT,\nis given by\n\n\n\nN T\n\ufffd\n\n\ni=1\n\n\n\n1\nlog 2\n\ufffd q i (x i )\n\n\n\n\ufffd\n\n\n\n\u03c1 LLaMA+TbyT =\n\n\n\n\ufffd Ni=1 T [b] [i] .\n\n\n\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use q i (x i ) as the probability of token x i at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1 LLM+AC . It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\n\n\n\u03c1 LLM+AC \u2248\n\n\n\nN T 1\n\ufffd i=1 log 2 q i (x i )\n\n\ufffd Ni=1 T [b] [i] . (7)\n\n\n\nClearly, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and \u03c1 LLM+AC also provide upper bounds on H(S). H ub, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and\n\u03c1 LLM+AC are estimated using a finite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\n\n\nIV. R ESULTS\n\n\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it difficult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\n[Nevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of](http://mattmahoney.net/dc/text8.zip)\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M ) is shown in Table II. As expected, the\ncompression performance improves with increasing M . We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n\n\n6\n\n\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\n\n\nV. A CKNOWLEDGEMENT\n\n\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\n\n\nTABLE I\n\nR ESULTS FOR 1MB OF TEXT FROM TEXT 8 DATASET\n\n\n\nBatch\nN c N T\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC ZPAQ pq8h\n\n\n\nNo. (bpc) file size (bits) (bpc) (bpc) (bpc) (bpc)\n\n\n1 466, 650 100, 000 0.6882 1.0513 0.8215 0.689\n\n\n2 461, 477 100, 000 0.6893 1.0558 0.8242 0.6901\n\n\n3 454, 599 100, 000 0.699 1.0681 0.8357 0.6999\n\n\n4 462, 755 100, 000 0.6748 1.0346 0.8093 0.6757\n\n\n5 453, 847 100, 000 0.7481 1.1265 0.8831 0.749\n\n\n6 458, 252 100, 000 0.7218 1.0957 0.8567 0.7227\n\n\n7 451, 036 100, 000 0.6959 1.0729 0.8353 0.6968\n\n\n8 447, 953 100, 000 0.7092 1.0896 0.8489 0.7101\n\n\n9 462, 665 100, 000 0.7394 1.1126 0.8713 0.7402\n\n\n10 449, 621 100, 000 0.7269 1.1046 0.8643 0.7277\n\n\nTotal 9, 137, 710 2, 000, 000 0.7093 1.0812 0.845 0.7101 1.4 [1] 1.2 [2]\n\n\nTABLE II\n\nC OMPRESSION PERFORMANCE OF THE LLM ON THE TEXT 8 DATASET, AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\n\n\n(bpc) file size (bits) (bpc) (bpc)\n\n\n\n31 4, 568, 855 1, 000, 000 0.9139 1.3159 1.0425 0.9145\n\n\n127 4, 568, 855 1, 000, 000 0.7511 1.1303 0.8847 0.752\n\n\n255 4, 568, 855 1, 000, 000 0.7242 1.0985 0.859 0.725\n\n\n511 4, 568, 855 1, 000, 000 0.7093 1.0812 0.845 0.7101\n\n\n1 This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2 This result is taken from [12] and it corresponds to the full 100MB dataset text8\n\n\n7\n\n\n\nTABLE III\n\nM EAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\nM\n\n(bpc) (bpc) (bpc) (bpc)\n\n\n31 0.9139 \u00b1 0.0263 1.3159 \u00b1 0.0329 1.0425 \u00b1 0.0262 0.9145 \u00b1 0.0263\n\n\n127 0.7511 \u00b1 0.0233 1.1303 \u00b1 0.0292 0.8847 \u00b1 0.0231 0.752 \u00b1 0.0233\n\n\n255 0.7242 \u00b1 0.0234 1.0985 \u00b1 0.0289 0.859 \u00b1 0.0232 0.725 \u00b1 0.0234\n\n\n511 0.7093 \u00b1 0.0228 1.0812 \u00b1 0.028 0.845 \u00b1 0.0226 0.7101 \u00b1 0.0228\n\n\nTABLE IV\n\nC OMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN P ROJECT G UTENBERG [7], AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC Standalone Zlib\n\n\n(bpc) (bpc) (bpc) (bpc) (bpc)\n\n\n\n31 508, 463 115, 000 1.0919 1.5316 1.2152 1.0924 2.80\n\n\n127 508, 463 115, 000 0.8973 1.3128 1.0235 0.8982 2.80\n\n\n255 508, 463 115, 000 0.8618 1.2684 0.9899 0.8627 2.80\n\n\n511 508, 463 115, 000 0.8417 1.2465 0.9711 0.8426 2.80\n\n\n8\n\n\nR EFERENCES\n\n\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR filters),\u201d 2023.\n\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and efficient foundation language models,\u201d\n2023.\n\n[7] J. Frank Dobie, Legends of Texas, United States, Texas Folk-Lore Society, 1924; Project Gutenberg, May 25, 2023, 2023,\n[https://www.gutenberg.org/ebooks/70859.](https://www.gutenberg.org/ebooks/70859)\n\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n\n\n"}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Final Answer:[00m [92m

Thought: The paper "LLMZip: Lossless Text Compression using Large Language Models" (arXiv:2306.04050) has been successfully downloaded. I now have access to its full content. I will proceed to extract the relevant information to create a concise synopsis of its likely future content, focusing on facts, equations, definitions, datasets, and references, as requested.

Based on the document prefix and the full content, the likely future content will include:
- **Methodology (Section III. Compression Using LLMs):** Detailed explanation of how LLMs are used for compression, including the concept of tokenization, probability mass functions (PMF), rank computation, and different encoding schemes (zlib, Token-by-Token, Arithmetic Coding).
- **Equations:** Formal definitions and equations related to entropy bounds (H(S), H(X)), compression ratio (œÅ), and the calculation of codeword lengths for prefix-free codes. Specifically, equations (1), (2), (3), (4), (5), (6), and (7) are present.
- **Definitions:** Formal definitions of terms like 'tokens', 'language model with memory M', 'rank of the input sequence', 'compression ratio œÅ', 'asymptotic upper bound on H(S) (H_ub)'.
- **Experimental Setup and Results (Section IV. Results):**
    - **LLM used:** LLaMA-7B.
    - **Tokenizer:** SentencePiece tokenizer (dictionary size 32000).
    - **Datasets:** text8 dataset (1MB section, 100,000 tokens per batch, total 1M tokens for average performance) and a recently released book from Project Gutenberg [7] (100KB section, 100,000 tokens).
    - **Baselines:** BSC, ZPAQ (1.4 bpc on text8), paq8h (1.2 bpc on text8), and standalone zlib (2.8 bpc on Project Gutenberg text).
    - **Key Results:**
        - Asymptotic upper bound on entropy (H_ub): 0.709 bits/character for text8 (1MB), 0.8417 bits/character for Project Gutenberg text (100KB).
        - Compression ratios (œÅ_LLM+AC): 0.7101 bits/character for text8 (1MB), 0.8426 bits/character for Project Gutenberg text (100KB).
        - Performance comparison with baselines, showing LLMZip outperforms them.
        - Dependence of compression performance on LLM memory (M).
        - Discussion on potential optimism of text8 results due to LLaMA-7B training corpus.
- **Acknowledgements (Section V. Acknowledgement):** A section acknowledging contributions or discussions.
- **References:** A comprehensive list of cited works [1] through [12].

I will now synthesize this information into the final answer.# LLMZip: Lossless Text Compression using Large Language Models - Synopsis of Future Content

The document will elaborate on the methodology, present detailed experimental results, and provide a comprehensive list of references.

## III. Compression Using LLMs

This section formally defines the compression scheme.
*   **Tokenization:** A sentence `s` of `N_c` characters is parsed into `N_T` tokens `x = x_1, x_2, ..., x_{N_T}` using a dictionary `X = [1, D]` of `D` tokens. There is a one-to-one mapping between `s` and `x`.
*   **Language Model Prediction:** A language model with memory `M` predicts the next token `x_i` based on the past `M` tokens `x_{i-M}, ..., x_{i-1}`, producing a probability mass function (PMF) `q_i(x_i) := Pr(X_i = x_i | x_{i-1}, ..., x_{i-M})`.
*   **Rank Computation:** The PMF vector `q_i` is sorted to `~q_i`, and a permutation `Œ≥_i: X -> X` defines the rank `r_i := Œ≥_i(x_i)` of the actual token `x_i` at epoch `i`.
*   **Compression Ratio Definition:** The compression ratio `œÅ` is defined as `œÅ := N_b / N_c` bits/character, where `N_b` is the number of bits after compression and `N_c` is the number of characters.

### A. Entropy Bounds

This subsection establishes the theoretical connection between the entropy of the character sequence `H(S)` and the entropy of the token sequence `H(X)`.
*   **Tokenizer Function:** `m_i(s) = min n {length(T(s_n)) >= i}` (Equation 1) defines the character index where the `i`-th token is output.
*   **Characters per Token:** `b_i = m_i(s) - m_{i-1}(s)` is the number of characters in the `i`-th token. The total number of characters is `N_c = Œ£_{i=1}^{N_T} B_i`.
*   **Shannon-McMillan-Breiman Theorem:** Applied to character sequences (Equation 2) and token sequences (Equation 3).
*   **Average Characters per Token:** `lim_{i->‚àû} m_i(S) / i = E[B]` almost surely (Equation 4).
*   **Relationship between H(S) and H(X):** `H(S) = H(X) / E[B]` almost surely.
*   **Asymptotic Upper Bound on H(S):**
    `H(S) <= lim_{N_T->‚àû} (-1/N_T) Œ£_{i=1}^{N_T} log_2 q_i(X_i) / E[B]` (Equation 6).
    This is referred to as `H_ub` (asymptotic upper bound on H(S)).
    Previous estimates: Cover and King [1] provide 1.3 bits/character. Recent estimates in [2, Table 4] use 'bits per character (bpc)' which is considered equivalent to `H_ub`.

### B. Encoding Schemes

Three schemes for the lossless compression block are discussed:
1.  **Compressing the ranks using zlib (LLaMA+zlib):** The sequence of ranks `r_i` is compressed using the zlib algorithm.
2.  **Token-by-Token Compression (LLaMA+TbyT):** Each token `x_i` is encoded using a prefix-free code with codeword length `l_i = ceil(log_2(1/q_i(x_i)))`. The compression ratio is `œÅ_LLaMA+TbyT = Œ£_{i=1}^{N_T} l_i / Œ£_{i=1}^{N_T} b_i`.
3.  **Arithmetic Coding (LLM+AC):** This scheme uses arithmetic coding with the time-varying probabilities `q_i(x_i)`. The compression ratio is expected to be `œÅ_LLM+AC ‚âà Œ£_{i=1}^{N_T} log_2(1/q_i(x_i)) / Œ£_{i=1}^{N_T} b_i` (Equation 7). This is considered nearly optimal.

## IV. Results

This section presents the experimental setup and findings.
*   **LLM and Tokenizer:** LLaMA-7B [6] and SentencePiece tokenizer [11] (dictionary size 32000).
*   **Datasets:**
    *   `text8` dataset: 1MB section, processed into 10 batches of 100,000 tokens each.
    *   Project Gutenberg book [7]: A recently released book (May 25, 2023), 100KB section, 100,000 tokens.
*   **Baselines:** ZPAQ (1.4 bpc on text8 [5]), paq8h (1.2 bpc on text8 [12]), and standalone zlib (2.8 bpc on Project Gutenberg text).
*   **Key Findings:**
    *   **text8 dataset (1MB):**
        *   Average `H_ub`: 0.7093 bits/character.
        *   Average `œÅ_LLM+AC`: 0.7101 bits/character.
        *   Average `œÅ_LLaMA+zlib`: 1.0812 bits/character.
        *   Average `œÅ_LLaMA+TbyT`: 0.845 bits/character.
        *   These results are substantially better than ZPAQ (1.4 bpc) and paq8h (1.2 bpc).
    *   **Project Gutenberg dataset (100KB):**
        *   `H_ub`: 0.8417 bits/character.
        *   `œÅ_LLM+AC`: 0.8426 bits/character.
        *   `œÅ_LLaMA+zlib`: 1.2465 bits/character.
        *   `œÅ_LLaMA+TbyT`: 0.9711 bits/character.
        *   These results are also better than state-of-the-art for text8, and significantly better than standalone zlib (2.8 bpc).
    *   **Memory Dependence:** Compression performance improves with increasing LLM memory `M` (e.g., from `M=31` to `M=511`). Inference time scales approximately linearly with memory length.
    *   **Variance:** Empirical mean and standard deviation of entropy bounds and compression ratios are provided over 10 batches.
    *   **Limitations:** LLaMA-7B could not be run on the entire 100MB text8 dataset. Results for text8 might be optimistic due to LLaMA-7B's training corpus including Wikipedia.

## V. Acknowledgement

This section will acknowledge contributions, specifically thanking Andreas Kirsch for discussions on arithmetic coding.

## References

A list of 12 references will be provided, including:
*   [1] Thomas Cover and Roger King, "A convergent gambling estimate of the entropy of english," IEEE Transactions on Information Theory, 1978.
*   [2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, "Focus your attention (with adaptive IIR filters)," 2023.
*   [3] Claude E Shannon, "Prediction and entropy of printed english," Bell system technical journal, 1951.
*   [4] John Cleary and Ian Witten, "Data compression using adaptive coding and partial string matching," IEEE transactions on Communications, 1984.
*   [5] Mohit Goyal et al., "Deepzip: Lossless data compression using recurrent neural networks," arXiv preprint arXiv:1811.08162, 2018.
*   [6] Hugo Touvron et al., "Llama: Open and efficient foundation language models," 2023.
*   [7] J. Frank Dobie, Legends of Texas, Project Gutenberg, 2023.
*   [8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, 1999.
*   [9] Timothy Bell, Ian H Witten, and John G Cleary, "Modeling for text compression," ACM Computing Surveys (CSUR), 1989.
*   [10] David JC MacKay, Information theory, inference and learning algorithms, 2003.
*   [11] Taku Kudo and John Richardson, "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing," CoRR, 2018.
*   [12] "text8 results," http://mattmahoney.net/dc/textdata.html.

The document will include tables summarizing the results for the text8 dataset (Table I, II, III) and the Project Gutenberg dataset (Table IV), detailing `H_ub`, `œÅ_LLaMA+zlib`, `œÅ_LLaMA+TbyT`, `œÅ_LLM+AC`, and comparisons with baselines.[00m


üöÄ Crew: crew
‚îú‚îÄ‚îÄ üìã Task: c8ca34b1-9034-4e11-8900-1d06ebc68bed
‚îÇ   Assigned to: AI Assistant
‚îÇ   Status: ‚úÖ Completed
‚îÇ   ‚îú‚îÄ‚îÄ ‚úÖ Reasoning Completed
‚îÇ   ‚îî‚îÄ‚îÄ üîß Used download_paper (1)
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îú‚îÄ‚îÄ üîÑ Tool Usage Started
‚îú‚îÄ‚îÄ üîß Using create_reasoning_plan from LLM available_function
‚îî‚îÄ‚îÄ ‚úÖ Tool Usage Completed‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Task Completion ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ  Task Completed                                                             ‚îÇ
‚îÇ  Name: c8ca34b1-9034-4e11-8900-1d06ebc68bed                                 ‚îÇ
‚îÇ  Agent: AI Assistant                                                        ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Crew Completion ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ  Crew Execution Completed                                                   ‚îÇ
‚îÇ  Name: crew                                                                 ‚îÇ
‚îÇ  ID: 00aaf669-5a79-40dc-9242-76268973947e                                   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

AGENT_RESULT_JSON:{"final_text": "\nThought: The paper \"LLMZip: Lossless Text Compression using Large Language Models\" (arXiv:2306.04050) has been successfully downloaded. I now have access to its full content. I will proceed to extract the relevant information to create a concise synopsis of its likely future content, focusing on facts, equations, definitions, datasets, and references, as requested.\n\nBased on the document prefix and the full content, the likely future content will include:\n- **Methodology (Section III. Compression Using LLMs):** Detailed explanation of how LLMs are used for compression, including the concept of tokenization, probability mass functions (PMF), rank computation, and different encoding schemes (zlib, Token-by-Token, Arithmetic Coding).\n- **Equations:** Formal definitions and equations related to entropy bounds (H(S), H(X)), compression ratio (\u03c1), and the calculation of codeword lengths for prefix-free codes. Specifically, equations (1), (2), (3), (4), (5), (6), and (7) are present.\n- **Definitions:** Formal definitions of terms like 'tokens', 'language model with memory M', 'rank of the input sequence', 'compression ratio \u03c1', 'asymptotic upper bound on H(S) (H_ub)'.\n- **Experimental Setup and Results (Section IV. Results):**\n    - **LLM used:** LLaMA-7B.\n    - **Tokenizer:** SentencePiece tokenizer (dictionary size 32000).\n    - **Datasets:** text8 dataset (1MB section, 100,000 tokens per batch, total 1M tokens for average performance) and a recently released book from Project Gutenberg [7] (100KB section, 100,000 tokens).\n    - **Baselines:** BSC, ZPAQ (1.4 bpc on text8), paq8h (1.2 bpc on text8), and standalone zlib (2.8 bpc on Project Gutenberg text).\n    - **Key Results:**\n        - Asymptotic upper bound on entropy (H_ub): 0.709 bits/character for text8 (1MB), 0.8417 bits/character for Project Gutenberg text (100KB).\n        - Compression ratios (\u03c1_LLM+AC): 0.7101 bits/character for text8 (1MB), 0.8426 bits/character for Project Gutenberg text (100KB).\n        - Performance comparison with baselines, showing LLMZip outperforms them.\n        - Dependence of compression performance on LLM memory (M).\n        - Discussion on potential optimism of text8 results due to LLaMA-7B training corpus.\n- **Acknowledgements (Section V. Acknowledgement):** A section acknowledging contributions or discussions.\n- **References:** A comprehensive list of cited works [1] through [12].\n\nI will now synthesize this information into the final answer.# LLMZip: Lossless Text Compression using Large Language Models - Synopsis of Future Content\n\nThe document will elaborate on the methodology, present detailed experimental results, and provide a comprehensive list of references.\n\n## III. Compression Using LLMs\n\nThis section formally defines the compression scheme.\n*   **Tokenization:** A sentence `s` of `N_c` characters is parsed into `N_T` tokens `x = x_1, x_2, ..., x_{N_T}` using a dictionary `X = [1, D]` of `D` tokens. There is a one-to-one mapping between `s` and `x`.\n*   **Language Model Prediction:** A language model with memory `M` predicts the next token `x_i` based on the past `M` tokens `x_{i-M}, ..., x_{i-1}`, producing a probability mass function (PMF) `q_i(x_i) := Pr(X_i = x_i | x_{i-1}, ..., x_{i-M})`.\n*   **Rank Computation:** The PMF vector `q_i` is sorted to `~q_i`, and a permutation `\u03b3_i: X -> X` defines the rank `r_i := \u03b3_i(x_i)` of the actual token `x_i` at epoch `i`.\n*   **Compression Ratio Definition:** The compression ratio `\u03c1` is defined as `\u03c1 := N_b / N_c` bits/character, where `N_b` is the number of bits after compression and `N_c` is the number of characters.\n\n### A. Entropy Bounds\n\nThis subsection establishes the theoretical connection between the entropy of the character sequence `H(S)` and the entropy of the token sequence `H(X)`.\n*   **Tokenizer Function:** `m_i(s) = min n {length(T(s_n)) >= i}` (Equation 1) defines the character index where the `i`-th token is output.\n*   **Characters per Token:** `b_i = m_i(s) - m_{i-1}(s)` is the number of characters in the `i`-th token. The total number of characters is `N_c = \u03a3_{i=1}^{N_T} B_i`.\n*   **Shannon-McMillan-Breiman Theorem:** Applied to character sequences (Equation 2) and token sequences (Equation 3).\n*   **Average Characters per Token:** `lim_{i->\u221e} m_i(S) / i = E[B]` almost surely (Equation 4).\n*   **Relationship between H(S) and H(X):** `H(S) = H(X) / E[B]` almost surely.\n*   **Asymptotic Upper Bound on H(S):**\n    `H(S) <= lim_{N_T->\u221e} (-1/N_T) \u03a3_{i=1}^{N_T} log_2 q_i(X_i) / E[B]` (Equation 6).\n    This is referred to as `H_ub` (asymptotic upper bound on H(S)).\n    Previous estimates: Cover and King [1] provide 1.3 bits/character. Recent estimates in [2, Table 4] use 'bits per character (bpc)' which is considered equivalent to `H_ub`.\n\n### B. Encoding Schemes\n\nThree schemes for the lossless compression block are discussed:\n1.  **Compressing the ranks using zlib (LLaMA+zlib):** The sequence of ranks `r_i` is compressed using the zlib algorithm.\n2.  **Token-by-Token Compression (LLaMA+TbyT):** Each token `x_i` is encoded using a prefix-free code with codeword length `l_i = ceil(log_2(1/q_i(x_i)))`. The compression ratio is `\u03c1_LLaMA+TbyT = \u03a3_{i=1}^{N_T} l_i / \u03a3_{i=1}^{N_T} b_i`.\n3.  **Arithmetic Coding (LLM+AC):** This scheme uses arithmetic coding with the time-varying probabilities `q_i(x_i)`. The compression ratio is expected to be `\u03c1_LLM+AC \u2248 \u03a3_{i=1}^{N_T} log_2(1/q_i(x_i)) / \u03a3_{i=1}^{N_T} b_i` (Equation 7). This is considered nearly optimal.\n\n## IV. Results\n\nThis section presents the experimental setup and findings.\n*   **LLM and Tokenizer:** LLaMA-7B [6] and SentencePiece tokenizer [11] (dictionary size 32000).\n*   **Datasets:**\n    *   `text8` dataset: 1MB section, processed into 10 batches of 100,000 tokens each.\n    *   Project Gutenberg book [7]: A recently released book (May 25, 2023), 100KB section, 100,000 tokens.\n*   **Baselines:** ZPAQ (1.4 bpc on text8 [5]), paq8h (1.2 bpc on text8 [12]), and standalone zlib (2.8 bpc on Project Gutenberg text).\n*   **Key Findings:**\n    *   **text8 dataset (1MB):**\n        *   Average `H_ub`: 0.7093 bits/character.\n        *   Average `\u03c1_LLM+AC`: 0.7101 bits/character.\n        *   Average `\u03c1_LLaMA+zlib`: 1.0812 bits/character.\n        *   Average `\u03c1_LLaMA+TbyT`: 0.845 bits/character.\n        *   These results are substantially better than ZPAQ (1.4 bpc) and paq8h (1.2 bpc).\n    *   **Project Gutenberg dataset (100KB):**\n        *   `H_ub`: 0.8417 bits/character.\n        *   `\u03c1_LLM+AC`: 0.8426 bits/character.\n        *   `\u03c1_LLaMA+zlib`: 1.2465 bits/character.\n        *   `\u03c1_LLaMA+TbyT`: 0.9711 bits/character.\n        *   These results are also better than state-of-the-art for text8, and significantly better than standalone zlib (2.8 bpc).\n    *   **Memory Dependence:** Compression performance improves with increasing LLM memory `M` (e.g., from `M=31` to `M=511`). Inference time scales approximately linearly with memory length.\n    *   **Variance:** Empirical mean and standard deviation of entropy bounds and compression ratios are provided over 10 batches.\n    *   **Limitations:** LLaMA-7B could not be run on the entire 100MB text8 dataset. Results for text8 might be optimistic due to LLaMA-7B's training corpus including Wikipedia.\n\n## V. Acknowledgement\n\nThis section will acknowledge contributions, specifically thanking Andreas Kirsch for discussions on arithmetic coding.\n\n## References\n\nA list of 12 references will be provided, including:\n*   [1] Thomas Cover and Roger King, \"A convergent gambling estimate of the entropy of english,\" IEEE Transactions on Information Theory, 1978.\n*   [2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \"Focus your attention (with adaptive IIR filters),\" 2023.\n*   [3] Claude E Shannon, \"Prediction and entropy of printed english,\" Bell system technical journal, 1951.\n*   [4] John Cleary and Ian Witten, \"Data compression using adaptive coding and partial string matching,\" IEEE transactions on Communications, 1984.\n*   [5] Mohit Goyal et al., \"Deepzip: Lossless data compression using recurrent neural networks,\" arXiv preprint arXiv:1811.08162, 2018.\n*   [6] Hugo Touvron et al., \"Llama: Open and efficient foundation language models,\" 2023.\n*   [7] J. Frank Dobie, Legends of Texas, Project Gutenberg, 2023.\n*   [8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, 1999.\n*   [9] Timothy Bell, Ian H Witten, and John G Cleary, \"Modeling for text compression,\" ACM Computing Surveys (CSUR), 1989.\n*   [10] David JC MacKay, Information theory, inference and learning algorithms, 2003.\n*   [11] Taku Kudo and John Richardson, \"Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\" CoRR, 2018.\n*   [12] \"text8 results,\" http://mattmahoney.net/dc/textdata.html.\n\nThe document will include tables summarizing the results for the text8 dataset (Table I, II, III) and the Project Gutenberg dataset (Table IV), detailing `H_ub`, `\u03c1_LLaMA+zlib`, `\u03c1_LLaMA+TbyT`, `\u03c1_LLM+AC`, and comparisons with baselines.", "duration_ms": 29495}

Thought: The paper "LLMZip: Lossless Text Compression using Large Language Models" (arXiv:2306.04050) has been successfully downloaded. I now have access to its full content. I will proceed to extract the relevant information to create a concise synopsis of its likely future content, focusing on facts, equations, definitions, datasets, and references, as requested.

Based on the document prefix and the full content, the likely future content will include:
- **Methodology (Section III. Compression Using LLMs):** Detailed explanation of how LLMs are used for compression, including the concept of tokenization, probability mass functions (PMF), rank computation, and different encoding schemes (zlib, Token-by-Token, Arithmetic Coding).
- **Equations:** Formal definitions and equations related to entropy bounds (H(S), H(X)), compression ratio (œÅ), and the calculation of codeword lengths for prefix-free codes. Specifically, equations (1), (2), (3), (4), (5), (6), and (7) are present.
- **Definitions:** Formal definitions of terms like 'tokens', 'language model with memory M', 'rank of the input sequence', 'compression ratio œÅ', 'asymptotic upper bound on H(S) (H_ub)'.
- **Experimental Setup and Results (Section IV. Results):**
    - **LLM used:** LLaMA-7B.
    - **Tokenizer:** SentencePiece tokenizer (dictionary size 32000).
    - **Datasets:** text8 dataset (1MB section, 100,000 tokens per batch, total 1M tokens for average performance) and a recently released book from Project Gutenberg [7] (100KB section, 100,000 tokens).
    - **Baselines:** BSC, ZPAQ (1.4 bpc on text8), paq8h (1.2 bpc on text8), and standalone zlib (2.8 bpc on Project Gutenberg text).
    - **Key Results:**
        - Asymptotic upper bound on entropy (H_ub): 0.709 bits/character for text8 (1MB), 0.8417 bits/character for Project Gutenberg text (100KB).
        - Compression ratios (œÅ_LLM+AC): 0.7101 bits/character for text8 (1MB), 0.8426 bits/character for Project Gutenberg text (100KB).
        - Performance comparison with baselines, showing LLMZip outperforms them.
        - Dependence of compression performance on LLM memory (M).
        - Discussion on potential optimism of text8 results due to LLaMA-7B training corpus.
- **Acknowledgements (Section V. Acknowledgement):** A section acknowledging contributions or discussions.
- **References:** A comprehensive list of cited works [1] through [12].

I will now synthesize this information into the final answer.# LLMZip: Lossless Text Compression using Large Language Models - Synopsis of Future Content

The document will elaborate on the methodology, present detailed experimental results, and provide a comprehensive list of references.

## III. Compression Using LLMs

This section formally defines the compression scheme.
*   **Tokenization:** A sentence `s` of `N_c` characters is parsed into `N_T` tokens `x = x_1, x_2, ..., x_{N_T}` using a dictionary `X = [1, D]` of `D` tokens. There is a one-to-one mapping between `s` and `x`.
*   **Language Model Prediction:** A language model with memory `M` predicts the next token `x_i` based on the past `M` tokens `x_{i-M}, ..., x_{i-1}`, producing a probability mass function (PMF) `q_i(x_i) := Pr(X_i = x_i | x_{i-1}, ..., x_{i-M})`.
*   **Rank Computation:** The PMF vector `q_i` is sorted to `~q_i`, and a permutation `Œ≥_i: X -> X` defines the rank `r_i := Œ≥_i(x_i)` of the actual token `x_i` at epoch `i`.
*   **Compression Ratio Definition:** The compression ratio `œÅ` is defined as `œÅ := N_b / N_c` bits/character, where `N_b` is the number of bits after compression and `N_c` is the number of characters.

### A. Entropy Bounds

This subsection establishes the theoretical connection between the entropy of the character sequence `H(S)` and the entropy of the token sequence `H(X)`.
*   **Tokenizer Function:** `m_i(s) = min n {length(T(s_n)) >= i}` (Equation 1) defines the character index where the `i`-th token is output.
*   **Characters per Token:** `b_i = m_i(s) - m_{i-1}(s)` is the number of characters in the `i`-th token. The total number of characters is `N_c = Œ£_{i=1}^{N_T} B_i`.
*   **Shannon-McMillan-Breiman Theorem:** Applied to character sequences (Equation 2) and token sequences (Equation 3).
*   **Average Characters per Token:** `lim_{i->‚àû} m_i(S) / i = E[B]` almost surely (Equation 4).
*   **Relationship between H(S) and H(X):** `H(S) = H(X) / E[B]` almost surely.
*   **Asymptotic Upper Bound on H(S):**
    `H(S) <= lim_{N_T->‚àû} (-1/N_T) Œ£_{i=1}^{N_T} log_2 q_i(X_i) / E[B]` (Equation 6).
    This is referred to as `H_ub` (asymptotic upper bound on H(S)).
    Previous estimates: Cover and King [1] provide 1.3 bits/character. Recent estimates in [2, Table 4] use 'bits per character (bpc)' which is considered equivalent to `H_ub`.

### B. Encoding Schemes

Three schemes for the lossless compression block are discussed:
1.  **Compressing the ranks using zlib (LLaMA+zlib):** The sequence of ranks `r_i` is compressed using the zlib algorithm.
2.  **Token-by-Token Compression (LLaMA+TbyT):** Each token `x_i` is encoded using a prefix-free code with codeword length `l_i = ceil(log_2(1/q_i(x_i)))`. The compression ratio is `œÅ_LLaMA+TbyT = Œ£_{i=1}^{N_T} l_i / Œ£_{i=1}^{N_T} b_i`.
3.  **Arithmetic Coding (LLM+AC):** This scheme uses arithmetic coding with the time-varying probabilities `q_i(x_i)`. The compression ratio is expected to be `œÅ_LLM+AC ‚âà Œ£_{i=1}^{N_T} log_2(1/q_i(x_i)) / Œ£_{i=1}^{N_T} b_i` (Equation 7). This is considered nearly optimal.

## IV. Results

This section presents the experimental setup and findings.
*   **LLM and Tokenizer:** LLaMA-7B [6] and SentencePiece tokenizer [11] (dictionary size 32000).
*   **Datasets:**
    *   `text8` dataset: 1MB section, processed into 10 batches of 100,000 tokens each.
    *   Project Gutenberg book [7]: A recently released book (May 25, 2023), 100KB section, 100,000 tokens.
*   **Baselines:** ZPAQ (1.4 bpc on text8 [5]), paq8h (1.2 bpc on text8 [12]), and standalone zlib (2.8 bpc on Project Gutenberg text).
*   **Key Findings:**
    *   **text8 dataset (1MB):**
        *   Average `H_ub`: 0.7093 bits/character.
        *   Average `œÅ_LLM+AC`: 0.7101 bits/character.
        *   Average `œÅ_LLaMA+zlib`: 1.0812 bits/character.
        *   Average `œÅ_LLaMA+TbyT`: 0.845 bits/character.
        *   These results are substantially better than ZPAQ (1.4 bpc) and paq8h (1.2 bpc).
    *   **Project Gutenberg dataset (100KB):**
        *   `H_ub`: 0.8417 bits/character.
        *   `œÅ_LLM+AC`: 0.8426 bits/character.
        *   `œÅ_LLaMA+zlib`: 1.2465 bits/character.
        *   `œÅ_LLaMA+TbyT`: 0.9711 bits/character.
        *   These results are also better than state-of-the-art for text8, and significantly better than standalone zlib (2.8 bpc).
    *   **Memory Dependence:** Compression performance improves with increasing LLM memory `M` (e.g., from `M=31` to `M=511`). Inference time scales approximately linearly with memory length.
    *   **Variance:** Empirical mean and standard deviation of entropy bounds and compression ratios are provided over 10 batches.
    *   **Limitations:** LLaMA-7B could not be run on the entire 100MB text8 dataset. Results for text8 might be optimistic due to LLaMA-7B's training corpus including Wikipedia.

## V. Acknowledgement

This section will acknowledge contributions, specifically thanking Andreas Kirsch for discussions on arithmetic coding.

## References

A list of 12 references will be provided, including:
*   [1] Thomas Cover and Roger King, "A convergent gambling estimate of the entropy of english," IEEE Transactions on Information Theory, 1978.
*   [2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, "Focus your attention (with adaptive IIR filters)," 2023.
*   [3] Claude E Shannon, "Prediction and entropy of printed english," Bell system technical journal, 1951.
*   [4] John Cleary and Ian Witten, "Data compression using adaptive coding and partial string matching," IEEE transactions on Communications, 1984.
*   [5] Mohit Goyal et al., "Deepzip: Lossless data compression using recurrent neural networks," arXiv preprint arXiv:1811.08162, 2018.
*   [6] Hugo Touvron et al., "Llama: Open and efficient foundation language models," 2023.
*   [7] J. Frank Dobie, Legends of Texas, Project Gutenberg, 2023.
*   [8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, 1999.
*   [9] Timothy Bell, Ian H Witten, and John G Cleary, "Modeling for text compression," ACM Computing Surveys (CSUR), 1989.
*   [10] David JC MacKay, Information theory, inference and learning algorithms, 2003.
*   [11] Taku Kudo and John Richardson, "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing," CoRR, 2018.
*   [12] "text8 results," http://mattmahoney.net/dc/textdata.html.

The document will include tables summarizing the results for the text8 dataset (Table I, II, III) and the Project Gutenberg dataset (Table IV), detailing `H_ub`, `œÅ_LLaMA+zlib`, `œÅ_LLaMA+TbyT`, `œÅ_LLM+AC`, and comparisons with baselines.
