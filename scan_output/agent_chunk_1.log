┌────────────────────────── Crew Execution Started ───────────────────────────┐
C:\tools\Anaconda3\Lib\site-packages\paramiko\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
│                                                                             │
  "cipher": algorithms.TripleDES,
│  Crew Execution Started                                                     │
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.
│  Name: crew                                                                 │
  "class": algorithms.Blowfish,
│  ID: 1305f696-9e44-40a4-8b28-e496f68ab681                                   │
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
│                                                                             │
  "class": algorithms.TripleDES,
│                                                                             │
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\_internal\_generate_schema.py:628: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.
└─────────────────────────────────────────────────────────────────────────────┘
  warn(

- Reading configuration...

  - Reading registry from registry.yaml

  - Reading catalog from docker-mcp.yaml

  - Reading config from config.yaml

- Configuration read in 17.5336ms
🚀 Crew: crew
- Watching registry at C:\Users\Noah\.docker\mcp\registry.yaml
└── 📋 Task: 3d4c6b53-e20d-4ff9-9dd1-edfa0361cca1
- Watching config at C:\Users\Noah\.docker\mcp\config.yaml
    Status: Executing Task...
- Those servers are enabled: arxiv-mcp-server, duckduckgo, wikipedia-mcp
    ├── 🧠 Reasoning...
- Using images:
    └── ❌ LLM Failed┌───────────────────────────────── LLM Error ─────────────────────────────────┐
  - mcp/arxiv-mcp-server@sha256:6dc6bba6dfed97f4ad6eb8d23a5c98ef5b7fa6184937d54b2d675801cd9dd29e
│                                                                             │
  - mcp/duckduckgo@sha256:68eb20db6109f5c312a695fc5ec3386ad15d93ffb765a0b4eb1baf4328dec14f
│  ❌ LLM Call Failed                                                         │
  - mcp/wikipedia-mcp@sha256:e90e3f6e3bb795f10d3d8d8ac1523f6d46d4f2ab5ac3b3a4720e1be879466472
│  Error:           │
> Images pulled in 54.4812ms
│  VertexAIException - {                                                      │
- Listing MCP tools...
│    "error": {                                                               │
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
│      "code": 503,                                                           │
  - Running mcp/duckduckgo with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=duckduckgo -l docker-mcp-transport=stdio]
│      "message": "The model is overloaded. Please try again later.",         │
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers:ro]
│      "status": "UNAVAILABLE"                                                │
  > duckduckgo: (2 tools)
│    }                                                                        │
  > wikipedia-mcp: (10 tools) (9 resourceTemplates)
│  }                                                                          │
  > arxiv-mcp-server: (4 tools) (1 prompts)
│                                                                             │
> 16 tools listed in 2.678974s
│                                                                             │
- Watching for configuration updates...
└─────────────────────────────────────────────────────────────────────────────┘
> Initialized in 2.7509888s

> Start stdio server
🚀 Crew: crew
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\fields.py:1093: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'items', 'anyOf', 'enum', 'properties'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
└── 📋 Task: 3d4c6b53-e20d-4ff9-9dd1-edfa0361cca1
  warn(
    Status: Executing Task...
  - Scanning tool call arguments for secrets...
    ├── ✅ Reasoning Completed
  > No secret found in arguments.
    └── ❌ LLM Failed┌───────────────────────────── 🧠 Reasoning Plan ─────────────────────────────┐
  - Calling tool download_paper with arguments: {"check_status":false,"paper_id":"2306.04050v2"}
│                                                                             │
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
│  ## Plan for Retrieving Future Document Content                             │
  > Calling tool download_paper took: 1.8172985s
│                                                                             │
  - Scanning tool call response for secrets...
│  ### 1. Understanding the Task                                              │
  > No secret found in response.
│                                                                             │
  - Scanning tool call arguments for secrets...
│  The task requires me to act as a deterministic research agent. Given a     │
  > No secret found in arguments.
│  document prefix, I need to predict and retrieve information likely to      │
  - Calling tool summarize_article_section with arguments: {"max_length":500,"section_title":"II. INTUITIVE EXPLANATION OF THE MAIN IDEA","title":"LLMZip: Lossless Text Compression using Large Language Models"}
│  appear later in the same document. This information should be presented    │
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
│  as a concise textual synopsis, suitable for conditioning a language model  │
  > Calling tool summarize_article_section took: 1.4994153s
│  compressor. I must prioritize authoritative sources (the document itself)  │
  - Scanning tool call response for secrets...
│  and avoid hallucination, only including content directly supported by the  │
  > No secret found in response.
│  available tools. The synopsis should cover facts, equations, definitions,  │
> Stop watching for updates
│  datasets, and references, maximizing relevance to the document prefix.     │
│                                                                             │
│  The provided document prefix introduces "LLMZip," a lossless text          │
│  compression method using Large Language Models (LLMs), specifically        │
│  LLaMA-7B. It claims new, lower estimates for English entropy and superior  │
│  performance over state-of-the-art compressors like BSC, ZPAQ, and paq8h.   │
│  It also highlights the connection between prediction and compression,      │
│  mentioning Shannon's work and arithmetic coding.                           │
│                                                                             │
│  ### 2. Key Steps to Complete the Task                                      │
│                                                                             │
│  1.  **Access the Full Document:** The most authoritative source for        │
│  "likely future content" is the full document itself. I will use the        │
│  provided arXiv ID to download and read the paper.                          │
│  2.  **Identify Document Structure:** Once the paper is read, I will        │
│  identify its main sections to understand where specific types of           │
│  information (methodology, experiments, results, related work) are          │
│  typically located.                                                         │
│  3.  **Extract Relevant Information:** I will systematically go through     │
│  the later sections of the paper, focusing on details that elaborate on     │
│  the claims made in the abstract and introduction. This includes:           │
│      *   The detailed methodology of LLMZip (how LLaMA-7B is integrated     │
│  with a lossless compression scheme, likely arithmetic coding).             │
│      *   The specific "new estimates" of English entropy.                   │
│      *   The experimental setup (datasets, evaluation metrics, hardware).   │
│      *   The quantitative results comparing LLMZip with BSC, ZPAQ, and      │
│  paq8h.                                                                     │
│      *   Any key equations, algorithms, or definitions central to the       │
│  proposed method.                                                           │
│      *   Elaboration on the references mentioned ([1], [2], [3], [4]) and   │
│  other related work.                                                        │
│  4.  **Synthesize Synopsis:** I will compile the extracted information      │
│  into a concise, factual synopsis, ensuring it directly addresses the       │
│  themes introduced in the prefix and is free from speculation.              │
│                                                                             │
│  ### 3. Approach to Challenges                                              │
│                                                                             │
│  *   **Information Overload:** The full paper will contain more             │
│  information than needed for a concise synopsis. I will use                 │
│  section-specific summarization and fact extraction tools to filter for     │
│  the most relevant details pertaining to the core claims and methodology.   │
│  *   **Conciseness:** I will focus on key facts, numerical results, and     │
│  methodological steps, avoiding lengthy explanations or background          │
│  information already covered in the prefix. The goal is a "compressor       │
│  conditioning" synopsis, implying dense, relevant information.              │
│  *   **Avoiding Hallucination:** I will strictly adhere to the content      │
│  found within the downloaded paper. If a detail is not explicitly stated,   │
│  it will not be included.                                                   │
│                                                                             │
│  ### 4. Strategic Use of Available Tools                                    │
│                                                                             │
│  1.  **`download_paper(arxiv_id="2306.04050v2")`**: This is the initial     │
│  and most crucial step. It will retrieve the full PDF of the paper, which   │
│  is the primary authoritative source.                                       │
│  2.  **`read_paper(file_path="2306.04050v2.pdf")`**: After downloading,     │
│  this tool will convert the PDF content into a readable text format,        │
│  allowing for further processing.                                           │
│  3.  **`get_sections(paper_content)`**: This tool will be used on the       │
│  `paper_content` obtained from `read_paper` to get a structured list of     │
│  sections. This helps in navigating the document efficiently and            │
│  identifying where the methodology, results, and detailed discussions are   │
│  located.                                                                   │
│  4.  **`summarize_article_section(section_content, query="...")` or         │
│  `extract_key_facts(section_content)`**: For each relevant section (e.g.,   │
│  "Methodology," "Experiments," "Results," "Related Work"), I will use       │
│  these tools.                                                               │
│      *   `summarize_article_section` will be used with specific queries     │
│  (e.g., "details on LLMZip algorithm," "new entropy estimates,"             │
│  "experimental setup and datasets," "performance comparison with BSC,       │
│  ZPAQ, paq8h") to extract the core information efficiently.                 │
│      *   `extract_key_facts` might be used if specific, granular facts      │
│  (like exact equations, specific dataset names, or precise numerical        │
│  results) are needed and `summarize_article_section` provides too           │
│  high-level an overview.                                                    │
│      *   I will iterate through the identified sections, extracting         │
│  information related to the LLMZip algorithm, entropy estimation,           │
│  experimental setup, and performance results.                               │
│                                                                             │
│  ### 5. Expected Outcome                                                    │
│                                                                             │
│  The expected outcome is a concise, plain-text synopsis. It will detail:    │
│  *   The specific architecture or algorithmic steps of LLMZip, explaining   │
│  how LLaMA-7B's predictions are integrated with a lossless compression      │
│  scheme (e.g., arithmetic coding).                                          │
│  *   The quantitative values of the new asymptotic upper bound on English   │
│  entropy.                                                                   │
│  *   The datasets used for training/evaluation and the experimental setup   │
│  (e.g., hardware, specific metrics).                                        │
│  *   The comparative performance results of LLMZip against BSC, ZPAQ, and   │
│  paq8h, likely including specific compression ratios or bits per character  │
│  (BPC) values.                                                              │
│  *   Any key mathematical equations or definitions that underpin the        │
│  LLMZip method or its evaluation.                                           │
│  *   Contextual information or specific findings related to the references  │
│  [1], [2], [3], [4] if elaborated upon in the paper.                        │
│                                                                             │
│  This outcome directly addresses the task requirements by providing         │
│  relevant, factual, and non-hallucinated information from the               │
│  authoritative source, structured for conditioning a language model         │
│  compressor.                                                                │
│                                                                             │
│  READY: I am ready to execute the task.                                     │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Task:[00m [92mYou act as a deterministic research agent. Given the observed prefix of a document, use available MCP tools to retrieve information that is likely to appear later in the same document. Prefer authoritative sources. Produce a concise textual synopsis of the likely future content (facts, equations, definitions, datasets, references), suitable for conditioning a language model compressor. Maximize relevance to the document. Do not hallucinate; only include content supported by the tools.

Document prefix (UTF-8 text):
# LLMZip: Lossless Text Compression using Large Language Models

**Authors:** Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas Shakkottai  
**Affiliation:** Department of Electrical and Computer Engineering, Texas A&M University  
**Email:** {vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu  
**arXiv:** 2306.04050v2 [cs.IT] 26 Jun 2023

## Abstract

We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.

## I. INTRODUCTION

There are close connections between learning, prediction, and compression. The success of ChatGPT has captured the fascination of general public and brought the connection between learning and prediction to the fore. The main advance brought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in a paragraph based on knowing the past several words (tokens).

The connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the entropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past values can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many algorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of lossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very effective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor and every time there

Output: A concise synopsis (plain text).

Reasoning Plan:
## Plan for Retrieving Future Document Content

### 1. Understanding the Task

The task requires me to act as a deterministic research agent. Given a document prefix, I need to predict and retrieve information likely to appear later in the same document. This information should be presented as a concise textual synopsis, suitable for conditioning a language model compressor. I must prioritize authoritative sources (the document itself) and avoid hallucination, only including content directly supported by the available tools. The synopsis should cover facts, equations, definitions, datasets, and references, maximizing relevance to the document prefix.

The provided document prefix introduces "LLMZip," a lossless text compression method using Large Language Models (LLMs), specifically LLaMA-7B. It claims new, lower estimates for English entropy and superior performance over state-of-the-art compressors like BSC, ZPAQ, and paq8h. It also highlights the connection between prediction and compression, mentioning Shannon's work and arithmetic coding.

### 2. Key Steps to Complete the Task

1.  **Access the Full Document:** The most authoritative source for "likely future content" is the full document itself. I will use the provided arXiv ID to download and read the paper.
2.  **Identify Document Structure:** Once the paper is read, I will identify its main sections to understand where specific types of information (methodology, experiments, results, related work) are typically located.
3.  **Extract Relevant Information:** I will systematically go through the later sections of the paper, focusing on details that elaborate on the claims made in the abstract and introduction. This includes:
    *   The detailed methodology of LLMZip (how LLaMA-7B is integrated with a lossless compression scheme, likely arithmetic coding).
    *   The specific "new estimates" of English entropy.
    *   The experimental setup (datasets, evaluation metrics, hardware).
    *   The quantitative results comparing LLMZip with BSC, ZPAQ, and paq8h.
    *   Any key equations, algorithms, or definitions central to the proposed method.
    *   Elaboration on the references mentioned ([1], [2], [3], [4]) and other related work.
4.  **Synthesize Synopsis:** I will compile the extracted information into a concise, factual synopsis, ensuring it directly addresses the themes introduced in the prefix and is free from speculation.

### 3. Approach to Challenges

*   **Information Overload:** The full paper will contain more information than needed for a concise synopsis. I will use section-specific summarization and fact extraction tools to filter for the most relevant details pertaining to the core claims and methodology.
*   **Conciseness:** I will focus on key facts, numerical results, and methodological steps, avoiding lengthy explanations or background information already covered in the prefix. The goal is a "compressor conditioning" synopsis, implying dense, relevant information.
*   **Avoiding Hallucination:** I will strictly adhere to the content found within the downloaded paper. If a detail is not explicitly stated, it will not be included.

### 4. Strategic Use of Available Tools

1.  **`download_paper(arxiv_id="2306.04050v2")`**: This is the initial and most crucial step. It will retrieve the full PDF of the paper, which is the primary authoritative source.
2.  **`read_paper(file_path="2306.04050v2.pdf")`**: After downloading, this tool will convert the PDF content into a readable text format, allowing for further processing.
3.  **`get_sections(paper_content)`**: This tool will be used on the `paper_content` obtained from `read_paper` to get a structured list of sections. This helps in navigating the document efficiently and identifying where the methodology, results, and detailed discussions are located.
4.  **`summarize_article_section(section_content, query="...")` or `extract_key_facts(section_content)`**: For each relevant section (e.g., "Methodology," "Experiments," "Results," "Related Work"), I will use these tools.
    *   `summarize_article_section` will be used with specific queries (e.g., "details on LLMZip algorithm," "new entropy estimates," "experimental setup and datasets," "performance comparison with BSC, ZPAQ, paq8h") to extract the core information efficiently.
    *   `extract_key_facts` might be used if specific, granular facts (like exact equations, specific dataset names, or precise numerical results) are needed and `summarize_article_section` provides too high-level an overview.
    *   I will iterate through the identified sections, extracting information related to the LLMZip algorithm, entropy estimation, experimental setup, and performance results.

### 5. Expected Outcome

The expected outcome is a concise, plain-text synopsis. It will detail:
*   The specific architecture or algorithmic steps of LLMZip, explaining how LLaMA-7B's predictions are integrated with a lossless compression scheme (e.g., arithmetic coding).
*   The quantitative values of the new asymptotic upper bound on English entropy.
*   The datasets used for training/evaluation and the experimental setup (e.g., hardware, specific metrics).
*   The comparative performance results of LLMZip against BSC, ZPAQ, and paq8h, likely including specific compression ratios or bits per character (BPC) values.
*   Any key mathematical equations or definitions that underpin the LLMZip method or its evaluation.
*   Contextual information or specific findings related to the references [1], [2], [3], [4] if elaborated upon in the paper.

This outcome directly addresses the task requirements by providing relevant, factual, and non-hallucinated information from the authoritative source, structured for conditioning a language model compressor.

READY: I am ready to execute the task.[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mAction: download_paper[00m
[95m## Using tool:[00m [92mdownload_paper[00m
[95m## Tool Input:[00m [92m
"{\"paper_id\": \"2306.04050v2\"}"[00m
[95m## Tool Output:[00m [92m
{"status": "success", "message": "Paper already available", "resource_uri": "file:///app/papers/2306.04050v2.md", "pdf_uri": "https://arxiv.org/pdf/2306.04050v2.pdf", "content": "1\n\n# LLMZip: Lossless Text Compression using Large Language Models\n\n\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\n\n\nAbstract\n\n\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\n\n\nI. I NTRODUCTION\n\n\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\n\n\nII. I NTUITIVE EXPLANATION OF THE MAIN IDEA\n\n\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\n\nMy first attempt at writing a book\n\n\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032, \u2032 a \u2032, \u2032 book \u2032\n\n\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n\n\n2\n\n\nFigure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be\nwords such as [\u2032] reading [\u2032], [\u2032] writing [\u2032], [\u2032] driving [\u2032], [\u2032] cooking [\u2032] etc. The main idea is to compute the rank of the actual word in\nour sentence ( [\u2032] writing [\u2032] ) in this list and call it R 5 . We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for [\u2032] writing [\u2032] is R 5 = 1.\n\n\nTokens\n\n\n\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x5|x4 1)<br>5|Rank<br>computation|R5 = 1|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nreading 0.3\nwriting 0.2\ncycling 0.1\ndriving 0.05\n\n... ...\n\n\n\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\n\n\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, [\u2032] a [\u2032], and hence, the rank R 6 would be 0.\n\n\nTokens\n\n\n\n\n\n\u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x6|x5 2)<br>6|Rank<br>computation|R6 = 0|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nan 0.2\n\nthe 0.05\n\n... ...\n\n\n\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\n\n\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n\n\n1, 0, 0, . . .\n\n\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\n\n\nSequence of ranks\n\n|e of ranks|Col2|Col3|\n|---|---|---|\n|. ., rNT|Compression (zip)|Nb|\n||||\n\n\n\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\n\n\nWhen we wish to reconstruct the sequence, we first decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank R i during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\n\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n\n\n3\n\n\nIII. C OMPRESSION USING LLM S\n\n\nLet s denote a sentence from the English language composed of N c letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We first parse s into a sequence of N T tokens\ndenoted by x = x 1, x 2, . . ., x i\u22121, x i, x i+1, . . . x N T, where x i \u2208X . There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. x i \u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter X i .\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens x i\u2212M, x i\u2212M+1, . . ., x i\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nq i (x i ) := Pr(X i = x i |x i\u22121, x i\u22122, . . ., x i\u2212M ), \u2200x i \u2208X . The PMF vector q i := [q i (1), q i (2), . . ., q i (D)] [T] is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcq i . Let \u03b3 i : X \u2192X be a permutation on the integers\nfrom 1 to D such that\n\n\u02dc\nq i (\u03b3 i (j)) = q i (j), \u2200j \u2208X .\n\n\nThat is, \u03b3 i (j) is the rank of the token j at epoch i. We define the rank of the input sequence at epoch i as the rank of the\ntoken x i at epoch i, r i := \u03b3 i (x i ). The sequence {r i } [N] i=1 [T] [is compressed by a lossless compression algorithm (such as zlib) to]\nproduce N b bits which are the final bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors q i \u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 defined as\n\n\u03c1 := [N] [b] bits/character.\n\nN c\n\n\n\nLosseless\n\nCompression\n\n\n\nTokens . . ., x i\u2212M, . . ., x i\u22121\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n\nInput sentence s with N c characters\n\n\n\ni i i\u22121 Rank\n\nLLM\ncomputation\n\n\n\nq i = Pr(X i = j|x [i] i [\u2212] \u2212 [M] 1 [)] Rank r i Losseless N b bits\n\n\n\nFig. 4. Schematic showing the prediction at epoch i.\n\n\nA. Entropy bounds\n\n\nLet S \u2208S [\u221e] be a random process that represents language input. The nth character in the sequence is denoted by S n,\nwhereas the string of characters from the beginning to the nth character is expressed as S n . The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X 1, X 2, . . . using a variable-length mapping. In this sequence, X i is the ith\ntoken. The number of characters employed to generate X i depends on the realization of the random process and, as such,\nwe introduce random variable B i to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which B i \u2265 1 and B i is uniformly bounded, with B i < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded N T \u2192\u221e, the number of characters must also approach infinity N c \u2192\u221e. Formally, consider the tokenizer\nfunction T : S [N] \u2192X [N] operating on infinite symbol sequences; that is, T (s) = x where s is an infinite sequence in S [\u221e] . For\nnatural number, i \u2208 N, define m i : S [N] \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Specifically, suppose s is given, then\n\n\nm i (s) = min n [{][length (][T][ (][s] [n] [))][ \u2265] [i][}][ .] (1)\n\n\nWe note that, by construction, lim n\u2192\u221e length (T (s n )) = \u221e and, as such, m i (\u00b7) is well-defined. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple finite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (s n ) = T (s n+1 ). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (finite) series of input symbols in the restricted\nset for which T (y n ) = x i is s m i (s) . Given a fixed sequence s, we can express the number of characters contained in a token\n\nas\nb i = m i (s) \u2212 m i\u22121 (s)\n\n\n4\n\n\nwith initial condition m \u22121 = 0. Consequently, the number of characters embedded in the first N T tokens for a random input\nbecomes N c = [\ufffd] i [N] =1 [T] [B] [i] [.]\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {S k } [\u221e] k=1 [,]\n{B i } [\u221e] i=1 [, and][ {][X] [i] [}] [\u221e] i=1 [are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that]\n\n\n\n\u2212 [1]\n\n\n\n\n[1]\n\nn [log] [2] [ p] [S] [n] [(][S] [1] [, . . ., S] [n] [) =][ \u2212] n [1]\n\n\n\nn [log] [2] [ p] [S] [n] [(][S] [n] [)][ \u2192] [H][(][S][)] almost surely. (2)\n\n\n\nLet \u2126 S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\n\n\n\n[1]\n\ni [log] [2] [ p] [X] [i] [(][X] [1] [, . . ., X] [i] [) =][ \u2212] [1] i\n\n\n\n\u2212 [1]\n\n\n\ni [log] [2] [ p] [X] [i] [(][X] [i] [)][ \u2192] [H][(][X][)] almost surely. (3)\n\n\n\nDefine \u2126 X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\n\n\nm i (S)\nlim = E [B] almost surely. (4)\ni\u2192\u221e i\n\n\nSet \u2126 B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126 S \u2229 \u2126 X \u2229 \u2126 B, we deduce that\n\n\nH(S) = lim\nk\u2192\u221e [\u2212] k [1] [log] [2] [ p] [S] [k] [(][S] [k] [(][\u03c9][))]\n\n= lim i\u2192\u221e [\u2212] l [1] i log 2 p S li (S l i (\u03c9))\n\n\n= lim log 2 Pr (X i = T (S l i (\u03c9)))\ni\u2192\u221e [\u2212] l [1] i\n\n\n\n1\n\ni [log] [2] [ Pr (][X] [i] [ =][ x] [i] [) =][ H] E [(] B [X][)]\n\n\n\n1 1\n\n= \u2212\nE[B] i [lim] \u2192\u221e i\n\n\n\nE[B] [.]\n\n\n\nThe first equality follows from (2). The second equality is a consequence of the fact that {l i = m i (S(\u03c9))|i \u2208 N} is an infinite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\n\ntwo events,\n{\u03c9 \u2208 \u2126|X i (\u03c9) = x i } = {\u03c9 \u2208 \u2126|T (S m i (S(\u03c9)) ) = x i }.\n\n\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126 B . The sets \u2126 S, \u2126 X, and \u2126 B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\n\nH(S) = [H][(][X][)] almost surely.\n\nE[B]\n\n\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\n\n\n\nPr\n\n\ufffd\n\n\n\n\ufffd\n\n\n\nH(X) \u2264 lim\nN T \u2192\u221e [\u2212] N [1] T\n\n\n\nN T\n\ufffd\n\n\n\n\ufffd log 2 q i (X i )\n\n\ni=1\n\n\n\n= 1, (5)\n\n\n\nwhere q i (\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\n\n\n\n\n[1] N T\n\nN T \ufffd i=1 [log] 2 [q] [i] [(][X] [i] [)]\n\n\n\nH(S) \u2264 lim N T \u2192\u221e \u2212 N [1]\n\n\n\n. (6)\nE[B]\n\n\n\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by H ub . The\nnumerator in (6) represents the average number of bits required to represent the tokens X N T and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\n\n\nB. Encoding schemes\n\n\nWe consider three schemes for the lossless compression block in Fig. 3.\n\n\n5\n\n\n1) Compressing the ranks using zlib: The first scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1 LLaMA+zlib .\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token x i at epoch i by using a prefix-free code assuming q i to be the true distribution of\nthe tokens. A natural choice for a prefix-free code is a Huffman code. Instead, for simplicity, we use a prefix-free code where\n1\nthe codeword for the token x i is of length l i = \u2308log 2 q i (x i ) [\u2309][. A prefix-free code with this length for][ x] [i] [ is guaranteed to exist]\nsince this choice of lengths satisfies the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1 LLaMA+TbyT,\nis given by\n\n\n\nN T\n\ufffd\n\n\ni=1\n\n\n\n1\nlog 2\n\ufffd q i (x i )\n\n\n\n\ufffd\n\n\n\n\u03c1 LLaMA+TbyT =\n\n\n\n\ufffd Ni=1 T [b] [i] .\n\n\n\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use q i (x i ) as the probability of token x i at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1 LLM+AC . It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\n\n\n\u03c1 LLM+AC \u2248\n\n\n\nN T 1\n\ufffd i=1 log 2 q i (x i )\n\n\ufffd Ni=1 T [b] [i] . (7)\n\n\n\nClearly, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and \u03c1 LLM+AC also provide upper bounds on H(S). H ub, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and\n\u03c1 LLM+AC are estimated using a finite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\n\n\nIV. R ESULTS\n\n\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it difficult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\n[Nevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of](http://mattmahoney.net/dc/text8.zip)\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M ) is shown in Table II. As expected, the\ncompression performance improves with increasing M . We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n\n\n6\n\n\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\n\n\nV. A CKNOWLEDGEMENT\n\n\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\n\n\nTABLE I\n\nR ESULTS FOR 1MB OF TEXT FROM TEXT 8 DATASET\n\n\n\nBatch\nN c N T\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC ZPAQ pq8h\n\n\n\nNo. (bpc) file size (bits) (bpc) (bpc) (bpc) (bpc)\n\n\n1 466, 650 100, 000 0.6882 1.0513 0.8215 0.689\n\n\n2 461, 477 100, 000 0.6893 1.0558 0.8242 0.6901\n\n\n3 454, 599 100, 000 0.699 1.0681 0.8357 0.6999\n\n\n4 462, 755 100, 000 0.6748 1.0346 0.8093 0.6757\n\n\n5 453, 847 100, 000 0.7481 1.1265 0.8831 0.749\n\n\n6 458, 252 100, 000 0.7218 1.0957 0.8567 0.7227\n\n\n7 451, 036 100, 000 0.6959 1.0729 0.8353 0.6968\n\n\n8 447, 953 100, 000 0.7092 1.0896 0.8489 0.7101\n\n\n9 462, 665 100, 000 0.7394 1.1126 0.8713 0.7402\n\n\n10 449, 621 100, 000 0.7269 1.1046 0.8643 0.7277\n\n\nTotal 9, 137, 710 2, 000, 000 0.7093 1.0812 0.845 0.7101 1.4 [1] 1.2 [2]\n\n\nTABLE II\n\nC OMPRESSION PERFORMANCE OF THE LLM ON THE TEXT 8 DATASET, AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\n\n\n(bpc) file size (bits) (bpc) (bpc)\n\n\n\n31 4, 568, 855 1, 000, 000 0.9139 1.3159 1.0425 0.9145\n\n\n127 4, 568, 855 1, 000, 000 0.7511 1.1303 0.8847 0.752\n\n\n255 4, 568, 855 1, 000, 000 0.7242 1.0985 0.859 0.725\n\n\n511 4, 568, 855 1, 000, 000 0.7093 1.0812 0.845 0.7101\n\n\n1 This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2 This result is taken from [12] and it corresponds to the full 100MB dataset text8\n\n\n7\n\n\n\nTABLE III\n\nM EAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\nM\n\n(bpc) (bpc) (bpc) (bpc)\n\n\n31 0.9139 \u00b1 0.0263 1.3159 \u00b1 0.0329 1.0425 \u00b1 0.0262 0.9145 \u00b1 0.0263\n\n\n127 0.7511 \u00b1 0.0233 1.1303 \u00b1 0.0292 0.8847 \u00b1 0.0231 0.752 \u00b1 0.0233\n\n\n255 0.7242 \u00b1 0.0234 1.0985 \u00b1 0.0289 0.859 \u00b1 0.0232 0.725 \u00b1 0.0234\n\n\n511 0.7093 \u00b1 0.0228 1.0812 \u00b1 0.028 0.845 \u00b1 0.0226 0.7101 \u00b1 0.0228\n\n\nTABLE IV\n\nC OMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN P ROJECT G UTENBERG [7], AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC Standalone Zlib\n\n\n(bpc) (bpc) (bpc) (bpc) (bpc)\n\n\n\n31 508, 463 115, 000 1.0919 1.5316 1.2152 1.0924 2.80\n\n\n127 508, 463 115, 000 0.8973 1.3128 1.0235 0.8982 2.80\n\n\n255 508, 463 115, 000 0.8618 1.2684 0.9899 0.8627 2.80\n\n\n511 508, 463 115, 000 0.8417 1.2465 0.9711 0.8426 2.80\n\n\n8\n\n\nR EFERENCES\n\n\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR filters),\u201d 2023.\n\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and efficient foundation language models,\u201d\n2023.\n\n[7] J. Frank Dobie, Legends of Texas, United States, Texas Folk-Lore Society, 1924; Project Gutenberg, May 25, 2023, 2023,\n[https://www.gutenberg.org/ebooks/70859.](https://www.gutenberg.org/ebooks/70859)\n\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n\n\n"}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mThought: The paper has been downloaded and its content is available. I need to extract the relevant information to form a concise synopsis of the likely future content. I will go through the sections and identify key facts, equations, definitions, datasets, and references.
Based on the prefix and the full content, the following sections are relevant:
*   **II. INTUITIVE EXPLANATION OF THE MAIN IDEA**: Explains the core concept of using LLMs to predict tokens and compress ranks.
*   **III. COMPRESSION USING LLMS**: Details the formal methodology, entropy bounds, and different encoding schemes (zlib, token-by-token, arithmetic coding). This section will contain key equations and definitions.
*   **IV. RESULTS**: Presents the experimental setup, datasets used, and the quantitative performance comparison of LLMZip against baselines. This will include specific numerical results and comparisons.
*   **REFERENCES**: Provides the full details for the cited works.
I will now extract information from these sections.
First, I will summarize the "Intuitive Explanation of the Main Idea" to capture the core mechanism.[00m
[95m## Using tool:[00m [92msummarize_article_section[00m
[95m## Tool Input:[00m [92m
"{\"title\": \"LLMZip: Lossless Text Compression using Large Language Models\", \"section_title\": \"II. INTUITIVE EXPLANATION OF THE MAIN IDEA\", \"max_length\": 500}"[00m
[95m## Tool Output:[00m [92m
{"title":"LLMZip: Lossless Text Compression using Large Language Models","section_title":"II. INTUITIVE EXPLANATION OF THE MAIN IDEA","summary":"No Wikipedia article found for 'LLMZip: Lossless Text Compression using Large Language Models'."}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Final Answer:[00m [92m
LLMZip's methodology involves parsing text into tokens, using LLaMA-7B to predict the probability mass function (PMF) for the next token given past tokens, and then compressing either the ranks of the actual tokens or directly using the PMF with arithmetic coding. Three encoding schemes are proposed: LLaMA+zlib (compressing ranks with zlib), LLaMA+TbyT (token-by-token compression using prefix-free codes based on predicted probabilities), and LLM+AC (arithmetic coding using predicted probabilities, expected to be nearly optimal).

The paper provides an asymptotic upper bound on the entropy rate of English, H(S), calculated as `[-1/N_T * sum(i=1 to N_T) log2(q_i(X_i))] / E[B]`, where `q_i(X_i)` is the predicted probability of token `X_i` and `E[B]` is the average characters per token. This bound is compared to previous estimates, such as 1.3 bits/character by Cover and King [1] and more recent neural network-based estimates [2].

Experiments utilize LLaMA-7B [6] with a SentencePiece tokenizer [11] (dictionary size 32000). Performance is evaluated on a 1MB section of the `text8` dataset and a 100KB section (100,000 tokens) from a Project Gutenberg book [7]. Baselines include ZPAQ (1.4 bpc on `text8` [5]), paq8h (1.2 bpc on `text8` [12]), and standalone zlib (2.8 bpc on the Gutenberg text).

Results show that on 1MB of `text8` (average over 10 batches, memory M=511), the asymptotic upper bound H_ub is 0.7093 bits/character, and LLM+AC achieves 0.7101 bits/character. LLaMA+TbyT yields 0.845 bpc, and LLaMA+zlib yields 1.0812 bpc. These significantly outperform ZPAQ and paq8h. On the Project Gutenberg text (M=511), H_ub is 0.8417 bpc, and LLM+AC achieves 0.8426 bpc, also outperforming baselines including standalone zlib. Compression performance improves with increased LLM memory (M), with inference time scaling linearly. The authors note that `text8` results might be optimistic due to LLaMA-7B's training on Wikipedia, from which `text8` is derived.[00m


🚀 Crew: crew
└── 📋 Task: 3d4c6b53-e20d-4ff9-9dd1-edfa0361cca1
    Assigned to: AI Assistant
    Status: ✅ Completed
    ├── ✅ Reasoning Completed
    ├── 🔧 Used download_paper (1)
    └── 🔧 Used summarize_article_section (1)┌────────────────────────────── Task Completion ──────────────────────────────┐
│                                                                             │
│  Task Completed                                                             │
│  Name: 3d4c6b53-e20d-4ff9-9dd1-edfa0361cca1                                 │
│  Agent: AI Assistant                                                        │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

┌────────────────────────────── Crew Completion ──────────────────────────────┐
│                                                                             │
│  Crew Execution Completed                                                   │
│  Name: crew                                                                 │
│  ID: 1305f696-9e44-40a4-8b28-e496f68ab681                                   │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

AGENT_RESULT_JSON:{"final_text": "LLMZip's methodology involves parsing text into tokens, using LLaMA-7B to predict the probability mass function (PMF) for the next token given past tokens, and then compressing either the ranks of the actual tokens or directly using the PMF with arithmetic coding. Three encoding schemes are proposed: LLaMA+zlib (compressing ranks with zlib), LLaMA+TbyT (token-by-token compression using prefix-free codes based on predicted probabilities), and LLM+AC (arithmetic coding using predicted probabilities, expected to be nearly optimal).\n\nThe paper provides an asymptotic upper bound on the entropy rate of English, H(S), calculated as `[-1/N_T * sum(i=1 to N_T) log2(q_i(X_i))] / E[B]`, where `q_i(X_i)` is the predicted probability of token `X_i` and `E[B]` is the average characters per token. This bound is compared to previous estimates, such as 1.3 bits/character by Cover and King [1] and more recent neural network-based estimates [2].\n\nExperiments utilize LLaMA-7B [6] with a SentencePiece tokenizer [11] (dictionary size 32000). Performance is evaluated on a 1MB section of the `text8` dataset and a 100KB section (100,000 tokens) from a Project Gutenberg book [7]. Baselines include ZPAQ (1.4 bpc on `text8` [5]), paq8h (1.2 bpc on `text8` [12]), and standalone zlib (2.8 bpc on the Gutenberg text).\n\nResults show that on 1MB of `text8` (average over 10 batches, memory M=511), the asymptotic upper bound H_ub is 0.7093 bits/character, and LLM+AC achieves 0.7101 bits/character. LLaMA+TbyT yields 0.845 bpc, and LLaMA+zlib yields 1.0812 bpc. These significantly outperform ZPAQ and paq8h. On the Project Gutenberg text (M=511), H_ub is 0.8417 bpc, and LLM+AC achieves 0.8426 bpc, also outperforming baselines including standalone zlib. Compression performance improves with increased LLM memory (M), with inference time scaling linearly. The authors note that `text8` results might be optimistic due to LLaMA-7B's training on Wikipedia, from which `text8` is derived.", "duration_ms": 49334}
LLMZip's methodology involves parsing text into tokens, using LLaMA-7B to predict the probability mass function (PMF) for the next token given past tokens, and then compressing either the ranks of the actual tokens or directly using the PMF with arithmetic coding. Three encoding schemes are proposed: LLaMA+zlib (compressing ranks with zlib), LLaMA+TbyT (token-by-token compression using prefix-free codes based on predicted probabilities), and LLM+AC (arithmetic coding using predicted probabilities, expected to be nearly optimal).

The paper provides an asymptotic upper bound on the entropy rate of English, H(S), calculated as `[-1/N_T * sum(i=1 to N_T) log2(q_i(X_i))] / E[B]`, where `q_i(X_i)` is the predicted probability of token `X_i` and `E[B]` is the average characters per token. This bound is compared to previous estimates, such as 1.3 bits/character by Cover and King [1] and more recent neural network-based estimates [2].

Experiments utilize LLaMA-7B [6] with a SentencePiece tokenizer [11] (dictionary size 32000). Performance is evaluated on a 1MB section of the `text8` dataset and a 100KB section (100,000 tokens) from a Project Gutenberg book [7]. Baselines include ZPAQ (1.4 bpc on `text8` [5]), paq8h (1.2 bpc on `text8` [12]), and standalone zlib (2.8 bpc on the Gutenberg text).

Results show that on 1MB of `text8` (average over 10 batches, memory M=511), the asymptotic upper bound H_ub is 0.7093 bits/character, and LLM+AC achieves 0.7101 bits/character. LLaMA+TbyT yields 0.845 bpc, and LLaMA+zlib yields 1.0812 bpc. These significantly outperform ZPAQ and paq8h. On the Project Gutenberg text (M=511), H_ub is 0.8417 bpc, and LLM+AC achieves 0.8426 bpc, also outperforming baselines including standalone zlib. Compression performance improves with increased LLM memory (M), with inference time scaling linearly. The authors note that `text8` results might be optimistic due to LLaMA-7B's training on Wikipedia, from which `text8` is derived.
C:\tools\Anaconda3\Lib\site-packages\paramiko\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "cipher": algorithms.TripleDES,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.
  "class": algorithms.Blowfish,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "class": algorithms.TripleDES,
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\_internal\_generate_schema.py:628: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.
  warn(
- Reading configuration...
  - Reading registry from registry.yaml
  - Reading catalog from docker-mcp.yaml
  - Reading config from config.yaml
- Configuration read in 52.6018ms
- Watching registry at C:\Users\Noah\.docker\mcp\registry.yaml
- Watching config at C:\Users\Noah\.docker\mcp\config.yaml
- Those servers are enabled: arxiv-mcp-server, duckduckgo, wikipedia-mcp
- Using images:
  - mcp/arxiv-mcp-server@sha256:6dc6bba6dfed97f4ad6eb8d23a5c98ef5b7fa6184937d54b2d675801cd9dd29e
  - mcp/duckduckgo@sha256:68eb20db6109f5c312a695fc5ec3386ad15d93ffb765a0b4eb1baf4328dec14f
  - mcp/wikipedia-mcp@sha256:e90e3f6e3bb795f10d3d8d8ac1523f6d46d4f2ab5ac3b3a4720e1be879466472
> Images pulled in 62.3478ms
- Listing MCP tools...
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
  - Running mcp/duckduckgo with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=duckduckgo -l docker-mcp-transport=stdio]
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers:ro]
  > duckduckgo: (2 tools)
  > wikipedia-mcp: (10 tools) (9 resourceTemplates)
  > arxiv-mcp-server: (4 tools) (1 prompts)
> 16 tools listed in 3.3573044s
- Watching for configuration updates...
> Initialized in 3.4729186s
> Start stdio server
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\fields.py:1093: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'items', 'anyOf', 'enum', 'properties'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
  warn(
┌────────────────────────── Crew Execution Started ───────────────────────────┐
│                                                                             │
│  Crew Execution Started                                                     │
│  Name: crew                                                                 │
│  ID: 4d6ef100-247c-4215-ba90-0c186d0cb992                                   │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

🚀 Crew: crew
├── 📋 Task: 0134a745-0117-4160-b40c-6ebe439ed84b
│   Status: Executing Task...
│   └── ✅ Reasoning Completed
├── 🔧 Using create_reasoning_plan from LLM available_function
├── 🔄 Tool Usage Started
├── 🔧 Using create_reasoning_plan from LLM available_function
└── ✅ Tool Usage Completed┌───────────────────────────── 🧠 Reasoning Plan ─────────────────────────────┐
│                                                                             │
│  1. **Identify the document:** The document is "LLMZip: Lossless Text       │
│  Compression using Large Language Models" (arXiv: 2306.04050v2).            │
│  2. **Search for the full document:** Use `search_papers` with the title    │
│  and arXiv ID to locate the paper.                                          │
│  3. **Retrieve the document content:** Use `read_paper` with the obtained   │
│  `paper_id` to access the full text of the paper.                           │
│  4. **Analyze the document structure:** Use `get_sections` to identify the  │
│  main sections of the paper, focusing on those that would logically follow  │
│  the Abstract and Introduction (e.g., Methodology, Experiments, Results,    │
│  Discussion, Conclusion, References).                                       │
│  5. **Extract relevant information from later sections:** For each          │
│  identified relevant section, use `extract_key_facts` to retrieve key       │
│  facts, equations, definitions, datasets, and references.                   │
│  6. **Synthesize the synopsis:** Combine the extracted information into a   │
│  concise textual synopsis, ensuring it is relevant to the document,         │
│  factual, and avoids hallucination. The synopsis will cover the LLMZip      │
│  algorithm, experimental setup, results, discussion, and any significant    │
│  definitions or equations.                                                  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool download_paper with arguments: {"check_status":false,"paper_id":"2306.04050"}
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
  > Calling tool download_paper took: 2.3505235s
  - Scanning tool call response for secrets...
  > No secret found in response.
[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Task:[00m [92mYou act as a deterministic research agent. Given the observed prefix of a document, use available MCP tools to retrieve information that is likely to appear later in the same document. Prefer authoritative sources. Produce a concise textual synopsis of the likely future content (facts, equations, definitions, datasets, references), suitable for conditioning a language model compressor. Maximize relevance to the document. Do not hallucinate; only include content supported by the tools.

Document prefix (UTF-8 text):
# LLMZip: Lossless Text Compression using Large Language Models

**Authors:** Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas Shakkottai  
**Affiliation:** Department of Electrical and Computer Engineering, Texas A&M University  
**Email:** {vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu  
**arXiv:** 2306.04050v2 [cs.IT] 26 Jun 2023

## Abstract

We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.

## I. INTRODUCTION

There are close connections between learning, prediction, and compression. The success of ChatGPT has captured the fascination of general public and brought the connection between learning and prediction to the fore. The main advance brought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in a paragraph based on knowing the past several words (tokens).

The connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the entropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past values can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many algorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of lossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very effective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor and every time there

Output: A concise synopsis (plain text).

Reasoning Plan:
1. **Identify the document:** The document is "LLMZip: Lossless Text Compression using Large Language Models" (arXiv: 2306.04050v2).
2. **Search for the full document:** Use `search_papers` with the title and arXiv ID to locate the paper.
3. **Retrieve the document content:** Use `read_paper` with the obtained `paper_id` to access the full text of the paper.
4. **Analyze the document structure:** Use `get_sections` to identify the main sections of the paper, focusing on those that would logically follow the Abstract and Introduction (e.g., Methodology, Experiments, Results, Discussion, Conclusion, References).
5. **Extract relevant information from later sections:** For each identified relevant section, use `extract_key_facts` to retrieve key facts, equations, definitions, datasets, and references.
6. **Synthesize the synopsis:** Combine the extracted information into a concise textual synopsis, ensuring it is relevant to the document, factual, and avoids hallucination. The synopsis will cover the LLMZip algorithm, experimental setup, results, discussion, and any significant definitions or equations.[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mAction: download_paper[00m
[95m## Using tool:[00m [92mdownload_paper[00m
[95m## Tool Input:[00m [92m
"{\"paper_id\": \"2306.04050\"}"[00m
[95m## Tool Output:[00m [92m
  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool read_paper with arguments: {"paper_id":"2306.04050"}
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
  > Calling tool read_paper took: 1.7152258s
  - Scanning tool call response for secrets...
  > No secret found in response.
{"status": "success", "message": "Paper already available", "resource_uri": "file:///app/papers/2306.04050.md", "pdf_uri": "https://arxiv.org/pdf/2306.04050.pdf", "content": "1\n\n# LLMZip: Lossless Text Compression using Large Language Models\n\n\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\n\n\nAbstract\n\n\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\n\n\nI. I NTRODUCTION\n\n\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\n\n\nII. I NTUITIVE EXPLANATION OF THE MAIN IDEA\n\n\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\n\nMy first attempt at writing a book\n\n\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032, \u2032 a \u2032, \u2032 book \u2032\n\n\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n\n\n2\n\n\nFigure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be\nwords such as [\u2032] reading [\u2032], [\u2032] writing [\u2032], [\u2032] driving [\u2032], [\u2032] cooking [\u2032] etc. The main idea is to compute the rank of the actual word in\nour sentence ( [\u2032] writing [\u2032] ) in this list and call it R 5 . We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for [\u2032] writing [\u2032] is R 5 = 1.\n\n\nTokens\n\n\n\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x5|x4 1)<br>5|Rank<br>computation|R5 = 1|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nreading 0.3\nwriting 0.2\ncycling 0.1\ndriving 0.05\n\n... ...\n\n\n\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\n\n\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, [\u2032] a [\u2032], and hence, the rank R 6 would be 0.\n\n\nTokens\n\n\n\n\n\n\u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x6|x5 2)<br>6|Rank<br>computation|R6 = 0|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nan 0.2\n\nthe 0.05\n\n... ...\n\n\n\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\n\n\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n\n\n1, 0, 0, . . .\n\n\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\n\n\nSequence of ranks\n\n|e of ranks|Col2|Col3|\n|---|---|---|\n|. ., rNT|Compression (zip)|Nb|\n||||\n\n\n\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\n\n\nWhen we wish to reconstruct the sequence, we first decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank R i during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\n\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n\n\n3\n\n\nIII. C OMPRESSION USING LLM S\n\n\nLet s denote a sentence from the English language composed of N c letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We first parse s into a sequence of N T tokens\ndenoted by x = x 1, x 2, . . ., x i\u22121, x i, x i+1, . . . x N T, where x i \u2208X . There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. x i \u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter X i .\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens x i\u2212M, x i\u2212M+1, . . ., x i\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nq i (x i ) := Pr(X i = x i |x i\u22121, x i\u22122, . . ., x i\u2212M ), \u2200x i \u2208X . The PMF vector q i := [q i (1), q i (2), . . ., q i (D)] [T] is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcq i . Let \u03b3 i : X \u2192X be a permutation on the integers\nfrom 1 to D such that\n\n\u02dc\nq i (\u03b3 i (j)) = q i (j), \u2200j \u2208X .\n\n\nThat is, \u03b3 i (j) is the rank of the token j at epoch i. We define the rank of the input sequence at epoch i as the rank of the\ntoken x i at epoch i, r i := \u03b3 i (x i ). The sequence {r i } [N] i=1 [T] [is compressed by a lossless compression algorithm (such as zlib) to]\nproduce N b bits which are the final bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors q i \u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 defined as\n\n\u03c1 := [N] [b] bits/character.\n\nN c\n\n\n\nLosseless\n\nCompression\n\n\n\nTokens . . ., x i\u2212M, . . ., x i\u22121\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n\nInput sentence s with N c characters\n\n\n\ni i i\u22121 Rank\n\nLLM\ncomputation\n\n\n\nq i = Pr(X i = j|x [i] i [\u2212] \u2212 [M] 1 [)] Rank r i Losseless N b bits\n\n\n\nFig. 4. Schematic showing the prediction at epoch i.\n\n\nA. Entropy bounds\n\n\nLet S \u2208S [\u221e] be a random process that represents language input. The nth character in the sequence is denoted by S n,\nwhereas the string of characters from the beginning to the nth character is expressed as S n . The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X 1, X 2, . . . using a variable-length mapping. In this sequence, X i is the ith\ntoken. The number of characters employed to generate X i depends on the realization of the random process and, as such,\nwe introduce random variable B i to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which B i \u2265 1 and B i is uniformly bounded, with B i < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded N T \u2192\u221e, the number of characters must also approach infinity N c \u2192\u221e. Formally, consider the tokenizer\nfunction T : S [N] \u2192X [N] operating on infinite symbol sequences; that is, T (s) = x where s is an infinite sequence in S [\u221e] . For\nnatural number, i \u2208 N, define m i : S [N] \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Specifically, suppose s is given, then\n\n\nm i (s) = min n [{][length (][T][ (][s] [n] [))][ \u2265] [i][}][ .] (1)\n\n\nWe note that, by construction, lim n\u2192\u221e length (T (s n )) = \u221e and, as such, m i (\u00b7) is well-defined. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple finite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (s n ) = T (s n+1 ). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (finite) series of input symbols in the restricted\nset for which T (y n ) = x i is s m i (s) . Given a fixed sequence s, we can express the number of characters contained in a token\n\nas\nb i = m i (s) \u2212 m i\u22121 (s)\n\n\n4\n\n\nwith initial condition m \u22121 = 0. Consequently, the number of characters embedded in the first N T tokens for a random input\nbecomes N c = [\ufffd] i [N] =1 [T] [B] [i] [.]\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {S k } [\u221e] k=1 [,]\n{B i } [\u221e] i=1 [, and][ {][X] [i] [}] [\u221e] i=1 [are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that]\n\n\n\n\u2212 [1]\n\n\n\n\n[1]\n\nn [log] [2] [ p] [S] [n] [(][S] [1] [, . . ., S] [n] [) =][ \u2212] n [1]\n\n\n\nn [log] [2] [ p] [S] [n] [(][S] [n] [)][ \u2192] [H][(][S][)] almost surely. (2)\n\n\n\nLet \u2126 S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\n\n\n\n[1]\n\ni [log] [2] [ p] [X] [i] [(][X] [1] [, . . ., X] [i] [) =][ \u2212] [1] i\n\n\n\n\u2212 [1]\n\n\n\ni [log] [2] [ p] [X] [i] [(][X] [i] [)][ \u2192] [H][(][X][)] almost surely. (3)\n\n\n\nDefine \u2126 X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\n\n\nm i (S)\nlim = E [B] almost surely. (4)\ni\u2192\u221e i\n\n\nSet \u2126 B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126 S \u2229 \u2126 X \u2229 \u2126 B, we deduce that\n\n\nH(S) = lim\nk\u2192\u221e [\u2212] k [1] [log] [2] [ p] [S] [k] [(][S] [k] [(][\u03c9][))]\n\n= lim i\u2192\u221e [\u2212] l [1] i log 2 p S li (S l i (\u03c9))\n\n\n= lim log 2 Pr (X i = T (S l i (\u03c9)))\ni\u2192\u221e [\u2212] l [1] i\n\n\n\n1\n\ni [log] [2] [ Pr (][X] [i] [ =][ x] [i] [) =][ H] E [(] B [X][)]\n\n\n\n1 1\n\n= \u2212\nE[B] i [lim] \u2192\u221e i\n\n\n\nE[B] [.]\n\n\n\nThe first equality follows from (2). The second equality is a consequence of the fact that {l i = m i (S(\u03c9))|i \u2208 N} is an infinite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\n\ntwo events,\n{\u03c9 \u2208 \u2126|X i (\u03c9) = x i } = {\u03c9 \u2208 \u2126|T (S m i (S(\u03c9)) ) = x i }.\n\n\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126 B . The sets \u2126 S, \u2126 X, and \u2126 B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\n\nH(S) = [H][(][X][)] almost surely.\n\nE[B]\n\n\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\n\n\n\nPr\n\n\ufffd\n\n\n\n\ufffd\n\n\n\nH(X) \u2264 lim\nN T \u2192\u221e [\u2212] N [1] T\n\n\n\nN T\n\ufffd\n\n\n\n\ufffd log 2 q i (X i )\n\n\ni=1\n\n\n\n= 1, (5)\n\n\n\nwhere q i (\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\n\n\n\n\n[1] N T\n\nN T \ufffd i=1 [log] 2 [q] [i] [(][X] [i] [)]\n\n\n\nH(S) \u2264 lim N T \u2192\u221e \u2212 N [1]\n\n\n\n. (6)\nE[B]\n\n\n\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by H ub . The\nnumerator in (6) represents the average number of bits required to represent the tokens X N T and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\n\n\nB. Encoding schemes\n\n\nWe consider three schemes for the lossless compression block in Fig. 3.\n\n\n5\n\n\n1) Compressing the ranks using zlib: The first scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1 LLaMA+zlib .\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token x i at epoch i by using a prefix-free code assuming q i to be the true distribution of\nthe tokens. A natural choice for a prefix-free code is a Huffman code. Instead, for simplicity, we use a prefix-free code where\n1\nthe codeword for the token x i is of length l i = \u2308log 2 q i (x i ) [\u2309][. A prefix-free code with this length for][ x] [i] [ is guaranteed to exist]\nsince this choice of lengths satisfies the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1 LLaMA+TbyT,\nis given by\n\n\n\nN T\n\ufffd\n\n\ni=1\n\n\n\n1\nlog 2\n\ufffd q i (x i )\n\n\n\n\ufffd\n\n\n\n\u03c1 LLaMA+TbyT =\n\n\n\n\ufffd Ni=1 T [b] [i] .\n\n\n\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use q i (x i ) as the probability of token x i at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1 LLM+AC . It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\n\n\n\u03c1 LLM+AC \u2248\n\n\n\nN T 1\n\ufffd i=1 log 2 q i (x i )\n\n\ufffd Ni=1 T [b] [i] . (7)\n\n\n\nClearly, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and \u03c1 LLM+AC also provide upper bounds on H(S). H ub, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and\n\u03c1 LLM+AC are estimated using a finite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\n\n\nIV. R ESULTS\n\n\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it difficult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\n[Nevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of](http://mattmahoney.net/dc/text8.zip)\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M ) is shown in Table II. As expected, the\ncompression performance improves with increasing M . We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n\n\n6\n\n\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\n\n\nV. A CKNOWLEDGEMENT\n\n\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\n\n\nTABLE I\n\nR ESULTS FOR 1MB OF TEXT FROM TEXT 8 DATASET\n\n\n\nBatch\nN c N T\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC ZPAQ pq8h\n\n\n\nNo. (bpc) file size (bits) (bpc) (bpc) (bpc) (bpc)\n\n\n1 466, 650 100, 000 0.6882 1.0513 0.8215 0.689\n\n\n2 461, 477 100, 000 0.6893 1.0558 0.8242 0.6901\n\n\n3 454, 599 100, 000 0.699 1.0681 0.8357 0.6999\n\n\n4 462, 755 100, 000 0.6748 1.0346 0.8093 0.6757\n\n\n5 453, 847 100, 000 0.7481 1.1265 0.8831 0.749\n\n\n6 458, 252 100, 000 0.7218 1.0957 0.8567 0.7227\n\n\n7 451, 036 100, 000 0.6959 1.0729 0.8353 0.6968\n\n\n8 447, 953 100, 000 0.7092 1.0896 0.8489 0.7101\n\n\n9 462, 665 100, 000 0.7394 1.1126 0.8713 0.7402\n\n\n10 449, 621 100, 000 0.7269 1.1046 0.8643 0.7277\n\n\nTotal 9, 137, 710 2, 000, 000 0.7093 1.0812 0.845 0.7101 1.4 [1] 1.2 [2]\n\n\nTABLE II\n\nC OMPRESSION PERFORMANCE OF THE LLM ON THE TEXT 8 DATASET, AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\n\n\n(bpc) file size (bits) (bpc) (bpc)\n\n\n\n31 4, 568, 855 1, 000, 000 0.9139 1.3159 1.0425 0.9145\n\n\n127 4, 568, 855 1, 000, 000 0.7511 1.1303 0.8847 0.752\n\n\n255 4, 568, 855 1, 000, 000 0.7242 1.0985 0.859 0.725\n\n\n511 4, 568, 855 1, 000, 000 0.7093 1.0812 0.845 0.7101\n\n\n1 This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2 This result is taken from [12] and it corresponds to the full 100MB dataset text8\n\n\n7\n\n\n\nTABLE III\n\nM EAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\nM\n\n(bpc) (bpc) (bpc) (bpc)\n\n\n31 0.9139 \u00b1 0.0263 1.3159 \u00b1 0.0329 1.0425 \u00b1 0.0262 0.9145 \u00b1 0.0263\n\n\n127 0.7511 \u00b1 0.0233 1.1303 \u00b1 0.0292 0.8847 \u00b1 0.0231 0.752 \u00b1 0.0233\n\n\n255 0.7242 \u00b1 0.0234 1.0985 \u00b1 0.0289 0.859 \u00b1 0.0232 0.725 \u00b1 0.0234\n\n\n511 0.7093 \u00b1 0.0228 1.0812 \u00b1 0.028 0.845 \u00b1 0.0226 0.7101 \u00b1 0.0228\n\n\nTABLE IV\n\nC OMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN P ROJECT G UTENBERG [7], AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC Standalone Zlib\n\n\n(bpc) (bpc) (bpc) (bpc) (bpc)\n\n\n\n31 508, 463 115, 000 1.0919 1.5316 1.2152 1.0924 2.80\n\n\n127 508, 463 115, 000 0.8973 1.3128 1.0235 0.8982 2.80\n\n\n255 508, 463 115, 000 0.8618 1.2684 0.9899 0.8627 2.80\n\n\n511 508, 463 115, 000 0.8417 1.2465 0.9711 0.8426 2.80\n\n\n8\n\n\nR EFERENCES\n\n\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR filters),\u201d 2023.\n\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and efficient foundation language models,\u201d\n2023.\n\n[7] J. Frank Dobie, Legends of Texas, United States, Texas Folk-Lore Society, 1924; Project Gutenberg, May 25, 2023, 2023,\n[https://www.gutenberg.org/ebooks/70859.](https://www.gutenberg.org/ebooks/70859)\n\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n\n\n"}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mThought: The paper has been downloaded and its content is available. Now I need to analyze the document structure to identify the main sections that follow the Abstract and Introduction. Then, I will extract relevant information from those sections to create the synopsis.
Based on the provided content, the sections are:
- II. INTUITIVE EXPLANATION OF THE MAIN IDEA
- III. COMPRESSION USING LLMS
  - A. Entropy bounds
  - B. Encoding schemes
    - 1) Compressing the ranks using zlib
    - 2) Token-by-Token Compression
    - 3) Arithmetic Coding
- IV. RESULTS
- V. ACKNOWLEDGEMENT
- REFERENCES
I will now read the paper to extract the content from these sections.[00m
[95m## Using tool:[00m [92mread_paper[00m
[95m## Tool Input:[00m [92m
"{\"paper_id\": \"2306.04050\"}"[00m
[95m## Tool Output:[00m [92m
{"status": "success", "paper_id": "2306.04050", "content": "1\n\n# LLMZip: Lossless Text Compression using Large Language Models\n\n\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\n\n\nAbstract\n\n\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\n\n\nI. I NTRODUCTION\n\n\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\n\n\nII. I NTUITIVE EXPLANATION OF THE MAIN IDEA\n\n\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\n\nMy first attempt at writing a book\n\n\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032, \u2032 a \u2032, \u2032 book \u2032\n\n\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n\n\n2\n\n\nFigure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be\nwords such as [\u2032] reading [\u2032], [\u2032] writing [\u2032], [\u2032] driving [\u2032], [\u2032] cooking [\u2032] etc. The main idea is to compute the rank of the actual word in\nour sentence ( [\u2032] writing [\u2032] ) in this list and call it R 5 . We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for [\u2032] writing [\u2032] is R 5 = 1.\n\n\nTokens\n\n\n\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x5|x4 1)<br>5|Rank<br>computation|R5 = 1|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nreading 0.3\nwriting 0.2\ncycling 0.1\ndriving 0.05\n\n... ...\n\n\n\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\n\n\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, [\u2032] a [\u2032], and hence, the rank R 6 would be 0.\n\n\nTokens\n\n\n\n\n\n\u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x6|x5 2)<br>6|Rank<br>computation|R6 = 0|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nan 0.2\n\nthe 0.05\n\n... ...\n\n\n\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\n\n\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n\n\n1, 0, 0, . . .\n\n\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\n\n\nSequence of ranks\n\n|e of ranks|Col2|Col3|\n|---|---|---|\n|. ., rNT|Compression (zip)|Nb|\n||||\n\n\n\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\n\n\nWhen we wish to reconstruct the sequence, we first decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank R i during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\n\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n\n\n3\n\n\nIII. C OMPRESSION USING LLM S\n\n\nLet s denote a sentence from the English language composed of N c letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We first parse s into a sequence of N T tokens\ndenoted by x = x 1, x 2, . . ., x i\u22121, x i, x i+1, . . . x N T, where x i \u2208X . There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. x i \u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter X i .\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens x i\u2212M, x i\u2212M+1, . . ., x i\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nq i (x i ) := Pr(X i = x i |x i\u22121, x i\u22122, . . ., x i\u2212M ), \u2200x i \u2208X . The PMF vector q i := [q i (1), q i (2), . . ., q i (D)] [T] is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcq i . Let \u03b3 i : X \u2192X be a permutation on the integers\nfrom 1 to D such that\n\n\u02dc\nq i (\u03b3 i (j)) = q i (j), \u2200j \u2208X .\n\n\nThat is, \u03b3 i (j) is the rank of the token j at epoch i. We define the rank of the input sequence at epoch i as the rank of the\ntoken x i at epoch i, r i := \u03b3 i (x i ). The sequence {r i } [N] i=1 [T] [is compressed by a lossless compression algorithm (such as zlib) to]\nproduce N b bits which are the final bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors q i \u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 defined as\n\n\u03c1 := [N] [b] bits/character.\n\nN c\n\n\n\nLosseless\n\nCompression\n\n\n\nTokens . . ., x i\u2212M, . . ., x i\u22121\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n\nInput sentence s with N c characters\n\n\n\ni i i\u22121 Rank\n\nLLM\ncomputation\n\n\n\nq i = Pr(X i = j|x [i] i [\u2212] \u2212 [M] 1 [)] Rank r i Losseless N b bits\n\n\n\nFig. 4. Schematic showing the prediction at epoch i.\n\n\nA. Entropy bounds\n\n\nLet S \u2208S [\u221e] be a random process that represents language input. The nth character in the sequence is denoted by S n,\nwhereas the string of characters from the beginning to the nth character is expressed as S n . The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X 1, X 2, . . . using a variable-length mapping. In this sequence, X i is the ith\ntoken. The number of characters employed to generate X i depends on the realization of the random process and, as such,\nwe introduce random variable B i to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which B i \u2265 1 and B i is uniformly bounded, with B i < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded N T \u2192\u221e, the number of characters must also approach infinity N c \u2192\u221e. Formally, consider the tokenizer\nfunction T : S [N] \u2192X [N] operating on infinite symbol sequences; that is, T (s) = x where s is an infinite sequence in S [\u221e] . For\nnatural number, i \u2208 N, define m i : S [N] \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Specifically, suppose s is given, then\n\n\nm i (s) = min n [{][length (][T][ (][s] [n] [))][ \u2265] [i][}][ .] (1)\n\n\nWe note that, by construction, lim n\u2192\u221e length (T (s n )) = \u221e and, as such, m i (\u00b7) is well-defined. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple finite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (s n ) = T (s n+1 ). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (finite) series of input symbols in the restricted\nset for which T (y n ) = x i is s m i (s) . Given a fixed sequence s, we can express the number of characters contained in a token\n\nas\nb i = m i (s) \u2212 m i\u22121 (s)\n\n\n4\n\n\nwith initial condition m \u22121 = 0. Consequently, the number of characters embedded in the first N T tokens for a random input\nbecomes N c = [\ufffd] i [N] =1 [T] [B] [i] [.]\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {S k } [\u221e] k=1 [,]\n{B i } [\u221e] i=1 [, and][ {][X] [i] [}] [\u221e] i=1 [are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that]\n\n\n\n\u2212 [1]\n\n\n\n\n[1]\n\nn [log] [2] [ p] [S] [n] [(][S] [1] [, . . ., S] [n] [) =][ \u2212] n [1]\n\n\n\nn [log] [2] [ p] [S] [n] [(][S] [n] [)][ \u2192] [H][(][S][)] almost surely. (2)\n\n\n\nLet \u2126 S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\n\n\n\n[1]\n\ni [log] [2] [ p] [X] [i] [(][X] [1] [, . . ., X] [i] [) =][ \u2212] [1] i\n\n\n\n\u2212 [1]\n\n\n\ni [log] [2] [ p] [X] [i] [(][X] [i] [)][ \u2192] [H][(][X][)] almost surely. (3)\n\n\n\nDefine \u2126 X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\n\n\nm i (S)\nlim = E [B] almost surely. (4)\ni\u2192\u221e i\n\n\nSet \u2126 B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126 S \u2229 \u2126 X \u2229 \u2126 B, we deduce that\n\n\nH(S) = lim\nk\u2192\u221e [\u2212] k [1] [log] [2] [ p] [S] [k] [(][S] [k] [(][\u03c9][))]\n\n= lim i\u2192\u221e [\u2212] l [1] i log 2 p S li (S l i (\u03c9))\n\n\n= lim log 2 Pr (X i = T (S l i (\u03c9)))\ni\u2192\u221e [\u2212] l [1] i\n\n\n\n1\n\ni [log] [2] [ Pr (][X] [i] [ =][ x] [i] [) =][ H] E [(] B [X][)]\n\n\n\n1 1\n\n= \u2212\nE[B] i [lim] \u2192\u221e i\n\n\n\nE[B] [.]\n\n\n\nThe first equality follows from (2). The second equality is a consequence of the fact that {l i = m i (S(\u03c9))|i \u2208 N} is an infinite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\n\ntwo events,\n{\u03c9 \u2208 \u2126|X i (\u03c9) = x i } = {\u03c9 \u2208 \u2126|T (S m i (S(\u03c9)) ) = x i }.\n\n\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126 B . The sets \u2126 S, \u2126 X, and \u2126 B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\n\nH(S) = [H][(][X][)] almost surely.\n\nE[B]\n\n\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\n\n\n\nPr\n\n\ufffd\n\n\n\n\ufffd\n\n\n\nH(X) \u2264 lim\nN T \u2192\u221e [\u2212] N [1] T\n\n\n\nN T\n\ufffd\n\n\n\n\ufffd log 2 q i (X i )\n\n\ni=1\n\n\n\n= 1, (5)\n\n\n\nwhere q i (\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\n\n\n\n\n[1] N T\n\nN T \ufffd i=1 [log] 2 [q] [i] [(][X] [i] [)]\n\n\n\nH(S) \u2264 lim N T \u2192\u221e \u2212 N [1]\n\n\n\n. (6)\nE[B]\n\n\n\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by H ub . The\nnumerator in (6) represents the average number of bits required to represent the tokens X N T and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\n\n\nB. Encoding schemes\n\n\nWe consider three schemes for the lossless compression block in Fig. 3.\n\n\n5\n\n\n1) Compressing the ranks using zlib: The first scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1 LLaMA+zlib .\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token x i at epoch i by using a prefix-free code assuming q i to be the true distribution of\nthe tokens. A natural choice for a prefix-free code is a Huffman code. Instead, for simplicity, we use a prefix-free code where\n1\nthe codeword for the token x i is of length l i = \u2308log 2 q i (x i ) [\u2309][. A prefix-free code with this length for][ x] [i] [ is guaranteed to exist]\nsince this choice of lengths satisfies the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1 LLaMA+TbyT,\nis given by\n\n\n\nN T\n\ufffd\n\n\ni=1\n\n\n\n1\nlog 2\n\ufffd q i (x i )\n\n\n\n\ufffd\n\n\n\n\u03c1 LLaMA+TbyT =\n\n\n\n\ufffd Ni=1 T [b] [i] .\n\n\n\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use q i (x i ) as the probability of token x i at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1 LLM+AC . It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\n\n\n\u03c1 LLM+AC \u2248\n\n\n\nN T 1\n\ufffd i=1 log 2 q i (x i )\n\n\ufffd Ni=1 T [b] [i] . (7)\n\n\n\nClearly, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and \u03c1 LLM+AC also provide upper bounds on H(S). H ub, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and\n\u03c1 LLM+AC are estimated using a finite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\n\n\nIV. R ESULTS\n\n\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it difficult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\n[Nevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of](http://mattmahoney.net/dc/text8.zip)\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M ) is shown in Table II. As expected, the\ncompression performance improves with increasing M . We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n\n\n6\n\n\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\n\n\nV. A CKNOWLEDGEMENT\n\n\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\n\n\nTABLE I\n\nR ESULTS FOR 1MB OF TEXT FROM TEXT 8 DATASET\n\n\n\nBatch\nN c N T\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC ZPAQ pq8h\n\n\n\nNo. (bpc) file size (bits) (bpc) (bpc) (bpc) (bpc)\n\n\n1 466, 650 100, 000 0.6882 1.0513 0.8215 0.689\n\n\n2 461, 477 100, 000 0.6893 1.0558 0.8242 0.6901\n\n\n3 454, 599 100, 000 0.699 1.0681 0.8357 0.6999\n\n\n4 462, 755 100, 000 0.6748 1.0346 0.8093 0.6757\n\n\n5 453, 847 100, 000 0.7481 1.1265 0.8831 0.749\n\n\n6 458, 252 100, 000 0.7218 1.0957 0.8567 0.7227\n\n\n7 451, 036 100, 000 0.6959 1.0729 0.8353 0.6968\n\n\n8 447, 953 100, 000 0.7092 1.0896 0.8489 0.7101\n\n\n9 462, 665 100, 000 0.7394 1.1126 0.8713 0.7402\n\n\n10 449, 621 100, 000 0.7269 1.1046 0.8643 0.7277\n\n\nTotal 9, 137, 710 2, 000, 000 0.7093 1.0812 0.845 0.7101 1.4 [1] 1.2 [2]\n\n\nTABLE II\n\nC OMPRESSION PERFORMANCE OF THE LLM ON THE TEXT 8 DATASET, AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\n\n\n(bpc) file size (bits) (bpc) (bpc)\n\n\n\n31 4, 568, 855 1, 000, 000 0.9139 1.3159 1.0425 0.9145\n\n\n127 4, 568, 855 1, 000, 000 0.7511 1.1303 0.8847 0.752\n\n\n255 4, 568, 855 1, 000, 000 0.7242 1.0985 0.859 0.725\n\n\n511 4, 568, 855 1, 000, 000 0.7093 1.0812 0.845 0.7101\n\n\n1 This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2 This result is taken from [12] and it corresponds to the full 100MB dataset text8\n\n\n7\n\n\n\nTABLE III\n\nM EAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\nM\n\n(bpc) (bpc) (bpc) (bpc)\n\n\n31 0.9139 \u00b1 0.0263 1.3159 \u00b1 0.0329 1.0425 \u00b1 0.0262 0.9145 \u00b1 0.0263\n\n\n127 0.7511 \u00b1 0.0233 1.1303 \u00b1 0.0292 0.8847 \u00b1 0.0231 0.752 \u00b1 0.0233\n\n\n255 0.7242 \u00b1 0.0234 1.0985 \u00b1 0.0289 0.859 \u00b1 0.0232 0.725 \u00b1 0.0234\n\n\n511 0.7093 \u00b1 0.0228 1.0812 \u00b1 0.028 0.845 \u00b1 0.0226 0.7101 \u00b1 0.0228\n\n\nTABLE IV\n\nC OMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN P ROJECT G UTENBERG [7], AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC Standalone Zlib\n\n\n(bpc) (bpc) (bpc) (bpc) (bpc)\n\n\n\n31 508, 463 115, 000 1.0919 1.5316 1.2152 1.0924 2.80\n\n\n127 508, 463 115, 000 0.8973 1.3128 1.0235 0.8982 2.80\n\n\n255 508, 463 115, 000 0.8618 1.2684 0.9899 0.8627 2.80\n\n\n511 508, 463 115, 000 0.8417 1.2465 0.9711 0.8426 2.80\n\n\n8\n\n\nR EFERENCES\n\n\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR filters),\u201d 2023.\n\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and efficient foundation language models,\u201d\n2023.\n\n[7] J. Frank Dobie, Legends of Texas, United States, Texas Folk-Lore Society, 1924; Project Gutenberg, May 25, 2023, 2023,\n[https://www.gutenberg.org/ebooks/70859.](https://www.gutenberg.org/ebooks/70859)\n\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n\n\n"}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Final Answer:[00m [92m

## Synopsis of LLMZip: Lossless Text Compression using Large Language Models

This document details LLMZip, a lossless text compression algorithm leveraging Large Language Models (LLMs) for improved prediction and compression.

**Core Idea:** The method, inspired by Shannon's work, uses an LLM (specifically LLaMA-7B) to predict the next token in a sequence based on past tokens. The LLM outputs a rank-ordered list of possible next tokens and their probabilities. The rank of the actual next token in this list (or its probability) is then compressed using a standard lossless compression algorithm. For reconstruction, the same LLM is used to generate the rank-ordered list, and the decompressed rank (or probability) is used to select the correct token.

**Compression Methodology:**
The input sentence `s` (N_c characters) is tokenized into a sequence `x = x_1, ..., x_N_T` (N_T tokens). A language model with memory `M` predicts the probability mass function `q_i(x_i) := Pr(X_i = x_i | x_{i-1}, ..., x_{i-M})` for the `i`-th token. The rank `r_i` is defined as the position of `x_i` in the sorted `q_i` vector. The compression ratio `ρ` is defined as `N_b / N_c` bits/character, where `N_b` is the total bits after compression.

**Entropy Bounds:**
The paper establishes a relationship between the entropy of the character sequence `H(S)` and the entropy of the token sequence `H(X)`: `H(S) = H(X) / E[B]`, where `E[B]` is the average number of characters per token. An asymptotic upper bound on the entropy rate `H(S)` (denoted `H_ub`) is given by:
`H(S) ≤ lim_{N_T→∞} (-1/N_T * Σ_{i=1}^{N_T} log_2 q_i(X_i)) / E[B]` (Equation 6).
This `H_ub` is considered equivalent to "bits per character" (bpc) used in other literature.

**Encoding Schemes:**
Three lossless compression schemes are explored for the ranks or probabilities:
1.  **LLaMA+zlib:** Compresses the sequence of ranks `r_i` using the zlib algorithm.
2.  **LLaMA+TbyT (Token-by-Token Compression):** Uses a prefix-free code (e.g., Huffman-like) where the codeword length for token `x_i` is `l_i = ceil(log_2(1/q_i(x_i)))`. The compression ratio `ρ_LLaMA+TbyT` is `(Σ_{i=1}^{N_T} l_i) / (Σ_{i=1}^{N_T} b_i)`.
3.  **LLaMA+AC (Arithmetic Coding):** Employs arithmetic coding, which is well-suited for time-varying probabilities, directly using `q_i(x_i)` as the probability of token `x_i`. This scheme is expected to be nearly optimal, with `ρ_LLM+AC ≈ (Σ_{i=1}^{N_T} log_2(1/q_i(x_i))) / (Σ_{i=1}^{N_T} b_i)` (Equation 7).

**Results:**
*   **LLM and Tokenizer:** LLaMA-7B [6] and SentencePiece tokenizer [11] (dictionary size 32000) were used.
*   **Datasets:**
    *   1MB section of the `text8` dataset (derived from Wikipedia).
    *   100KB section of a recently published book from Project Gutenberg [7].
*   **Baselines:** ZPAQ (1.4 bpc on text8 [5]) and paq8h (1.2 bpc on text8 [12]).
*   **Performance on text8 (1MB):**
    *   LLaMA-7B with Arithmetic Coding achieved a compression ratio of **0.7101 bits/character**. This significantly outperforms ZPAQ and paq8h.
    *   The estimated asymptotic upper bound on entropy (`H_ub`) was **0.709 bits/character**, which is lower than previous estimates (e.g., Shannon [3], Cover and King [1], neural network-based estimates [2]).
    *   LLaMA+zlib and LLaMA+TbyT also showed better performance than baselines.
*   **Impact of LLM Memory (M):** Compression performance improves with increasing memory `M`. Inference time scales approximately linearly with `M`.
*   **Caution on text8 results:** The LLaMA-7B model was likely trained on a corpus including Wikipedia, potentially leading to optimistic results for the `text8` dataset.
*   **Performance on Project Gutenberg book (100KB):**
    *   Compression ratios and `H_ub` were slightly higher than for `text8`.
    *   LLaMA with Arithmetic Coding achieved **0.8426 bits/character**, still outperforming state-of-the-art results for `text8`.
    *   Standalone zlib on this text yielded 2.8 bits/character, demonstrating the substantial improvement offered by LLaMA-based compressors.

**Conclusion:** The study demonstrates that using LLaMA-7B as a predictor, especially when combined with arithmetic coding, yields significantly better lossless text compression ratios and sharper estimates of the entropy of English compared to existing state-of-the-art methods and earlier entropy bounds.

**References:** The paper includes 12 references, covering foundational works in information theory (Shannon, Cover and King), data compression (arithmetic coding, zlib), and recent advancements in language models and neural network-based compression. Key references include [1] Cover and King (1978) for entropy estimation, [3] Shannon (1951) for prediction and entropy, [4] Cleary and Witten (1984) for adaptive coding, [6] Touvron et al. (2023) for LLaMA, and [7] Project Gutenberg for the book dataset.[00m


🚀 Crew: crew
├── 📋 Task: 0134a745-0117-4160-b40c-6ebe439ed84b
│   Assigned to: AI Assistant
│   Status: ✅ Completed
│   ├── ✅ Reasoning Completed
│   ├── 🔧 Used download_paper (1)
│   └── 🔧 Used read_paper (1)
├── 🔧 Using create_reasoning_plan from LLM available_function
├── 🔄 Tool Usage Started
├── 🔧 Using create_reasoning_plan from LLM available_function
└── ✅ Tool Usage Completed┌────────────────────────────── Task Completion ──────────────────────────────┐
│                                                                             │
│  Task Completed                                                             │
│  Name: 0134a745-0117-4160-b40c-6ebe439ed84b                                 │
│  Agent: AI Assistant                                                        │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

┌────────────────────────────── Crew Completion ──────────────────────────────┐
│                                                                             │
│  Crew Execution Completed                                                   │
│  Name: crew                                                                 │
│  ID: 4d6ef100-247c-4215-ba90-0c186d0cb992                                   │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

AGENT_RESULT_JSON:{"final_text": "\n## Synopsis of LLMZip: Lossless Text Compression using Large Language Models\n\nThis document details LLMZip, a lossless text compression algorithm leveraging Large Language Models (LLMs) for improved prediction and compression.\n\n**Core Idea:** The method, inspired by Shannon's work, uses an LLM (specifically LLaMA-7B) to predict the next token in a sequence based on past tokens. The LLM outputs a rank-ordered list of possible next tokens and their probabilities. The rank of the actual next token in this list (or its probability) is then compressed using a standard lossless compression algorithm. For reconstruction, the same LLM is used to generate the rank-ordered list, and the decompressed rank (or probability) is used to select the correct token.\n\n**Compression Methodology:**\nThe input sentence `s` (N_c characters) is tokenized into a sequence `x = x_1, ..., x_N_T` (N_T tokens). A language model with memory `M` predicts the probability mass function `q_i(x_i) := Pr(X_i = x_i | x_{i-1}, ..., x_{i-M})` for the `i`-th token. The rank `r_i` is defined as the position of `x_i` in the sorted `q_i` vector. The compression ratio `\u03c1` is defined as `N_b / N_c` bits/character, where `N_b` is the total bits after compression.\n\n**Entropy Bounds:**\nThe paper establishes a relationship between the entropy of the character sequence `H(S)` and the entropy of the token sequence `H(X)`: `H(S) = H(X) / E[B]`, where `E[B]` is the average number of characters per token. An asymptotic upper bound on the entropy rate `H(S)` (denoted `H_ub`) is given by:\n`H(S) \u2264 lim_{N_T\u2192\u221e} (-1/N_T * \u03a3_{i=1}^{N_T} log_2 q_i(X_i)) / E[B]` (Equation 6).\nThis `H_ub` is considered equivalent to \"bits per character\" (bpc) used in other literature.\n\n**Encoding Schemes:**\nThree lossless compression schemes are explored for the ranks or probabilities:\n1.  **LLaMA+zlib:** Compresses the sequence of ranks `r_i` using the zlib algorithm.\n2.  **LLaMA+TbyT (Token-by-Token Compression):** Uses a prefix-free code (e.g., Huffman-like) where the codeword length for token `x_i` is `l_i = ceil(log_2(1/q_i(x_i)))`. The compression ratio `\u03c1_LLaMA+TbyT` is `(\u03a3_{i=1}^{N_T} l_i) / (\u03a3_{i=1}^{N_T} b_i)`.\n3.  **LLaMA+AC (Arithmetic Coding):** Employs arithmetic coding, which is well-suited for time-varying probabilities, directly using `q_i(x_i)` as the probability of token `x_i`. This scheme is expected to be nearly optimal, with `\u03c1_LLM+AC \u2248 (\u03a3_{i=1}^{N_T} log_2(1/q_i(x_i))) / (\u03a3_{i=1}^{N_T} b_i)` (Equation 7).\n\n**Results:**\n*   **LLM and Tokenizer:** LLaMA-7B [6] and SentencePiece tokenizer [11] (dictionary size 32000) were used.\n*   **Datasets:**\n    *   1MB section of the `text8` dataset (derived from Wikipedia).\n    *   100KB section of a recently published book from Project Gutenberg [7].\n*   **Baselines:** ZPAQ (1.4 bpc on text8 [5]) and paq8h (1.2 bpc on text8 [12]).\n*   **Performance on text8 (1MB):**\n    *   LLaMA-7B with Arithmetic Coding achieved a compression ratio of **0.7101 bits/character**. This significantly outperforms ZPAQ and paq8h.\n    *   The estimated asymptotic upper bound on entropy (`H_ub`) was **0.709 bits/character**, which is lower than previous estimates (e.g., Shannon [3], Cover and King [1], neural network-based estimates [2]).\n    *   LLaMA+zlib and LLaMA+TbyT also showed better performance than baselines.\n*   **Impact of LLM Memory (M):** Compression performance improves with increasing memory `M`. Inference time scales approximately linearly with `M`.\n*   **Caution on text8 results:** The LLaMA-7B model was likely trained on a corpus including Wikipedia, potentially leading to optimistic results for the `text8` dataset.\n*   **Performance on Project Gutenberg book (100KB):**\n    *   Compression ratios and `H_ub` were slightly higher than for `text8`.\n    *   LLaMA with Arithmetic Coding achieved **0.8426 bits/character**, still outperforming state-of-the-art results for `text8`.\n    *   Standalone zlib on this text yielded 2.8 bits/character, demonstrating the substantial improvement offered by LLaMA-based compressors.\n\n**Conclusion:** The study demonstrates that using LLaMA-7B as a predictor, especially when combined with arithmetic coding, yields significantly better lossless text compression ratios and sharper estimates of the entropy of English compared to existing state-of-the-art methods and earlier entropy bounds.\n\n**References:** The paper includes 12 references, covering foundational works in information theory (Shannon, Cover and King), data compression (arithmetic coding, zlib), and recent advancements in language models and neural network-based compression. Key references include [1] Cover and King (1978) for entropy estimation, [3] Shannon (1951) for prediction and entropy, [4] Cleary and Witten (1984) for adaptive coding, [6] Touvron et al. (2023) for LLaMA, and [7] Project Gutenberg for the book dataset.", "duration_ms": 28556}

## Synopsis of LLMZip: Lossless Text Compression using Large Language Models

This document details LLMZip, a lossless text compression algorithm leveraging Large Language Models (LLMs) for improved prediction and compression.

**Core Idea:** The method, inspired by Shannon's work, uses an LLM (specifically LLaMA-7B) to predict the next token in a sequence based on past tokens. The LLM outputs a rank-ordered list of possible next tokens and their probabilities. The rank of the actual next token in this list (or its probability) is then compressed using a standard lossless compression algorithm. For reconstruction, the same LLM is used to generate the rank-ordered list, and the decompressed rank (or probability) is used to select the correct token.

**Compression Methodology:**
The input sentence `s` (N_c characters) is tokenized into a sequence `x = x_1, ..., x_N_T` (N_T tokens). A language model with memory `M` predicts the probability mass function `q_i(x_i) := Pr(X_i = x_i | x_{i-1}, ..., x_{i-M})` for the `i`-th token. The rank `r_i` is defined as the position of `x_i` in the sorted `q_i` vector. The compression ratio `ρ` is defined as `N_b / N_c` bits/character, where `N_b` is the total bits after compression.

**Entropy Bounds:**
The paper establishes a relationship between the entropy of the character sequence `H(S)` and the entropy of the token sequence `H(X)`: `H(S) = H(X) / E[B]`, where `E[B]` is the average number of characters per token. An asymptotic upper bound on the entropy rate `H(S)` (denoted `H_ub`) is given by:
`H(S) ≤ lim_{N_T→∞} (-1/N_T * Σ_{i=1}^{N_T} log_2 q_i(X_i)) / E[B]` (Equation 6).
This `H_ub` is considered equivalent to "bits per character" (bpc) used in other literature.

**Encoding Schemes:**
Three lossless compression schemes are explored for the ranks or probabilities:
1.  **LLaMA+zlib:** Compresses the sequence of ranks `r_i` using the zlib algorithm.
2.  **LLaMA+TbyT (Token-by-Token Compression):** Uses a prefix-free code (e.g., Huffman-like) where the codeword length for token `x_i` is `l_i = ceil(log_2(1/q_i(x_i)))`. The compression ratio `ρ_LLaMA+TbyT` is `(Σ_{i=1}^{N_T} l_i) / (Σ_{i=1}^{N_T} b_i)`.
3.  **LLaMA+AC (Arithmetic Coding):** Employs arithmetic coding, which is well-suited for time-varying probabilities, directly using `q_i(x_i)` as the probability of token `x_i`. This scheme is expected to be nearly optimal, with `ρ_LLM+AC ≈ (Σ_{i=1}^{N_T} log_2(1/q_i(x_i))) / (Σ_{i=1}^{N_T} b_i)` (Equation 7).

**Results:**
*   **LLM and Tokenizer:** LLaMA-7B [6] and SentencePiece tokenizer [11] (dictionary size 32000) were used.
*   **Datasets:**
    *   1MB section of the `text8` dataset (derived from Wikipedia).
    *   100KB section of a recently published book from Project Gutenberg [7].
*   **Baselines:** ZPAQ (1.4 bpc on text8 [5]) and paq8h (1.2 bpc on text8 [12]).
*   **Performance on text8 (1MB):**
    *   LLaMA-7B with Arithmetic Coding achieved a compression ratio of **0.7101 bits/character**. This significantly outperforms ZPAQ and paq8h.
    *   The estimated asymptotic upper bound on entropy (`H_ub`) was **0.709 bits/character**, which is lower than previous estimates (e.g., Shannon [3], Cover and King [1], neural network-based estimates [2]).
    *   LLaMA+zlib and LLaMA+TbyT also showed better performance than baselines.
*   **Impact of LLM Memory (M):** Compression performance improves with increasing memory `M`. Inference time scales approximately linearly with `M`.
*   **Caution on text8 results:** The LLaMA-7B model was likely trained on a corpus including Wikipedia, potentially leading to optimistic results for the `text8` dataset.
*   **Performance on Project Gutenberg book (100KB):**
    *   Compression ratios and `H_ub` were slightly higher than for `text8`.
    *   LLaMA with Arithmetic Coding achieved **0.8426 bits/character**, still outperforming state-of-the-art results for `text8`.
    *   Standalone zlib on this text yielded 2.8 bits/character, demonstrating the substantial improvement offered by LLaMA-based compressors.

**Conclusion:** The study demonstrates that using LLaMA-7B as a predictor, especially when combined with arithmetic coding, yields significantly better lossless text compression ratios and sharper estimates of the entropy of English compared to existing state-of-the-art methods and earlier entropy bounds.

**References:** The paper includes 12 references, covering foundational works in information theory (Shannon, Cover and King), data compression (arithmetic coding, zlib), and recent advancements in language models and neural network-based compression. Key references include [1] Cover and King (1978) for entropy estimation, [3] Shannon (1951) for prediction and entropy, [4] Cleary and Witten (1984) for adaptive coding, [6] Touvron et al. (2023) for LLaMA, and [7] Project Gutenberg for the book dataset.
C:\tools\Anaconda3\Lib\site-packages\paramiko\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "cipher": algorithms.TripleDES,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.
  "class": algorithms.Blowfish,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "class": algorithms.TripleDES,
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\_internal\_generate_schema.py:628: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.
  warn(
- Reading configuration...
  - Reading registry from registry.yaml
  - Reading catalog from docker-mcp.yaml
  - Reading config from config.yaml
- Configuration read in 19.0575ms
- Watching registry at C:\Users\Noah\.docker\mcp\registry.yaml
- Watching config at C:\Users\Noah\.docker\mcp\config.yaml
- Those servers are enabled: arxiv-mcp-server, duckduckgo, wikipedia-mcp
- Using images:
  - mcp/arxiv-mcp-server@sha256:6dc6bba6dfed97f4ad6eb8d23a5c98ef5b7fa6184937d54b2d675801cd9dd29e
  - mcp/duckduckgo@sha256:68eb20db6109f5c312a695fc5ec3386ad15d93ffb765a0b4eb1baf4328dec14f
  - mcp/wikipedia-mcp@sha256:e90e3f6e3bb795f10d3d8d8ac1523f6d46d4f2ab5ac3b3a4720e1be879466472
> Images pulled in 104.0819ms
- Listing MCP tools...
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
  - Running mcp/duckduckgo with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=duckduckgo -l docker-mcp-transport=stdio]
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers:ro]
  > duckduckgo: (2 tools)
  > wikipedia-mcp: (10 tools) (9 resourceTemplates)
  > arxiv-mcp-server: (4 tools) (1 prompts)
> 16 tools listed in 3.0152332s
- Watching for configuration updates...
> Initialized in 3.1383726s
> Start stdio server
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\fields.py:1093: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'items', 'anyOf', 'enum', 'properties'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
  warn(
┌────────────────────────── Crew Execution Started ───────────────────────────┐
│                                                                             │
│  Crew Execution Started                                                     │
│  Name: crew                                                                 │
│  ID: e576a6b0-8f83-4092-98e3-4c5049c97dad                                   │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

🚀 Crew: crew
├── 📋 Task: f419d80c-228e-4c84-9deb-a66fc8d9f41e
│   Status: Executing Task...
│   └── ✅ Reasoning Completed
├── 🔧 Using create_reasoning_plan from LLM available_function
├── 🔄 Tool Usage Started
├── 🔧 Using create_reasoning_plan from LLM available_function
└── ✅ Tool Usage Completed┌───────────────────────────── 🧠 Reasoning Plan ─────────────────────────────┐
│                                                                             │
│  1. Analyze the document prefix: I will carefully read the provided title,  │
│  authors, abstract, and introduction to understand the core problem, the    │
│  proposed solution (LLMZip, using LLaMA-7B for entropy estimation and       │
│  lossless compression), and the preliminary results (outperforming BSC,     │
│  ZPAQ, paq8h). I will also note the existing references [1], [2], [3], [4]  │
│  mentioned.                                                                 │
│  2. Identify key concepts and terms: I will extract keywords such as        │
│  "LLMZip," "Lossless Text Compression," "Large Language Models,"            │
│  "LLaMA-7B," "entropy of English," "arithmetic coding," and the names of    │
│  state-of-the-art compressors (BSC, ZPAQ, paq8h).                           │
│  3. Formulate search queries: Given the arXiv ID (2306.04050v2), the most   │
│  efficient approach is to directly search for the paper.                    │
│  4. Use `search_papers` to find the full paper: I will use the query        │
│  "LLMZip: Lossless Text Compression using Large Language Models arXiv:      │
│  2306.04050" with the `search_papers` tool to locate the full document.     │
│  5. Read the full paper and extract relevant information: Once the paper    │
│  is found, I will use `read_paper` to access its content. I will then       │
│  systematically go through the sections following the introduction and      │
│  abstract, looking for:                                                     │
│      *   **Methodology:** Detailed explanation of how LLaMA-7B is used for  │
│  next-token prediction, the specific lossless compression scheme combined   │
│  with it (e.g., arithmetic coding details), and the overall LLMZip          │
│  algorithm.                                                                 │
│      *   **Experimental Setup:** Description of the datasets used for       │
│  evaluation, the specific configuration of LLaMA-7B, and the metrics for    │
│  comparison.                                                                │
│      *   **Results:** Quantitative results demonstrating the performance    │
│  of LLMZip against BSC, ZPAQ, and paq8h, potentially including tables,      │
│  graphs, and specific compression ratios or bits per character (BPC)        │
│  values.                                                                    │
│      *   **Discussion/Analysis:** Further insights into the entropy         │
│  estimates, the implications of the findings, and any limitations.          │
│      *   **Related Work:** A more comprehensive review of existing work in  │
│  text compression and language model-based compression, expanding on        │
│  references [1], [2], [3], [4].                                             │
│      *   **Equations and Definitions:** Any mathematical formulations or    │
│  formal definitions introduced in the methodology or analysis sections.     │
│      *   **References:** The complete list of references cited in the       │
│  paper.                                                                     │
│  6. Synthesize the synopsis: I will compile the extracted facts,            │
│  equations, definitions, datasets, and references into a concise textual    │
│  synopsis. The synopsis will focus on the detailed aspects of the           │
│  methodology, experimental results, and the broader implications of the     │
│  LLMZip approach, ensuring it directly addresses the likely future content  │
│  of the document. I will ensure no information is included that is not      │
│  directly supported by the retrieved content.                               │
│                                                                             │
│  Challenges and how to approach them:                                       │
│  *   **Finding the full paper:** The arXiv ID is a strong lead. If          │
│  `search_papers` fails, I will try `search` with the title and authors.     │
│  *   **Identifying "likely future content":** I will focus on the standard  │
│  structure of a research paper, expecting sections on methods,              │
│  experiments, results, and discussion to follow the introduction.           │
│  *   **Avoiding hallucination:** I will strictly adhere to the content      │
│  retrieved by the tools and will not infer or invent any information.       │
│  *   **Conciseness:** I will prioritize the most significant details        │
│  regarding the methodology, results, and key findings to keep the synopsis  │
│  brief and impactful.                                                       │
│                                                                             │
│  Strategic use of available tools:                                          │
│  *   `search_papers`: Primary tool to locate the full paper using the       │
│  title and arXiv ID.                                                        │
│  *   `read_paper`: Essential for accessing the full content of the paper    │
│  to extract detailed information from sections beyond the prefix.           │
│  *   `get_sections`: Could be used as an alternative or in conjunction      │
│  with `read_paper` to quickly navigate to specific sections if              │
│  `read_paper` returns a very long string. However, for a detailed           │
│  synopsis, reading the relevant sections will be more effective.            │
│                                                                             │
│  Expected outcome: A concise textual synopsis (plain text) detailing the    │
│  likely future content of the "LLMZip" paper, including specific            │
│  methodological details (e.g., how LLaMA-7B is integrated with a lossless   │
│  compression scheme), experimental setup (e.g., datasets, evaluation        │
│  metrics), quantitative results (e.g., compression ratios, BPC values,      │
│  comparisons to BSC, ZPAQ, paq8h), and a more comprehensive list of         │
│  references and any new definitions or equations presented. This synopsis   │
│  will be highly relevant to the document and free of hallucinated content.  │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool search_papers with arguments: {"categories":["cs.IT"],"max_results":1,"query":"LLMZip: Lossless Text Compression using Large Language Models"}
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
  > Calling tool search_papers took: 2.0803516s
  - Scanning tool call response for secrets...
  > No secret found in response.
  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool search_papers with arguments: {"categories":["cs.IT"],"max_results":1,"query":"2306.04050"}
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
  > Calling tool search_papers took: 2.431774s
  - Scanning tool call response for secrets...
  > No secret found in response.
  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool download_paper with arguments: {"check_status":false,"paper_id":"2306.04050v2"}
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
  > Calling tool download_paper took: 1.6084925s
  - Scanning tool call response for secrets...
  > No secret found in response.
[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Task:[00m [92mYou act as a deterministic research agent. Given the observed prefix of a document, use available MCP tools to retrieve information that is likely to appear later in the same document. Prefer authoritative sources. Produce a concise textual synopsis of the likely future content (facts, equations, definitions, datasets, references), suitable for conditioning a language model compressor. Maximize relevance to the document. Do not hallucinate; only include content supported by the tools.

Document prefix (UTF-8 text):
# LLMZip: Lossless Text Compression using Large Language Models

**Authors:** Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas Shakkottai  
**Affiliation:** Department of Electrical and Computer Engineering, Texas A&M University  
**Email:** {vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu  
**arXiv:** 2306.04050v2 [cs.IT] 26 Jun 2023

## Abstract

We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.

## I. INTRODUCTION

There are close connections between learning, prediction, and compression. The success of ChatGPT has captured the fascination of general public and brought the connection between learning and prediction to the fore. The main advance brought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in a paragraph based on knowing the past several words (tokens).

The connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the entropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past values can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many algorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of lossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very effective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor and every time there

Output: A concise synopsis (plain text).

Reasoning Plan:
1. Analyze the document prefix: I will carefully read the provided title, authors, abstract, and introduction to understand the core problem, the proposed solution (LLMZip, using LLaMA-7B for entropy estimation and lossless compression), and the preliminary results (outperforming BSC, ZPAQ, paq8h). I will also note the existing references [1], [2], [3], [4] mentioned.
2. Identify key concepts and terms: I will extract keywords such as "LLMZip," "Lossless Text Compression," "Large Language Models," "LLaMA-7B," "entropy of English," "arithmetic coding," and the names of state-of-the-art compressors (BSC, ZPAQ, paq8h).
3. Formulate search queries: Given the arXiv ID (2306.04050v2), the most efficient approach is to directly search for the paper.
4. Use `search_papers` to find the full paper: I will use the query "LLMZip: Lossless Text Compression using Large Language Models arXiv: 2306.04050" with the `search_papers` tool to locate the full document.
5. Read the full paper and extract relevant information: Once the paper is found, I will use `read_paper` to access its content. I will then systematically go through the sections following the introduction and abstract, looking for:
    *   **Methodology:** Detailed explanation of how LLaMA-7B is used for next-token prediction, the specific lossless compression scheme combined with it (e.g., arithmetic coding details), and the overall LLMZip algorithm.
    *   **Experimental Setup:** Description of the datasets used for evaluation, the specific configuration of LLaMA-7B, and the metrics for comparison.
    *   **Results:** Quantitative results demonstrating the performance of LLMZip against BSC, ZPAQ, and paq8h, potentially including tables, graphs, and specific compression ratios or bits per character (BPC) values.
    *   **Discussion/Analysis:** Further insights into the entropy estimates, the implications of the findings, and any limitations.
    *   **Related Work:** A more comprehensive review of existing work in text compression and language model-based compression, expanding on references [1], [2], [3], [4].
    *   **Equations and Definitions:** Any mathematical formulations or formal definitions introduced in the methodology or analysis sections.
    *   **References:** The complete list of references cited in the paper.
6. Synthesize the synopsis: I will compile the extracted facts, equations, definitions, datasets, and references into a concise textual synopsis. The synopsis will focus on the detailed aspects of the methodology, experimental results, and the broader implications of the LLMZip approach, ensuring it directly addresses the likely future content of the document. I will ensure no information is included that is not directly supported by the retrieved content.

Challenges and how to approach them:
*   **Finding the full paper:** The arXiv ID is a strong lead. If `search_papers` fails, I will try `search` with the title and authors.
*   **Identifying "likely future content":** I will focus on the standard structure of a research paper, expecting sections on methods, experiments, results, and discussion to follow the introduction.
*   **Avoiding hallucination:** I will strictly adhere to the content retrieved by the tools and will not infer or invent any information.
*   **Conciseness:** I will prioritize the most significant details regarding the methodology, results, and key findings to keep the synopsis brief and impactful.

Strategic use of available tools:
*   `search_papers`: Primary tool to locate the full paper using the title and arXiv ID.
*   `read_paper`: Essential for accessing the full content of the paper to extract detailed information from sections beyond the prefix.
*   `get_sections`: Could be used as an alternative or in conjunction with `read_paper` to quickly navigate to specific sections if `read_paper` returns a very long string. However, for a detailed synopsis, reading the relevant sections will be more effective.

Expected outcome: A concise textual synopsis (plain text) detailing the likely future content of the "LLMZip" paper, including specific methodological details (e.g., how LLaMA-7B is integrated with a lossless compression scheme), experimental setup (e.g., datasets, evaluation metrics), quantitative results (e.g., compression ratios, BPC values, comparisons to BSC, ZPAQ, paq8h), and a more comprehensive list of references and any new definitions or equations presented. This synopsis will be highly relevant to the document and free of hallucinated content.
[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mAction: search_papers[00m
[95m## Using tool:[00m [92msearch_papers[00m
[95m## Tool Input:[00m [92m
"{\"query\": \"LLMZip: Lossless Text Compression using Large Language Models\", \"categories\": [\"cs.IT\"], \"max_results\": 1}"[00m
[95m## Tool Output:[00m [92m
{
  "total_results": 1,
  "papers": [
    {
      "id": "2406.10036v1",
      "title": "Information Compression in the AI Era: Recent Advances and Future Challenges",
      "authors": [
        "Jun Chen",
        "Yong Fang",
        "Ashish Khisti",
        "Ayfer Ozgur",
        "Nir Shlezinger",
        "Chao Tian"
      ],
      "abstract": "This survey articles focuses on emerging connections between the fields of\nmachine learning and data compression. While fundamental limits of classical\n(lossy) data compression are established using rate-distortion theory, the\nconnections to machine learning have resulted in new theoretical analysis and\napplication areas. We survey recent works on task-based and goal-oriented\ncompression, the rate-distortion-perception theory and compression for\nestimation and inference. Deep learning based approaches also provide natural\ndata-driven algorithmic approaches to compression. We survey recent works on\napplying deep learning techniques to task-based or goal-oriented compression,\nas well as image and video compression. We also discuss the potential use of\nlarge language models for text compression. We finally provide some directions\nfor future research in this promising field.",
      "categories": [
        "cs.IT",
        "math.IT"
      ],
      "published": "2024-06-14T13:43:59+00:00",
      "url": "http://arxiv.org/pdf/2406.10036v1",
      "resource_uri": "arxiv://2406.10036v1"
    }
  ]
}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mThought: The initial search for "LLMZip: Lossless Text Compression using Large Language Models" with the category "cs.IT" did not return the exact paper with arXiv ID 2306.04050v2. It returned a different paper. This indicates that the search query might be too restrictive or the arXiv ID is crucial. I should try searching specifically with the arXiv ID.Action: search_papers[00m
[95m## Using tool:[00m [92msearch_papers[00m
[95m## Tool Input:[00m [92m
"{\"query\": \"2306.04050\", \"categories\": [\"cs.IT\"], \"max_results\": 1}"[00m
[95m## Tool Output:[00m [92m
{
  "total_results": 1,
  "papers": [
    {
      "id": "2306.04050v2",
      "title": "LLMZip: Lossless Text Compression using Large Language Models",
      "authors": [
        "Chandra Shekhara Kaushik Valmeekam",
        "Krishna Narayanan",
        "Dileep Kalathil",
        "Jean-Francois Chamberland",
        "Srinivas Shakkottai"
      ],
      "abstract": "We provide new estimates of an asymptotic upper bound on the entropy of\nEnglish using the large language model LLaMA-7B as a predictor for the next\ntoken given a window of past tokens. This estimate is significantly smaller\nthan currently available estimates in \\cite{cover1978convergent},\n\\cite{lutati2023focus}. A natural byproduct is an algorithm for lossless\ncompression of English text which combines the prediction from the large\nlanguage model with a lossless compression scheme. Preliminary results from\nlimited experiments suggest that our scheme outperforms state-of-the-art text\ncompression schemes such as BSC, ZPAQ, and paq8h.",
      "categories": [
        "cs.IT",
        "cs.CL",
        "cs.LG",
        "math.IT"
      ],
      "published": "2023-06-06T22:42:00+00:00",
      "url": "http://arxiv.org/pdf/2306.04050v2",
      "resource_uri": "arxiv://2306.04050v2"
    }
  ]
}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mThought: I have successfully found the paper using its arXiv ID. Now I need to download and read the paper to extract the detailed information for the synopsis.Action: download_paper[00m
[95m## Using tool:[00m [92mdownload_paper[00m
[95m## Tool Input:[00m [92m
"{\"paper_id\": \"2306.04050v2\"}"[00m
[95m## Tool Output:[00m [92m
{"status": "success", "message": "Paper already available", "resource_uri": "file:///app/papers/2306.04050v2.md", "pdf_uri": "https://arxiv.org/pdf/2306.04050v2.pdf", "content": "1\n\n# LLMZip: Lossless Text Compression using Large Language Models\n\n\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\n\n\nAbstract\n\n\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\n\n\nI. I NTRODUCTION\n\n\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\n\n\nII. I NTUITIVE EXPLANATION OF THE MAIN IDEA\n\n\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\n\nMy first attempt at writing a book\n\n\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032, \u2032 a \u2032, \u2032 book \u2032\n\n\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n\n\n2\n\n\nFigure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be\nwords such as [\u2032] reading [\u2032], [\u2032] writing [\u2032], [\u2032] driving [\u2032], [\u2032] cooking [\u2032] etc. The main idea is to compute the rank of the actual word in\nour sentence ( [\u2032] writing [\u2032] ) in this list and call it R 5 . We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for [\u2032] writing [\u2032] is R 5 = 1.\n\n\nTokens\n\n\n\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x5|x4 1)<br>5|Rank<br>computation|R5 = 1|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nreading 0.3\nwriting 0.2\ncycling 0.1\ndriving 0.05\n\n... ...\n\n\n\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\n\n\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, [\u2032] a [\u2032], and hence, the rank R 6 would be 0.\n\n\nTokens\n\n\n\n\n\n\u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x6|x5 2)<br>6|Rank<br>computation|R6 = 0|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nan 0.2\n\nthe 0.05\n\n... ...\n\n\n\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\n\n\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n\n\n1, 0, 0, . . .\n\n\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\n\n\nSequence of ranks\n\n|e of ranks|Col2|Col3|\n|---|---|---|\n|. ., rNT|Compression (zip)|Nb|\n||||\n\n\n\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\n\n\nWhen we wish to reconstruct the sequence, we first decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank R i during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\n\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n\n\n3\n\n\nIII. C OMPRESSION USING LLM S\n\n\nLet s denote a sentence from the English language composed of N c letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We first parse s into a sequence of N T tokens\ndenoted by x = x 1, x 2, . . ., x i\u22121, x i, x i+1, . . . x N T, where x i \u2208X . There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. x i \u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter X i .\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens x i\u2212M, x i\u2212M+1, . . ., x i\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nq i (x i ) := Pr(X i = x i |x i\u22121, x i\u22122, . . ., x i\u2212M ), \u2200x i \u2208X . The PMF vector q i := [q i (1), q i (2), . . ., q i (D)] [T] is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcq i . Let \u03b3 i : X \u2192X be a permutation on the integers\nfrom 1 to D such that\n\n\u02dc\nq i (\u03b3 i (j)) = q i (j), \u2200j \u2208X .\n\n\nThat is, \u03b3 i (j) is the rank of the token j at epoch i. We define the rank of the input sequence at epoch i as the rank of the\ntoken x i at epoch i, r i := \u03b3 i (x i ). The sequence {r i } [N] i=1 [T] [is compressed by a lossless compression algorithm (such as zlib) to]\nproduce N b bits which are the final bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors q i \u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 defined as\n\n\u03c1 := [N] [b] bits/character.\n\nN c\n\n\n\nLosseless\n\nCompression\n\n\n\nTokens . . ., x i\u2212M, . . ., x i\u22121\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n\nInput sentence s with N c characters\n\n\n\ni i i\u22121 Rank\n\nLLM\ncomputation\n\n\n\nq i = Pr(X i = j|x [i] i [\u2212] \u2212 [M] 1 [)] Rank r i Losseless N b bits\n\n\n\nFig. 4. Schematic showing the prediction at epoch i.\n\n\nA. Entropy bounds\n\n\nLet S \u2208S [\u221e] be a random process that represents language input. The nth character in the sequence is denoted by S n,\nwhereas the string of characters from the beginning to the nth character is expressed as S n . The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X 1, X 2, . . . using a variable-length mapping. In this sequence, X i is the ith\ntoken. The number of characters employed to generate X i depends on the realization of the random process and, as such,\nwe introduce random variable B i to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which B i \u2265 1 and B i is uniformly bounded, with B i < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded N T \u2192\u221e, the number of characters must also approach infinity N c \u2192\u221e. Formally, consider the tokenizer\nfunction T : S [N] \u2192X [N] operating on infinite symbol sequences; that is, T (s) = x where s is an infinite sequence in S [\u221e] . For\nnatural number, i \u2208 N, define m i : S [N] \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Specifically, suppose s is given, then\n\n\nm i (s) = min n [{][length (][T][ (][s] [n] [))][ \u2265] [i][}][ .] (1)\n\n\nWe note that, by construction, lim n\u2192\u221e length (T (s n )) = \u221e and, as such, m i (\u00b7) is well-defined. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple finite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (s n ) = T (s n+1 ). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (finite) series of input symbols in the restricted\nset for which T (y n ) = x i is s m i (s) . Given a fixed sequence s, we can express the number of characters contained in a token\n\nas\nb i = m i (s) \u2212 m i\u22121 (s)\n\n\n4\n\n\nwith initial condition m \u22121 = 0. Consequently, the number of characters embedded in the first N T tokens for a random input\nbecomes N c = [\ufffd] i [N] =1 [T] [B] [i] [.]\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {S k } [\u221e] k=1 [,]\n{B i } [\u221e] i=1 [, and][ {][X] [i] [}] [\u221e] i=1 [are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that]\n\n\n\n\u2212 [1]\n\n\n\n\n[1]\n\nn [log] [2] [ p] [S] [n] [(][S] [1] [, . . ., S] [n] [) =][ \u2212] n [1]\n\n\n\nn [log] [2] [ p] [S] [n] [(][S] [n] [)][ \u2192] [H][(][S][)] almost surely. (2)\n\n\n\nLet \u2126 S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\n\n\n\n[1]\n\ni [log] [2] [ p] [X] [i] [(][X] [1] [, . . ., X] [i] [) =][ \u2212] [1] i\n\n\n\n\u2212 [1]\n\n\n\ni [log] [2] [ p] [X] [i] [(][X] [i] [)][ \u2192] [H][(][X][)] almost surely. (3)\n\n\n\nDefine \u2126 X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\n\n\nm i (S)\nlim = E [B] almost surely. (4)\ni\u2192\u221e i\n\n\nSet \u2126 B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126 S \u2229 \u2126 X \u2229 \u2126 B, we deduce that\n\n\nH(S) = lim\nk\u2192\u221e [\u2212] k [1] [log] [2] [ p] [S] [k] [(][S] [k] [(][\u03c9][))]\n\n= lim i\u2192\u221e [\u2212] l [1] i log 2 p S li (S l i (\u03c9))\n\n\n= lim log 2 Pr (X i = T (S l i (\u03c9)))\ni\u2192\u221e [\u2212] l [1] i\n\n\n\n1\n\ni [log] [2] [ Pr (][X] [i] [ =][ x] [i] [) =][ H] E [(] B [X][)]\n\n\n\n1 1\n\n= \u2212\nE[B] i [lim] \u2192\u221e i\n\n\n\nE[B] [.]\n\n\n\nThe first equality follows from (2). The second equality is a consequence of the fact that {l i = m i (S(\u03c9))|i \u2208 N} is an infinite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\n\ntwo events,\n{\u03c9 \u2208 \u2126|X i (\u03c9) = x i } = {\u03c9 \u2208 \u2126|T (S m i (S(\u03c9)) ) = x i }.\n\n\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126 B . The sets \u2126 S, \u2126 X, and \u2126 B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\n\nH(S) = [H][(][X][)] almost surely.\n\nE[B]\n\n\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\n\n\n\nPr\n\n\ufffd\n\n\n\n\ufffd\n\n\n\nH(X) \u2264 lim\nN T \u2192\u221e [\u2212] N [1] T\n\n\n\nN T\n\ufffd\n\n\n\n\ufffd log 2 q i (X i )\n\n\ni=1\n\n\n\n= 1, (5)\n\n\n\nwhere q i (\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\n\n\n\n\n[1] N T\n\nN T \ufffd i=1 [log] 2 [q] [i] [(][X] [i] [)]\n\n\n\nH(S) \u2264 lim N T \u2192\u221e \u2212 N [1]\n\n\n\n. (6)\nE[B]\n\n\n\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by H ub . The\nnumerator in (6) represents the average number of bits required to represent the tokens X N T and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\n\n\nB. Encoding schemes\n\n\nWe consider three schemes for the lossless compression block in Fig. 3.\n\n\n5\n\n\n1) Compressing the ranks using zlib: The first scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1 LLaMA+zlib .\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token x i at epoch i by using a prefix-free code assuming q i to be the true distribution of\nthe tokens. A natural choice for a prefix-free code is a Huffman code. Instead, for simplicity, we use a prefix-free code where\n1\nthe codeword for the token x i is of length l i = \u2308log 2 q i (x i ) [\u2309][. A prefix-free code with this length for][ x] [i] [ is guaranteed to exist]\nsince this choice of lengths satisfies the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1 LLaMA+TbyT,\nis given by\n\n\n\nN T\n\ufffd\n\n\ni=1\n\n\n\n1\nlog 2\n\ufffd q i (x i )\n\n\n\n\ufffd\n\n\n\n\u03c1 LLaMA+TbyT =\n\n\n\n\ufffd Ni=1 T [b] [i] .\n\n\n\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use q i (x i ) as the probability of token x i at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1 LLM+AC . It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\n\n\n\u03c1 LLM+AC \u2248\n\n\n\nN T 1\n\ufffd i=1 log 2 q i (x i )\n\n\ufffd Ni=1 T [b] [i] . (7)\n\n\n\nClearly, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and \u03c1 LLM+AC also provide upper bounds on H(S). H ub, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and\n\u03c1 LLM+AC are estimated using a finite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\n\n\nIV. R ESULTS\n\n\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it difficult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\n[Nevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of](http://mattmahoney.net/dc/text8.zip)\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M ) is shown in Table II. As expected, the\ncompression performance improves with increasing M . We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n\n\n6\n\n\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\n\n\nV. A CKNOWLEDGEMENT\n\n\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\n\n\nTABLE I\n\nR ESULTS FOR 1MB OF TEXT FROM TEXT 8 DATASET\n\n\n\nBatch\nN c N T\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC ZPAQ pq8h\n\n\n\nNo. (bpc) file size (bits) (bpc) (bpc) (bpc) (bpc)\n\n\n1 466, 650 100, 000 0.6882 1.0513 0.8215 0.689\n\n\n2 461, 477 100, 000 0.6893 1.0558 0.8242 0.6901\n\n\n3 454, 599 100, 000 0.699 1.0681 0.8357 0.6999\n\n\n4 462, 755 100, 000 0.6748 1.0346 0.8093 0.6757\n\n\n5 453, 847 100, 000 0.7481 1.1265 0.8831 0.749\n\n\n6 458, 252 100, 000 0.7218 1.0957 0.8567 0.7227\n\n\n7 451, 036 100, 000 0.6959 1.0729 0.8353 0.6968\n\n\n8 447, 953 100, 000 0.7092 1.0896 0.8489 0.7101\n\n\n9 462, 665 100, 000 0.7394 1.1126 0.8713 0.7402\n\n\n10 449, 621 100, 000 0.7269 1.1046 0.8643 0.7277\n\n\nTotal 9, 137, 710 2, 000, 000 0.7093 1.0812 0.845 0.7101 1.4 [1] 1.2 [2]\n\n\nTABLE II\n\nC OMPRESSION PERFORMANCE OF THE LLM ON THE TEXT 8 DATASET, AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\n\n\n(bpc) file size (bits) (bpc) (bpc)\n\n\n\n31 4, 568, 855 1, 000, 000 0.9139 1.3159 1.0425 0.9145\n\n\n127 4, 568, 855 1, 000, 000 0.7511 1.1303 0.8847 0.752\n\n\n255 4, 568, 855 1, 000, 000 0.7242 1.0985 0.859 0.725\n\n\n511 4, 568, 855 1, 000, 000 0.7093 1.0812 0.845 0.7101\n\n\n1 This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2 This result is taken from [12] and it corresponds to the full 100MB dataset text8\n\n\n7\n\n\n\nTABLE III\n\nM EAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\nM\n\n(bpc) (bpc) (bpc) (bpc)\n\n\n31 0.9139 \u00b1 0.0263 1.3159 \u00b1 0.0329 1.0425 \u00b1 0.0262 0.9145 \u00b1 0.0263\n\n\n127 0.7511 \u00b1 0.0233 1.1303 \u00b1 0.0292 0.8847 \u00b1 0.0231 0.752 \u00b1 0.0233\n\n\n255 0.7242 \u00b1 0.0234 1.0985 \u00b1 0.0289 0.859 \u00b1 0.0232 0.725 \u00b1 0.0234\n\n\n511 0.7093 \u00b1 0.0228 1.0812 \u00b1 0.028 0.845 \u00b1 0.0226 0.7101 \u00b1 0.0228\n\n\nTABLE IV\n\nC OMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN P ROJECT G UTENBERG [7], AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC Standalone Zlib\n\n\n(bpc) (bpc) (bpc) (bpc) (bpc)\n\n\n\n31 508, 463 115, 000 1.0919 1.5316 1.2152 1.0924 2.80\n\n\n127 508, 463 115, 000 0.8973 1.3128 1.0235 0.8982 2.80\n\n\n255 508, 463 115, 000 0.8618 1.2684 0.9899 0.8627 2.80\n\n\n511 508, 463 115, 000 0.8417 1.2465 0.9711 0.8426 2.80\n\n\n8\n\n\nR EFERENCES\n\n\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR filters),\u201d 2023.\n\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and efficient foundation language models,\u201d\n2023.\n\n[7] J. Frank Dobie, Legends of Texas, United States, Texas Folk-Lore Society, 1924; Project Gutenberg, May 25, 2023, 2023,\n[https://www.gutenberg.org/ebooks/70859.](https://www.gutenberg.org/ebooks/70859)\n\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n\n\n"}


You ONLY have access to the following tools, and should NEVER make up tools that are not listed here:

Tool Name: download_paper
Tool Arguments: {'properties': {'check_status': {'anyOf': [], 'default': False, 'description': 'Set to true to only check the status of an ongoing or completed conversion without starting a new download. Use this to monitor long-running conversions or verify if a paper is already available.', 'enum': None, 'items': None, 'properties': {}, 'title': ''}, 'paper_id': {'anyOf': [], 'description': "The arXiv identifier of the paper to download (e.g., '2301.07041', '1706.03762', 'cs.AI/0301001'). This can be found in search results or arXiv URLs. The paper must exist on arXiv.", 'enum': None, 'items': None, 'properties': {}, 'title': '', 'type': 'string'}}, 'required': ['paper_id'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Download and convert an arXiv paper to readable markdown format for analysis and reading. This tool fetches the PDF from arXiv, converts it to markdown using advanced text extraction, and stores it locally for immediate access. Use this tool when you need to read, analyze, or work with the full text content of a specific paper. The conversion process extracts text, preserves formatting, and handles mathematical equations. Returns the full paper content directly upon successful completion.
Tool Name: extract_key_facts
Tool Arguments: {'properties': {'count': {'anyOf': [], 'default': 5, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Count'}, 'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}, 'topic_within_article': {'anyOf': [], 'default': '', 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Topic Within Article'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Extract key facts from a Wikipedia article, optionally focused on a topic.
Tool Name: fetch_content
Tool Arguments: {'properties': {'url': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Url', 'type': 'string'}}, 'required': ['url'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: 
    Fetch and parse content from a webpage URL.

    Args:
        url: The webpage URL to fetch content from
        ctx: MCP context for logging
    
Tool Name: get_article
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get the full content of a Wikipedia article.
Tool Name: get_coordinates
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get the coordinates of a Wikipedia article.
Tool Name: get_links
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get the links contained within a Wikipedia article.
Tool Name: get_related_topics
Tool Arguments: {'properties': {'limit': {'anyOf': [], 'default': 10, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Limit'}, 'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get topics related to a Wikipedia article based on links and categories.
Tool Name: get_sections
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get the sections of a Wikipedia article.
Tool Name: get_summary
Tool Arguments: {'properties': {'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get a summary of a Wikipedia article.
Tool Name: list_papers
Tool Arguments: {'properties': {}, 'title': 'DynamicModel', 'type': 'object'}
Tool Description: List all previously downloaded and converted papers that are available in local storage for immediate reading and analysis. This tool shows you what papers you already have access to without needing to download them again. Each paper in the list includes metadata like title, authors, abstract, and direct links. Use this tool to see your paper library, check if a specific paper is already downloaded, or browse previously acquired research papers before downloading new ones.
Tool Name: read_paper
Tool Arguments: {'properties': {'paper_id': {'anyOf': [], 'description': "The arXiv identifier of the paper to read (e.g., '2301.07041', '1706.03762'). This must be a paper that has been previously downloaded and converted to markdown format. Use list_papers to see available papers.", 'enum': None, 'items': None, 'properties': {}, 'title': '', 'type': 'string'}}, 'required': ['paper_id'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Read the full text content of a previously downloaded and converted research paper in clean markdown format. This tool retrieves the complete paper content including abstract, introduction, methodology, results, conclusions, and references. The content is formatted for easy reading and analysis, with preserved mathematical equations and structured sections. Use this tool when you need to access the full text of a paper for detailed study, quotation, analysis, or research. The paper must have been previously downloaded using the download_paper tool.
Tool Name: search
Tool Arguments: {'properties': {'max_results': {'anyOf': [], 'default': 10, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Max Results'}, 'query': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Query', 'type': 'string'}}, 'required': ['query'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: 
    Search DuckDuckGo and return formatted results.

    Args:
        query: The search query string
        max_results: Maximum number of results to return (default: 10)
        ctx: MCP context for logging
    
Tool Name: search_papers
Tool Arguments: {'properties': {'categories': {'anyOf': [], 'default': None, 'description': "Strongly recommended: arXiv categories to focus search (e.g., ['cs.AI', 'cs.MA'] for agent research, ['cs.LG'] for ML, ['cs.CL'] for NLP, ['cs.CV'] for vision). Greatly improves relevance.", 'enum': None, 'items': {'type': 'string'}, 'properties': {}, 'title': ''}, 'date_from': {'anyOf': [], 'default': None, 'description': "Start date for papers (YYYY-MM-DD format). Use to find recent work, e.g., '2023-01-01' for last 2 years.", 'enum': None, 'items': None, 'properties': {}, 'title': ''}, 'date_to': {'anyOf': [], 'default': None, 'description': "End date for papers (YYYY-MM-DD format). Use with date_from to find historical work, e.g., '2020-12-31' for older research.", 'enum': None, 'items': None, 'properties': {}, 'title': ''}, 'max_results': {'anyOf': [], 'default': None, 'description': 'Maximum number of results to return (default: 10, max: 50). Use 15-20 for comprehensive searches.', 'enum': None, 'items': None, 'properties': {}, 'title': ''}, 'query': {'anyOf': [], 'description': 'Search query using quoted phrases for exact matches (e.g., \'"machine learning" OR "deep learning"\') or specific technical terms. Avoid overly broad or generic terms.', 'enum': None, 'items': None, 'properties': {}, 'title': '', 'type': 'string'}, 'sort_by': {'anyOf': [], 'default': None, 'description': "Sort results by 'relevance' (most relevant first, default) or 'date' (newest first). Use 'relevance' for focused searches, 'date' for recent developments.", 'enum': ['relevance', 'date'], 'items': None, 'properties': {}, 'title': ''}}, 'required': ['query'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Search for papers on arXiv with advanced filtering and query optimization.

QUERY CONSTRUCTION GUIDELINES:
- Use QUOTED PHRASES for exact matches: "multi-agent systems", "neural networks", "machine learning"
- Combine related concepts with OR: "AI agents" OR "software agents" OR "intelligent agents"  
- Use field-specific searches for precision:
  - ti:"exact title phrase" - search in titles only
  - au:"author name" - search by author
  - abs:"keyword" - search in abstracts only
- Use ANDNOT to exclude unwanted results: "machine learning" ANDNOT "survey"
- For best results, use 2-4 core concepts rather than long keyword lists

ADVANCED SEARCH PATTERNS:
- Field + phrase: ti:"transformer architecture" for papers with exact title phrase
- Multiple fields: au:"Smith" AND ti:"quantum" for author Smith's quantum papers  
- Exclusions: "deep learning" ANDNOT ("survey" OR "review") to exclude survey papers
- Broad + narrow: "artificial intelligence" AND (robotics OR "computer vision")

CATEGORY FILTERING (highly recommended for relevance):
- cs.AI: Artificial Intelligence
- cs.MA: Multi-Agent Systems  
- cs.LG: Machine Learning
- cs.CL: Computation and Language (NLP)
- cs.CV: Computer Vision
- cs.RO: Robotics
- cs.HC: Human-Computer Interaction
- cs.CR: Cryptography and Security
- cs.DB: Databases

EXAMPLES OF EFFECTIVE QUERIES:
- ti:"reinforcement learning" with categories: ["cs.LG", "cs.AI"] - for RL papers by title
- au:"Hinton" AND "deep learning" with categories: ["cs.LG"] - for Hinton's deep learning work
- "multi-agent" ANDNOT "survey" with categories: ["cs.MA"] - exclude survey papers
- abs:"transformer" AND ti:"attention" with categories: ["cs.CL"] - attention papers with transformer abstracts

DATE FILTERING: Use YYYY-MM-DD format for historical research:
- date_to: "2015-12-31" - for foundational/classic work (pre-2016)
- date_from: "2020-01-01" - for recent developments (post-2020)
- Both together for specific time periods

RESULT QUALITY: Results sorted by RELEVANCE (most relevant papers first), not just newest papers.
This ensures you get the most pertinent results regardless of publication date.

TIPS FOR FOUNDATIONAL RESEARCH:
- Use date_to: "2010-12-31" to find classic papers on BDI, SOAR, ACT-R
- Combine with field searches: ti:"BDI" AND abs:"belief desire intention"  
- Try author searches: au:"Rao" AND "BDI" for Anand Rao's foundational BDI work
Tool Name: search_wikipedia
Tool Arguments: {'properties': {'limit': {'anyOf': [], 'default': 10, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Limit'}, 'query': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Query', 'type': 'string'}}, 'required': ['query'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Search Wikipedia for articles matching a query.
Tool Name: summarize_article_for_query
Tool Arguments: {'properties': {'max_length': {'anyOf': [], 'default': 250, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Max Length'}, 'query': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Query', 'type': 'string'}, 'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['query', 'title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get a summary of a Wikipedia article tailored to a specific query.
Tool Name: summarize_article_section
Tool Arguments: {'properties': {'max_length': {'anyOf': [], 'default': 150, 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Max Length'}, 'section_title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Section Title', 'type': 'string'}, 'title': {'anyOf': [], 'description': '', 'enum': None, 'items': None, 'properties': {}, 'title': 'Title', 'type': 'string'}}, 'required': ['section_title', 'title'], 'title': 'DynamicModel', 'type': 'object'}
Tool Description: Get a summary of a specific section of a Wikipedia article.

IMPORTANT: Use the following format in your response:

```
Thought: you should always think about what to do
Action: the action to take, only one name of [download_paper, extract_key_facts, fetch_content, get_article, get_coordinates, get_links, get_related_topics, get_sections, get_summary, list_papers, read_paper, search, search_papers, search_wikipedia, summarize_article_for_query, summarize_article_section], just the name, exactly as it's written.
Action Input: the input to the action, just a simple JSON object, enclosed in curly braces, using " to wrap keys and values.
Observation: the result of the action
```

Once all necessary information is gathered, return the following format:

```
Thought: I now know the final answer
Final Answer: the final answer to the original input question
```[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Final Answer:[00m [92m
The document will detail LLMZip, a lossless text compression algorithm using LLaMA-7B for next-token prediction. It will formally define the compression ratio (ρ = N_b / N_c bits/character) and derive an asymptotic upper bound on the entropy of English, H(S) ≤ lim_{N_T→∞} (-1/N_T) Σ_{i=1}^{N_T} log_2 q_i(X_i) / E[B], where q_i is the LLM's predicted probability, X_i is the actual token, and E[B] is the average characters per token. Three encoding schemes will be presented: LLaMA+zlib (compressing ranks), LLaMA+TbyT (token-by-token prefix-free coding with length l_i = ⌈log_2(1/q_i(x_i))⌉), and LLM+AC (arithmetic coding using LLM probabilities, expected to be nearly optimal with ρ_LLM+AC ≈ (Σ_{i=1}^{N_T} log_2(1/q_i(x_i))) / (Σ_{i=1}^{N_T} b_i)).

Experimental results will be presented using LLaMA-7B and the SentencePiece tokenizer on the text8 dataset (1MB sections, 100,000 tokens per batch) and a recently published book from Project Gutenberg (100,000 tokens). LLMZip will be compared against state-of-the-art compressors like ZPAQ (1.4 bpc) and paq8h (1.2 bpc). Key findings will include:
- An estimated asymptotic upper bound on English entropy (H_ub) of 0.7093 bits/character on text8 and 0.8417 bpc on the Project Gutenberg book, significantly lower than previous estimates.
- LLM+AC achieving an average compression ratio of 0.7101 bits/character on text8 and 0.8426 bits/character on the Project Gutenberg book, outperforming ZPAQ and paq8h.
- Compression performance improving with increased LLM memory (M), with inference time scaling linearly with M.
- Acknowledgment of potential optimistic results on text8 due to LLaMA-7B's training corpus and the inability to run LLaMA-7B on the full 100MB text8 dataset.

The document will include a comprehensive list of references, including foundational works by Shannon (1951) and Cover and King (1978), as well as more recent works on deep learning for compression and the LLaMA model.[00m


🚀 Crew: crew
├── 📋 Task: f419d80c-228e-4c84-9deb-a66fc8d9f41e
│   Assigned to: AI Assistant
│   Status: ✅ Completed
│   ├── ✅ Reasoning Completed
│   ├── 🔧 Used search_papers (1)
│   ├── 🔧 Used search_papers (2)
│   └── 🔧 Used download_paper (1)
├── 🔧 Using create_reasoning_plan from LLM available_function
├── 🔄 Tool Usage Started
├── 🔧 Using create_reasoning_plan from LLM available_function
└── ✅ Tool Usage Completed┌────────────────────────────── Task Completion ──────────────────────────────┐
│                                                                             │
│  Task Completed                                                             │
│  Name: f419d80c-228e-4c84-9deb-a66fc8d9f41e                                 │
│  Agent: AI Assistant                                                        │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

┌────────────────────────────── Crew Completion ──────────────────────────────┐
│                                                                             │
│  Crew Execution Completed                                                   │
│  Name: crew                                                                 │
│  ID: e576a6b0-8f83-4092-98e3-4c5049c97dad                                   │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

AGENT_RESULT_JSON:{"final_text": "The document will detail LLMZip, a lossless text compression algorithm using LLaMA-7B for next-token prediction. It will formally define the compression ratio (\u03c1 = N_b / N_c bits/character) and derive an asymptotic upper bound on the entropy of English, H(S) \u2264 lim_{N_T\u2192\u221e} (-1/N_T) \u03a3_{i=1}^{N_T} log_2 q_i(X_i) / E[B], where q_i is the LLM's predicted probability, X_i is the actual token, and E[B] is the average characters per token. Three encoding schemes will be presented: LLaMA+zlib (compressing ranks), LLaMA+TbyT (token-by-token prefix-free coding with length l_i = \u2308log_2(1/q_i(x_i))\u2309), and LLM+AC (arithmetic coding using LLM probabilities, expected to be nearly optimal with \u03c1_LLM+AC \u2248 (\u03a3_{i=1}^{N_T} log_2(1/q_i(x_i))) / (\u03a3_{i=1}^{N_T} b_i)).\n\nExperimental results will be presented using LLaMA-7B and the SentencePiece tokenizer on the text8 dataset (1MB sections, 100,000 tokens per batch) and a recently published book from Project Gutenberg (100,000 tokens). LLMZip will be compared against state-of-the-art compressors like ZPAQ (1.4 bpc) and paq8h (1.2 bpc). Key findings will include:\n- An estimated asymptotic upper bound on English entropy (H_ub) of 0.7093 bits/character on text8 and 0.8417 bpc on the Project Gutenberg book, significantly lower than previous estimates.\n- LLM+AC achieving an average compression ratio of 0.7101 bits/character on text8 and 0.8426 bits/character on the Project Gutenberg book, outperforming ZPAQ and paq8h.\n- Compression performance improving with increased LLM memory (M), with inference time scaling linearly with M.\n- Acknowledgment of potential optimistic results on text8 due to LLaMA-7B's training corpus and the inability to run LLaMA-7B on the full 100MB text8 dataset.\n\nThe document will include a comprehensive list of references, including foundational works by Shannon (1951) and Cover and King (1978), as well as more recent works on deep learning for compression and the LLaMA model.", "duration_ms": 47384}
The document will detail LLMZip, a lossless text compression algorithm using LLaMA-7B for next-token prediction. It will formally define the compression ratio (ρ = N_b / N_c bits/character) and derive an asymptotic upper bound on the entropy of English, H(S) ≤ lim_{N_T→∞} (-1/N_T) Σ_{i=1}^{N_T} log_2 q_i(X_i) / E[B], where q_i is the LLM's predicted probability, X_i is the actual token, and E[B] is the average characters per token. Three encoding schemes will be presented: LLaMA+zlib (compressing ranks), LLaMA+TbyT (token-by-token prefix-free coding with length l_i = ⌈log_2(1/q_i(x_i))⌉), and LLM+AC (arithmetic coding using LLM probabilities, expected to be nearly optimal with ρ_LLM+AC ≈ (Σ_{i=1}^{N_T} log_2(1/q_i(x_i))) / (Σ_{i=1}^{N_T} b_i)).

Experimental results will be presented using LLaMA-7B and the SentencePiece tokenizer on the text8 dataset (1MB sections, 100,000 tokens per batch) and a recently published book from Project Gutenberg (100,000 tokens). LLMZip will be compared against state-of-the-art compressors like ZPAQ (1.4 bpc) and paq8h (1.2 bpc). Key findings will include:
- An estimated asymptotic upper bound on English entropy (H_ub) of 0.7093 bits/character on text8 and 0.8417 bpc on the Project Gutenberg book, significantly lower than previous estimates.
- LLM+AC achieving an average compression ratio of 0.7101 bits/character on text8 and 0.8426 bits/character on the Project Gutenberg book, outperforming ZPAQ and paq8h.
- Compression performance improving with increased LLM memory (M), with inference time scaling linearly with M.
- Acknowledgment of potential optimistic results on text8 due to LLaMA-7B's training corpus and the inability to run LLaMA-7B on the full 100MB text8 dataset.

The document will include a comprehensive list of references, including foundational works by Shannon (1951) and Cover and King (1978), as well as more recent works on deep learning for compression and the LLaMA model.
C:\tools\Anaconda3\Lib\site-packages\paramiko\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "cipher": algorithms.TripleDES,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.
  "class": algorithms.Blowfish,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "class": algorithms.TripleDES,
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\_internal\_generate_schema.py:628: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.
  warn(
- Reading configuration...
  - Reading registry from registry.yaml
  - Reading catalog from docker-mcp.yaml
  - Reading config from config.yaml
- Configuration read in 22.6157ms
- Watching registry at C:\Users\Noah\.docker\mcp\registry.yaml
- Watching config at C:\Users\Noah\.docker\mcp\config.yaml
- Those servers are enabled: arxiv-mcp-server, duckduckgo, wikipedia-mcp
- Using images:
  - mcp/arxiv-mcp-server@sha256:6dc6bba6dfed97f4ad6eb8d23a5c98ef5b7fa6184937d54b2d675801cd9dd29e
  - mcp/duckduckgo@sha256:68eb20db6109f5c312a695fc5ec3386ad15d93ffb765a0b4eb1baf4328dec14f
  - mcp/wikipedia-mcp@sha256:e90e3f6e3bb795f10d3d8d8ac1523f6d46d4f2ab5ac3b3a4720e1be879466472
> Images pulled in 62.9997ms
- Listing MCP tools...
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
  - Running mcp/duckduckgo with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=duckduckgo -l docker-mcp-transport=stdio]
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers:ro]
  > duckduckgo: (2 tools)
  > wikipedia-mcp: (10 tools) (9 resourceTemplates)
  > arxiv-mcp-server: (4 tools) (1 prompts)
> 16 tools listed in 3.1354475s
- Watching for configuration updates...
> Initialized in 3.221603s
> Start stdio server
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\fields.py:1093: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'items', 'anyOf', 'enum', 'properties'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
  warn(
┌────────────────────────── Crew Execution Started ───────────────────────────┐
│                                                                             │
│  Crew Execution Started                                                     │
│  Name: crew                                                                 │
│  ID: 086d44ac-2b4c-4903-9e62-ef9c6c100bd1                                   │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

🚀 Crew: crew
├── 📋 Task: 0a388a52-bfff-458b-b4bc-80efbd19e943
│   Status: Executing Task...
│   └── ✅ Reasoning Completed
├── 🔧 Using create_reasoning_plan from LLM available_function
├── 🔄 Tool Usage Started
├── 🔧 Using create_reasoning_plan from LLM available_function
└── ✅ Tool Usage Completed┌───────────────────────────── 🧠 Reasoning Plan ─────────────────────────────┐
│                                                                             │
│  1. **Identify the document:** The document is "LLMZip: Lossless Text       │
│  Compression using Large Language Models" (arXiv: 2306.04050v2).            │
│  2. **Search for the full document:** Use `search_papers` with the title    │
│  and arXiv ID to locate the paper.                                          │
│  3. **Retrieve the document content:** Use `read_paper` with the obtained   │
│  `paper_id` to access the full text of the paper.                           │
│  4. **Analyze the document structure:** Use `get_sections` to identify the  │
│  main sections of the paper, focusing on those that would logically follow  │
│  the Abstract and Introduction (e.g., Methodology, Experiments, Results,    │
│  Discussion, Conclusion, References).                                       │
│  5. **Extract relevant information from later sections:** For each          │
│  identified relevant section, use `extract_key_facts` to retrieve key       │
│  facts, equations, definitions, datasets, and references.                   │
│  6. **Synthesize the synopsis:** Combine the extracted information into a   │
│  concise textual synopsis, ensuring it is relevant to the document,         │
│  factual, and avoids hallucination. The synopsis will cover the LLMZip      │
│  algorithm, experimental setup, results, discussion, and any significant    │
│  definitions or equations.                                                  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool download_paper with arguments: {"check_status":false,"paper_id":"2306.04050v2"}
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
  > Calling tool download_paper took: 1.7999716s
  - Scanning tool call response for secrets...
  > No secret found in response.
[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Task:[00m [92mYou act as a deterministic research agent. Given the observed prefix of a document, use available MCP tools to retrieve information that is likely to appear later in the same document. Prefer authoritative sources. Produce a concise textual synopsis of the likely future content (facts, equations, definitions, datasets, references), suitable for conditioning a language model compressor. Maximize relevance to the document. Do not hallucinate; only include content supported by the tools.

Document prefix (UTF-8 text):
# LLMZip: Lossless Text Compression using Large Language Models

**Authors:** Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas Shakkottai  
**Affiliation:** Department of Electrical and Computer Engineering, Texas A&M University  
**Email:** {vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu  
**arXiv:** 2306.04050v2 [cs.IT] 26 Jun 2023

## Abstract

We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.

## I. INTRODUCTION

There are close connections between learning, prediction, and compression. The success of ChatGPT has captured the fascination of general public and brought the connection between learning and prediction to the fore. The main advance brought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in a paragraph based on knowing the past several words (tokens).

The connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the entropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past values can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many algorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of lossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very effective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor and every time there

Output: A concise synopsis (plain text).

Reasoning Plan:
1. **Identify the document:** The document is "LLMZip: Lossless Text Compression using Large Language Models" (arXiv: 2306.04050v2).
2. **Search for the full document:** Use `search_papers` with the title and arXiv ID to locate the paper.
3. **Retrieve the document content:** Use `read_paper` with the obtained `paper_id` to access the full text of the paper.
4. **Analyze the document structure:** Use `get_sections` to identify the main sections of the paper, focusing on those that would logically follow the Abstract and Introduction (e.g., Methodology, Experiments, Results, Discussion, Conclusion, References).
5. **Extract relevant information from later sections:** For each identified relevant section, use `extract_key_facts` to retrieve key facts, equations, definitions, datasets, and references.
6. **Synthesize the synopsis:** Combine the extracted information into a concise textual synopsis, ensuring it is relevant to the document, factual, and avoids hallucination. The synopsis will cover the LLMZip algorithm, experimental setup, results, discussion, and any significant definitions or equations.[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mAction: download_paper[00m
[95m## Using tool:[00m [92mdownload_paper[00m
[95m## Tool Input:[00m [92m
"{\"paper_id\": \"2306.04050v2\"}"[00m
[95m## Tool Output:[00m [92m
  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool read_paper with arguments: {"paper_id":"2306.04050v2"}
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
  > Calling tool read_paper took: 1.8070112s
  - Scanning tool call response for secrets...
  > No secret found in response.
{"status": "success", "message": "Paper already available", "resource_uri": "file:///app/papers/2306.04050v2.md", "pdf_uri": "https://arxiv.org/pdf/2306.04050v2.pdf", "content": "1\n\n# LLMZip: Lossless Text Compression using Large Language Models\n\n\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\n\n\nAbstract\n\n\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\n\n\nI. I NTRODUCTION\n\n\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\n\n\nII. I NTUITIVE EXPLANATION OF THE MAIN IDEA\n\n\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\n\nMy first attempt at writing a book\n\n\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032, \u2032 a \u2032, \u2032 book \u2032\n\n\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n\n\n2\n\n\nFigure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be\nwords such as [\u2032] reading [\u2032], [\u2032] writing [\u2032], [\u2032] driving [\u2032], [\u2032] cooking [\u2032] etc. The main idea is to compute the rank of the actual word in\nour sentence ( [\u2032] writing [\u2032] ) in this list and call it R 5 . We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for [\u2032] writing [\u2032] is R 5 = 1.\n\n\nTokens\n\n\n\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x5|x4 1)<br>5|Rank<br>computation|R5 = 1|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nreading 0.3\nwriting 0.2\ncycling 0.1\ndriving 0.05\n\n... ...\n\n\n\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\n\n\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, [\u2032] a [\u2032], and hence, the rank R 6 would be 0.\n\n\nTokens\n\n\n\n\n\n\u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x6|x5 2)<br>6|Rank<br>computation|R6 = 0|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nan 0.2\n\nthe 0.05\n\n... ...\n\n\n\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\n\n\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n\n\n1, 0, 0, . . .\n\n\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\n\n\nSequence of ranks\n\n|e of ranks|Col2|Col3|\n|---|---|---|\n|. ., rNT|Compression (zip)|Nb|\n||||\n\n\n\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\n\n\nWhen we wish to reconstruct the sequence, we first decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank R i during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\n\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n\n\n3\n\n\nIII. C OMPRESSION USING LLM S\n\n\nLet s denote a sentence from the English language composed of N c letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We first parse s into a sequence of N T tokens\ndenoted by x = x 1, x 2, . . ., x i\u22121, x i, x i+1, . . . x N T, where x i \u2208X . There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. x i \u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter X i .\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens x i\u2212M, x i\u2212M+1, . . ., x i\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nq i (x i ) := Pr(X i = x i |x i\u22121, x i\u22122, . . ., x i\u2212M ), \u2200x i \u2208X . The PMF vector q i := [q i (1), q i (2), . . ., q i (D)] [T] is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcq i . Let \u03b3 i : X \u2192X be a permutation on the integers\nfrom 1 to D such that\n\n\u02dc\nq i (\u03b3 i (j)) = q i (j), \u2200j \u2208X .\n\n\nThat is, \u03b3 i (j) is the rank of the token j at epoch i. We define the rank of the input sequence at epoch i as the rank of the\ntoken x i at epoch i, r i := \u03b3 i (x i ). The sequence {r i } [N] i=1 [T] [is compressed by a lossless compression algorithm (such as zlib) to]\nproduce N b bits which are the final bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors q i \u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 defined as\n\n\u03c1 := [N] [b] bits/character.\n\nN c\n\n\n\nLosseless\n\nCompression\n\n\n\nTokens . . ., x i\u2212M, . . ., x i\u22121\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n\nInput sentence s with N c characters\n\n\n\ni i i\u22121 Rank\n\nLLM\ncomputation\n\n\n\nq i = Pr(X i = j|x [i] i [\u2212] \u2212 [M] 1 [)] Rank r i Losseless N b bits\n\n\n\nFig. 4. Schematic showing the prediction at epoch i.\n\n\nA. Entropy bounds\n\n\nLet S \u2208S [\u221e] be a random process that represents language input. The nth character in the sequence is denoted by S n,\nwhereas the string of characters from the beginning to the nth character is expressed as S n . The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X 1, X 2, . . . using a variable-length mapping. In this sequence, X i is the ith\ntoken. The number of characters employed to generate X i depends on the realization of the random process and, as such,\nwe introduce random variable B i to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which B i \u2265 1 and B i is uniformly bounded, with B i < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded N T \u2192\u221e, the number of characters must also approach infinity N c \u2192\u221e. Formally, consider the tokenizer\nfunction T : S [N] \u2192X [N] operating on infinite symbol sequences; that is, T (s) = x where s is an infinite sequence in S [\u221e] . For\nnatural number, i \u2208 N, define m i : S [N] \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Specifically, suppose s is given, then\n\n\nm i (s) = min n [{][length (][T][ (][s] [n] [))][ \u2265] [i][}][ .] (1)\n\n\nWe note that, by construction, lim n\u2192\u221e length (T (s n )) = \u221e and, as such, m i (\u00b7) is well-defined. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple finite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (s n ) = T (s n+1 ). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (finite) series of input symbols in the restricted\nset for which T (y n ) = x i is s m i (s) . Given a fixed sequence s, we can express the number of characters contained in a token\n\nas\nb i = m i (s) \u2212 m i\u22121 (s)\n\n\n4\n\n\nwith initial condition m \u22121 = 0. Consequently, the number of characters embedded in the first N T tokens for a random input\nbecomes N c = [\ufffd] i [N] =1 [T] [B] [i] [.]\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {S k } [\u221e] k=1 [,]\n{B i } [\u221e] i=1 [, and][ {][X] [i] [}] [\u221e] i=1 [are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that]\n\n\n\n\u2212 [1]\n\n\n\n\n[1]\n\nn [log] [2] [ p] [S] [n] [(][S] [1] [, . . ., S] [n] [) =][ \u2212] n [1]\n\n\n\nn [log] [2] [ p] [S] [n] [(][S] [n] [)][ \u2192] [H][(][S][)] almost surely. (2)\n\n\n\nLet \u2126 S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\n\n\n\n[1]\n\ni [log] [2] [ p] [X] [i] [(][X] [1] [, . . ., X] [i] [) =][ \u2212] [1] i\n\n\n\n\u2212 [1]\n\n\n\ni [log] [2] [ p] [X] [i] [(][X] [i] [)][ \u2192] [H][(][X][)] almost surely. (3)\n\n\n\nDefine \u2126 X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\n\n\nm i (S)\nlim = E [B] almost surely. (4)\ni\u2192\u221e i\n\n\nSet \u2126 B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126 S \u2229 \u2126 X \u2229 \u2126 B, we deduce that\n\n\nH(S) = lim\nk\u2192\u221e [\u2212] k [1] [log] [2] [ p] [S] [k] [(][S] [k] [(][\u03c9][))]\n\n= lim i\u2192\u221e [\u2212] l [1] i log 2 p S li (S l i (\u03c9))\n\n\n= lim log 2 Pr (X i = T (S l i (\u03c9)))\ni\u2192\u221e [\u2212] l [1] i\n\n\n\n1\n\ni [log] [2] [ Pr (][X] [i] [ =][ x] [i] [) =][ H] E [(] B [X][)]\n\n\n\n1 1\n\n= \u2212\nE[B] i [lim] \u2192\u221e i\n\n\n\nE[B] [.]\n\n\n\nThe first equality follows from (2). The second equality is a consequence of the fact that {l i = m i (S(\u03c9))|i \u2208 N} is an infinite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\n\ntwo events,\n{\u03c9 \u2208 \u2126|X i (\u03c9) = x i } = {\u03c9 \u2208 \u2126|T (S m i (S(\u03c9)) ) = x i }.\n\n\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126 B . The sets \u2126 S, \u2126 X, and \u2126 B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\n\nH(S) = [H][(][X][)] almost surely.\n\nE[B]\n\n\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\n\n\n\nPr\n\n\ufffd\n\n\n\n\ufffd\n\n\n\nH(X) \u2264 lim\nN T \u2192\u221e [\u2212] N [1] T\n\n\n\nN T\n\ufffd\n\n\n\n\ufffd log 2 q i (X i )\n\n\ni=1\n\n\n\n= 1, (5)\n\n\n\nwhere q i (\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\n\n\n\n\n[1] N T\n\nN T \ufffd i=1 [log] 2 [q] [i] [(][X] [i] [)]\n\n\n\nH(S) \u2264 lim N T \u2192\u221e \u2212 N [1]\n\n\n\n. (6)\nE[B]\n\n\n\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by H ub . The\nnumerator in (6) represents the average number of bits required to represent the tokens X N T and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\n\n\nB. Encoding schemes\n\n\nWe consider three schemes for the lossless compression block in Fig. 3.\n\n\n5\n\n\n1) Compressing the ranks using zlib: The first scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1 LLaMA+zlib .\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token x i at epoch i by using a prefix-free code assuming q i to be the true distribution of\nthe tokens. A natural choice for a prefix-free code is a Huffman code. Instead, for simplicity, we use a prefix-free code where\n1\nthe codeword for the token x i is of length l i = \u2308log 2 q i (x i ) [\u2309][. A prefix-free code with this length for][ x] [i] [ is guaranteed to exist]\nsince this choice of lengths satisfies the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1 LLaMA+TbyT,\nis given by\n\n\n\nN T\n\ufffd\n\n\ni=1\n\n\n\n1\nlog 2\n\ufffd q i (x i )\n\n\n\n\ufffd\n\n\n\n\u03c1 LLaMA+TbyT =\n\n\n\n\ufffd Ni=1 T [b] [i] .\n\n\n\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use q i (x i ) as the probability of token x i at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1 LLM+AC . It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\n\n\n\u03c1 LLM+AC \u2248\n\n\n\nN T 1\n\ufffd i=1 log 2 q i (x i )\n\n\ufffd Ni=1 T [b] [i] . (7)\n\n\n\nClearly, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and \u03c1 LLM+AC also provide upper bounds on H(S). H ub, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and\n\u03c1 LLM+AC are estimated using a finite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\n\n\nIV. R ESULTS\n\n\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it difficult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\n[Nevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of](http://mattmahoney.net/dc/text8.zip)\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M ) is shown in Table II. As expected, the\ncompression performance improves with increasing M . We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n\n\n6\n\n\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\n\n\nV. A CKNOWLEDGEMENT\n\n\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\n\n\nTABLE I\n\nR ESULTS FOR 1MB OF TEXT FROM TEXT 8 DATASET\n\n\n\nBatch\nN c N T\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC ZPAQ pq8h\n\n\n\nNo. (bpc) file size (bits) (bpc) (bpc) (bpc) (bpc)\n\n\n1 466, 650 100, 000 0.6882 1.0513 0.8215 0.689\n\n\n2 461, 477 100, 000 0.6893 1.0558 0.8242 0.6901\n\n\n3 454, 599 100, 000 0.699 1.0681 0.8357 0.6999\n\n\n4 462, 755 100, 000 0.6748 1.0346 0.8093 0.6757\n\n\n5 453, 847 100, 000 0.7481 1.1265 0.8831 0.749\n\n\n6 458, 252 100, 000 0.7218 1.0957 0.8567 0.7227\n\n\n7 451, 036 100, 000 0.6959 1.0729 0.8353 0.6968\n\n\n8 447, 953 100, 000 0.7092 1.0896 0.8489 0.7101\n\n\n9 462, 665 100, 000 0.7394 1.1126 0.8713 0.7402\n\n\n10 449, 621 100, 000 0.7269 1.1046 0.8643 0.7277\n\n\nTotal 9, 137, 710 2, 000, 000 0.7093 1.0812 0.845 0.7101 1.4 [1] 1.2 [2]\n\n\nTABLE II\n\nC OMPRESSION PERFORMANCE OF THE LLM ON THE TEXT 8 DATASET, AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\n\n\n(bpc) file size (bits) (bpc) (bpc)\n\n\n\n31 4, 568, 855 1, 000, 000 0.9139 1.3159 1.0425 0.9145\n\n\n127 4, 568, 855 1, 000, 000 0.7511 1.1303 0.8847 0.752\n\n\n255 4, 568, 855 1, 000, 000 0.7242 1.0985 0.859 0.725\n\n\n511 4, 568, 855 1, 000, 000 0.7093 1.0812 0.845 0.7101\n\n\n1 This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2 This result is taken from [12] and it corresponds to the full 100MB dataset text8\n\n\n7\n\n\n\nTABLE III\n\nM EAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\nM\n\n(bpc) (bpc) (bpc) (bpc)\n\n\n31 0.9139 \u00b1 0.0263 1.3159 \u00b1 0.0329 1.0425 \u00b1 0.0262 0.9145 \u00b1 0.0263\n\n\n127 0.7511 \u00b1 0.0233 1.1303 \u00b1 0.0292 0.8847 \u00b1 0.0231 0.752 \u00b1 0.0233\n\n\n255 0.7242 \u00b1 0.0234 1.0985 \u00b1 0.0289 0.859 \u00b1 0.0232 0.725 \u00b1 0.0234\n\n\n511 0.7093 \u00b1 0.0228 1.0812 \u00b1 0.028 0.845 \u00b1 0.0226 0.7101 \u00b1 0.0228\n\n\nTABLE IV\n\nC OMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN P ROJECT G UTENBERG [7], AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC Standalone Zlib\n\n\n(bpc) (bpc) (bpc) (bpc) (bpc)\n\n\n\n31 508, 463 115, 000 1.0919 1.5316 1.2152 1.0924 2.80\n\n\n127 508, 463 115, 000 0.8973 1.3128 1.0235 0.8982 2.80\n\n\n255 508, 463 115, 000 0.8618 1.2684 0.9899 0.8627 2.80\n\n\n511 508, 463 115, 000 0.8417 1.2465 0.9711 0.8426 2.80\n\n\n8\n\n\nR EFERENCES\n\n\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR filters),\u201d 2023.\n\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and efficient foundation language models,\u201d\n2023.\n\n[7] J. Frank Dobie, Legends of Texas, United States, Texas Folk-Lore Society, 1924; Project Gutenberg, May 25, 2023, 2023,\n[https://www.gutenberg.org/ebooks/70859.](https://www.gutenberg.org/ebooks/70859)\n\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n\n\n"}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mThought: The paper has been downloaded and its content is available. I need to extract information that is likely to appear later in the document, specifically focusing on the methodology, results, and conclusions, as well as any key definitions, equations, datasets, and references.
Based on the provided prefix and the full content, the document structure is:
- Abstract
- I. Introduction
- II. Intuitive Explanation of the Main Idea
- III. Compression Using LLMs
    - A. Entropy bounds
    - B. Encoding schemes
        - 1) Compressing the ranks using zlib
        - 2) Token-by-Token Compression
        - 3) Arithmetic Coding
- IV. Results
- V. Acknowledgement
- References
I will now go through these sections and extract the relevant information. I will focus on sections III and IV, as they contain the core methodology and results. I will also list the references.
Thought: The paper has been downloaded and its content is available. I need to extract information that is likely to appear later in the document, specifically focusing on the methodology, results, and conclusions, as well as any key definitions, equations, datasets, and references.
Based on the provided prefix and the full content, the document structure is:
- Abstract
- I. Introduction
- II. Intuitive Explanation of the Main Idea
- III. Compression Using LLMs
    - A. Entropy bounds
    - B. Encoding schemes
        - 1) Compressing the ranks using zlib
        - 2) Token-by-Token Compression
        - 3) Arithmetic Coding
- IV. Results
- V. Acknowledgement
- References
I will now go through these sections and extract the relevant information. I will focus on sections III and IV, as they contain the core methodology and results. I will also list the references.[00m
[95m## Using tool:[00m [92mread_paper[00m
[95m## Tool Input:[00m [92m
"{\"paper_id\": \"2306.04050v2\"}"[00m
[95m## Tool Output:[00m [92m
{"status": "success", "paper_id": "2306.04050v2", "content": "1\n\n# LLMZip: Lossless Text Compression using Large Language Models\n\n\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\n\n\nAbstract\n\n\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\n\n\nI. I NTRODUCTION\n\n\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\n\n\nII. I NTUITIVE EXPLANATION OF THE MAIN IDEA\n\n\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\n\nMy first attempt at writing a book\n\n\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032, \u2032 a \u2032, \u2032 book \u2032\n\n\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n\n\n2\n\n\nFigure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be\nwords such as [\u2032] reading [\u2032], [\u2032] writing [\u2032], [\u2032] driving [\u2032], [\u2032] cooking [\u2032] etc. The main idea is to compute the rank of the actual word in\nour sentence ( [\u2032] writing [\u2032] ) in this list and call it R 5 . We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for [\u2032] writing [\u2032] is R 5 = 1.\n\n\nTokens\n\n\n\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x5|x4 1)<br>5|Rank<br>computation|R5 = 1|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nreading 0.3\nwriting 0.2\ncycling 0.1\ndriving 0.05\n\n... ...\n\n\n\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\n\n\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, [\u2032] a [\u2032], and hence, the rank R 6 would be 0.\n\n\nTokens\n\n\n\n\n\n\u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x6|x5 2)<br>6|Rank<br>computation|R6 = 0|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nan 0.2\n\nthe 0.05\n\n... ...\n\n\n\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\n\n\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n\n\n1, 0, 0, . . .\n\n\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\n\n\nSequence of ranks\n\n|e of ranks|Col2|Col3|\n|---|---|---|\n|. ., rNT|Compression (zip)|Nb|\n||||\n\n\n\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\n\n\nWhen we wish to reconstruct the sequence, we first decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank R i during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\n\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n\n\n3\n\n\nIII. C OMPRESSION USING LLM S\n\n\nLet s denote a sentence from the English language composed of N c letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We first parse s into a sequence of N T tokens\ndenoted by x = x 1, x 2, . . ., x i\u22121, x i, x i+1, . . . x N T, where x i \u2208X . There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. x i \u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter X i .\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens x i\u2212M, x i\u2212M+1, . . ., x i\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nq i (x i ) := Pr(X i = x i |x i\u22121, x i\u22122, . . ., x i\u2212M ), \u2200x i \u2208X . The PMF vector q i := [q i (1), q i (2), . . ., q i (D)] [T] is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcq i . Let \u03b3 i : X \u2192X be a permutation on the integers\nfrom 1 to D such that\n\n\u02dc\nq i (\u03b3 i (j)) = q i (j), \u2200j \u2208X .\n\n\nThat is, \u03b3 i (j) is the rank of the token j at epoch i. We define the rank of the input sequence at epoch i as the rank of the\ntoken x i at epoch i, r i := \u03b3 i (x i ). The sequence {r i } [N] i=1 [T] [is compressed by a lossless compression algorithm (such as zlib) to]\nproduce N b bits which are the final bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors q i \u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 defined as\n\n\u03c1 := [N] [b] bits/character.\n\nN c\n\n\n\nLosseless\n\nCompression\n\n\n\nTokens . . ., x i\u2212M, . . ., x i\u22121\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n\nInput sentence s with N c characters\n\n\n\ni i i\u22121 Rank\n\nLLM\ncomputation\n\n\n\nq i = Pr(X i = j|x [i] i [\u2212] \u2212 [M] 1 [)] Rank r i Losseless N b bits\n\n\n\nFig. 4. Schematic showing the prediction at epoch i.\n\n\nA. Entropy bounds\n\n\nLet S \u2208S [\u221e] be a random process that represents language input. The nth character in the sequence is denoted by S n,\nwhereas the string of characters from the beginning to the nth character is expressed as S n . The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X 1, X 2, . . . using a variable-length mapping. In this sequence, X i is the ith\ntoken. The number of characters employed to generate X i depends on the realization of the random process and, as such,\nwe introduce random variable B i to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which B i \u2265 1 and B i is uniformly bounded, with B i < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded N T \u2192\u221e, the number of characters must also approach infinity N c \u2192\u221e. Formally, consider the tokenizer\nfunction T : S [N] \u2192X [N] operating on infinite symbol sequences; that is, T (s) = x where s is an infinite sequence in S [\u221e] . For\nnatural number, i \u2208 N, define m i : S [N] \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Specifically, suppose s is given, then\n\n\nm i (s) = min n [{][length (][T][ (][s] [n] [))][ \u2265] [i][}][ .] (1)\n\n\nWe note that, by construction, lim n\u2192\u221e length (T (s n )) = \u221e and, as such, m i (\u00b7) is well-defined. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple finite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (s n ) = T (s n+1 ). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (finite) series of input symbols in the restricted\nset for which T (y n ) = x i is s m i (s) . Given a fixed sequence s, we can express the number of characters contained in a token\n\nas\nb i = m i (s) \u2212 m i\u22121 (s)\n\n\n4\n\n\nwith initial condition m \u22121 = 0. Consequently, the number of characters embedded in the first N T tokens for a random input\nbecomes N c = [\ufffd] i [N] =1 [T] [B] [i] [.]\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {S k } [\u221e] k=1 [,]\n{B i } [\u221e] i=1 [, and][ {][X] [i] [}] [\u221e] i=1 [are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that]\n\n\n\n\u2212 [1]\n\n\n\n\n[1]\n\nn [log] [2] [ p] [S] [n] [(][S] [1] [, . . ., S] [n] [) =][ \u2212] n [1]\n\n\n\nn [log] [2] [ p] [S] [n] [(][S] [n] [)][ \u2192] [H][(][S][)] almost surely. (2)\n\n\n\nLet \u2126 S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\n\n\n\n[1]\n\ni [log] [2] [ p] [X] [i] [(][X] [1] [, . . ., X] [i] [) =][ \u2212] [1] i\n\n\n\n\u2212 [1]\n\n\n\ni [log] [2] [ p] [X] [i] [(][X] [i] [)][ \u2192] [H][(][X][)] almost surely. (3)\n\n\n\nDefine \u2126 X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\n\n\nm i (S)\nlim = E [B] almost surely. (4)\ni\u2192\u221e i\n\n\nSet \u2126 B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126 S \u2229 \u2126 X \u2229 \u2126 B, we deduce that\n\n\nH(S) = lim\nk\u2192\u221e [\u2212] k [1] [log] [2] [ p] [S] [k] [(][S] [k] [(][\u03c9][))]\n\n= lim i\u2192\u221e [\u2212] l [1] i log 2 p S li (S l i (\u03c9))\n\n\n= lim log 2 Pr (X i = T (S l i (\u03c9)))\ni\u2192\u221e [\u2212] l [1] i\n\n\n\n1\n\ni [log] [2] [ Pr (][X] [i] [ =][ x] [i] [) =][ H] E [(] B [X][)]\n\n\n\n1 1\n\n= \u2212\nE[B] i [lim] \u2192\u221e i\n\n\n\nE[B] [.]\n\n\n\nThe first equality follows from (2). The second equality is a consequence of the fact that {l i = m i (S(\u03c9))|i \u2208 N} is an infinite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\n\ntwo events,\n{\u03c9 \u2208 \u2126|X i (\u03c9) = x i } = {\u03c9 \u2208 \u2126|T (S m i (S(\u03c9)) ) = x i }.\n\n\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126 B . The sets \u2126 S, \u2126 X, and \u2126 B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\n\nH(S) = [H][(][X][)] almost surely.\n\nE[B]\n\n\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\n\n\n\nPr\n\n\ufffd\n\n\n\n\ufffd\n\n\n\nH(X) \u2264 lim\nN T \u2192\u221e [\u2212] N [1] T\n\n\n\nN T\n\ufffd\n\n\n\n\ufffd log 2 q i (X i )\n\n\ni=1\n\n\n\n= 1, (5)\n\n\n\nwhere q i (\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\n\n\n\n\n[1] N T\n\nN T \ufffd i=1 [log] 2 [q] [i] [(][X] [i] [)]\n\n\n\nH(S) \u2264 lim N T \u2192\u221e \u2212 N [1]\n\n\n\n. (6)\nE[B]\n\n\n\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by H ub . The\nnumerator in (6) represents the average number of bits required to represent the tokens X N T and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\n\n\nB. Encoding schemes\n\n\nWe consider three schemes for the lossless compression block in Fig. 3.\n\n\n5\n\n\n1) Compressing the ranks using zlib: The first scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1 LLaMA+zlib .\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token x i at epoch i by using a prefix-free code assuming q i to be the true distribution of\nthe tokens. A natural choice for a prefix-free code is a Huffman code. Instead, for simplicity, we use a prefix-free code where\n1\nthe codeword for the token x i is of length l i = \u2308log 2 q i (x i ) [\u2309][. A prefix-free code with this length for][ x] [i] [ is guaranteed to exist]\nsince this choice of lengths satisfies the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1 LLaMA+TbyT,\nis given by\n\n\n\nN T\n\ufffd\n\n\ni=1\n\n\n\n1\nlog 2\n\ufffd q i (x i )\n\n\n\n\ufffd\n\n\n\n\u03c1 LLaMA+TbyT =\n\n\n\n\ufffd Ni=1 T [b] [i] .\n\n\n\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use q i (x i ) as the probability of token x i at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1 LLM+AC . It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\n\n\n\u03c1 LLM+AC \u2248\n\n\n\nN T 1\n\ufffd i=1 log 2 q i (x i )\n\n\ufffd Ni=1 T [b] [i] . (7)\n\n\n\nClearly, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and \u03c1 LLM+AC also provide upper bounds on H(S). H ub, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and\n\u03c1 LLM+AC are estimated using a finite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\n\n\nIV. R ESULTS\n\n\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it difficult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\n[Nevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of](http://mattmahoney.net/dc/text8.zip)\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M ) is shown in Table II. As expected, the\ncompression performance improves with increasing M . We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n\n\n6\n\n\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\n\n\nV. A CKNOWLEDGEMENT\n\n\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\n\n\nTABLE I\n\nR ESULTS FOR 1MB OF TEXT FROM TEXT 8 DATASET\n\n\n\nBatch\nN c N T\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC ZPAQ pq8h\n\n\n\nNo. (bpc) file size (bits) (bpc) (bpc) (bpc) (bpc)\n\n\n1 466, 650 100, 000 0.6882 1.0513 0.8215 0.689\n\n\n2 461, 477 100, 000 0.6893 1.0558 0.8242 0.6901\n\n\n3 454, 599 100, 000 0.699 1.0681 0.8357 0.6999\n\n\n4 462, 755 100, 000 0.6748 1.0346 0.8093 0.6757\n\n\n5 453, 847 100, 000 0.7481 1.1265 0.8831 0.749\n\n\n6 458, 252 100, 000 0.7218 1.0957 0.8567 0.7227\n\n\n7 451, 036 100, 000 0.6959 1.0729 0.8353 0.6968\n\n\n8 447, 953 100, 000 0.7092 1.0896 0.8489 0.7101\n\n\n9 462, 665 100, 000 0.7394 1.1126 0.8713 0.7402\n\n\n10 449, 621 100, 000 0.7269 1.1046 0.8643 0.7277\n\n\nTotal 9, 137, 710 2, 000, 000 0.7093 1.0812 0.845 0.7101 1.4 [1] 1.2 [2]\n\n\nTABLE II\n\nC OMPRESSION PERFORMANCE OF THE LLM ON THE TEXT 8 DATASET, AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\n\n\n(bpc) file size (bits) (bpc) (bpc)\n\n\n\n31 4, 568, 855 1, 000, 000 0.9139 1.3159 1.0425 0.9145\n\n\n127 4, 568, 855 1, 000, 000 0.7511 1.1303 0.8847 0.752\n\n\n255 4, 568, 855 1, 000, 000 0.7242 1.0985 0.859 0.725\n\n\n511 4, 568, 855 1, 000, 000 0.7093 1.0812 0.845 0.7101\n\n\n1 This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2 This result is taken from [12] and it corresponds to the full 100MB dataset text8\n\n\n7\n\n\n\nTABLE III\n\nM EAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\nM\n\n(bpc) (bpc) (bpc) (bpc)\n\n\n31 0.9139 \u00b1 0.0263 1.3159 \u00b1 0.0329 1.0425 \u00b1 0.0262 0.9145 \u00b1 0.0263\n\n\n127 0.7511 \u00b1 0.0233 1.1303 \u00b1 0.0292 0.8847 \u00b1 0.0231 0.752 \u00b1 0.0233\n\n\n255 0.7242 \u00b1 0.0234 1.0985 \u00b1 0.0289 0.859 \u00b1 0.0232 0.725 \u00b1 0.0234\n\n\n511 0.7093 \u00b1 0.0228 1.0812 \u00b1 0.028 0.845 \u00b1 0.0226 0.7101 \u00b1 0.0228\n\n\nTABLE IV\n\nC OMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN P ROJECT G UTENBERG [7], AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC Standalone Zlib\n\n\n(bpc) (bpc) (bpc) (bpc) (bpc)\n\n\n\n31 508, 463 115, 000 1.0919 1.5316 1.2152 1.0924 2.80\n\n\n127 508, 463 115, 000 0.8973 1.3128 1.0235 0.8982 2.80\n\n\n255 508, 463 115, 000 0.8618 1.2684 0.9899 0.8627 2.80\n\n\n511 508, 463 115, 000 0.8417 1.2465 0.9711 0.8426 2.80\n\n\n8\n\n\nR EFERENCES\n\n\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR filters),\u201d 2023.\n\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and efficient foundation language models,\u201d\n2023.\n\n[7] J. Frank Dobie, Legends of Texas, United States, Texas Folk-Lore Society, 1924; Project Gutenberg, May 25, 2023, 2023,\n[https://www.gutenberg.org/ebooks/70859.](https://www.gutenberg.org/ebooks/70859)\n\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n\n\n"}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Final Answer:[00m [92m
## Synopsis of LLMZip: Lossless Text Compression using Large Language Models

**II. Intuitive Explanation of the Main Idea:** The core idea is to leverage large language models (LLMs) for lossless text compression by predicting the next token in a sequence. Instead of directly compressing the text, the LLM generates a rank-ordered list of possible next tokens and their probabilities. The rank of the actual next token in this list is then compressed using a standard lossless compression algorithm (e.g., zip, arithmetic coding). During decompression, the same LLM is used to reconstruct the token sequence by predicting and selecting tokens based on the decompressed ranks. Better compression can be achieved by directly using the probabilities produced by the LLM along with arithmetic coding.

**III. Compression Using LLMs:**
*   **Definitions:**
    *   `s`: A sentence from the English language composed of `N_c` letters.
    *   `X = [1, D]`: A dictionary of `D` tokens.
    *   `x = x_1, x_2, ..., x_{N_T}`: A sequence of `N_T` tokens parsed from `s`.
    *   `q_i(x_i) := Pr(X_i = x_i | x_{i-1}, x_{i-2}, ..., x_{i-M})`: The probability mass function for the next token `x_i` conditioned on the past `M` tokens, produced by a language model with memory `M`.
    *   `r_i := γ_i(x_i)`: The rank of the input token `x_i` at epoch `i` in the sorted list of probabilities.
    *   `ρ := N_b / N_c` bits/character: The compression ratio, where `N_b` is the total number of bits after compression.
    *   `B_i`: A random variable identifying the number of characters contained in the `i`-th token.
    *   `H_ub`: The asymptotic upper bound on the entropy rate `H(S)`.
*   **Equations:**
    *   `(1) m_i(s) = min n {length (T (s_n)) >= i}`: Defines `m_i(s)` as the (time) index during which the tokenizer outputs its `i`-th token for an input sequence `s`.
    *   `b_i = m_i(s) - m_{i-1}(s)`: Expresses the number of characters contained in a token `i`.
    *   `H(S) = H(X) / E[B]` (almost surely): Establishes the relationship between the entropy of the character sequence `H(S)` and the entropy of the token sequence `H(X)`, scaled by the expected number of characters per token `E[B]`.
    *   `(5) H(X) <= lim_{N_T -> inf} (-1/N_T) sum_{i=1}^{N_T} log_2 q_i(X_i) = 1`: An upper bound on the entropy of the token sequence `H(X)`.
    *   `(6) H(S) <= lim_{N_T -> inf} (-1/N_T) sum_{i=1}^{N_T} log_2 q_i(X_i) / E[B]`: The asymptotic upper bound on the entropy rate `H(S)`, denoted as `H_ub`.
    *   `ρ_LLaMA+TbyT = (sum_{i=1}^{N_T} ceil(log_2(1/q_i(x_i)))) / (sum_{i=1}^{N_T} b_i)`: The compression ratio for the Token-by-Token Compression scheme.
    *   `(7) ρ_LLM+AC ≈ (sum_{i=1}^{N_T} log_2(1/q_i(x_i))) / (sum_{i=1}^{N_T} b_i)`: The expected compression ratio for the Arithmetic Coding scheme, which is nearly optimal.
*   **Encoding Schemes:**
    1.  **LLaMA+zlib:** Compresses the sequence of ranks `r_i` using the zlib algorithm.
    2.  **LLaMA+TbyT (Token-by-Token Compression):** Uses a time-varying prefix-free code, where the codeword length for token `x_i` is `ceil(log_2(1/q_i(x_i)))`.
    3.  **LLaMA+AC (Arithmetic Coding):** Employs arithmetic coding, using `q_i(x_i)` as the probability of token `x_i` at time `i`.

**IV. Results:**
*   **LLM and Tokenizer Used:** LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11], which produces a dictionary of size 32000.
> Stop watching for updates
*   **Datasets:**
    *   A 1MB section of the `text8` dataset (available from `http://mattmahoney.net/dc/text8.zip`).
    *   A 100KB section of a recently released book [7] (J. Frank Dobie, Legends of Texas, Project Gutenberg, May 25, 2023).
*   **Baselines:** State-of-the-art text compression schemes: ZPAQ (reported 1.4 bits/character on text8 [5]) and paq8h (reported 1.2 bits/character on text8 [12]). Standalone zlib was also run on the Project Gutenberg text, yielding 2.8 bits/character.
*   **Key Findings (text8 dataset, 1MB section, M=511):**
    *   Asymptotic upper bound on entropy (H_ub): 0.7093 bits/character.
    *   LLaMA+AC compression ratio: 0.7101 bits/character.
    *   LLaMA+zlib compression ratio: 1.0812 bits/character.
    *   LLaMA+TbyT compression ratio: 0.845 bits/character.
    *   LLaMA-7B based schemes (especially LLaMA+AC) substantially outperform ZPAQ and paq8h.
*   **Key Findings (Project Gutenberg book, 100KB section, M=511):**
    *   Asymptotic upper bound on entropy (H_ub): 0.8417 bits/character.
    *   LLaMA+AC compression ratio: 0.8426 bits/character.
    *   LLaMA+zlib compression ratio: 1.2465 bits/character.
    *   LLaMA+TbyT compression ratio: 0.9711 bits/character.
    *   LLaMA-7B based compressors are significantly better than standalone zlib (2.8 bpc).
*   **Memory Dependence:** Compression performance improves with increasing LLM memory (M). Inference time scales approximately linearly with input memory length.
*   **Caveats:** The `text8` dataset results might be optimistic because LLaMA-7B was likely trained on a corpus including Wikipedia articles, from which `text8` is derived. Comparisons with state-of-the-art algorithms were made using different input sizes (1MB/100KB for LLMZip vs. 100MB for baselines).

**V. Acknowledgement:** The authors acknowledge Andreas Kirsch for email discussions about arithmetic coding.

**References:** The paper cites 12 references, including foundational works by Shannon (1951) [3] and Cover and King (1978) [1] on English entropy estimation, information theory texts by Cover and Thomas (1999) [8] and MacKay (2003) [10], and works on data compression using adaptive coding (Cleary and Witten, 1984 [4]; Bell et al., 1989 [9]) and recurrent neural networks (Goyal et al., 2018 [5]). Recent works include LLaMA (Touvron et al., 2023 [6]), SentencePiece (Kudo and Richardson, 2018 [11]), and a 2023 paper by Lutati et al. [2]. The Project Gutenberg book [7] and `text8` results [12] are also referenced.[00m


🚀 Crew: crew
├── 📋 Task: 0a388a52-bfff-458b-b4bc-80efbd19e943
│   Assigned to: AI Assistant
│   Status: ✅ Completed
│   ├── ✅ Reasoning Completed
│   ├── 🔧 Used download_paper (1)
│   └── 🔧 Used read_paper (1)
├── 🔧 Using create_reasoning_plan from LLM available_function
├── 🔄 Tool Usage Started
├── 🔧 Using create_reasoning_plan from LLM available_function
└── ✅ Tool Usage Completed┌────────────────────────────── Task Completion ──────────────────────────────┐
│                                                                             │
│  Task Completed                                                             │
│  Name: 0a388a52-bfff-458b-b4bc-80efbd19e943                                 │
│  Agent: AI Assistant                                                        │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

┌────────────────────────────── Crew Completion ──────────────────────────────┐
│                                                                             │
│  Crew Execution Completed                                                   │
│  Name: crew                                                                 │
│  ID: 086d44ac-2b4c-4903-9e62-ef9c6c100bd1                                   │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

AGENT_RESULT_JSON:{"final_text": "## Synopsis of LLMZip: Lossless Text Compression using Large Language Models\n\n**II. Intuitive Explanation of the Main Idea:** The core idea is to leverage large language models (LLMs) for lossless text compression by predicting the next token in a sequence. Instead of directly compressing the text, the LLM generates a rank-ordered list of possible next tokens and their probabilities. The rank of the actual next token in this list is then compressed using a standard lossless compression algorithm (e.g., zip, arithmetic coding). During decompression, the same LLM is used to reconstruct the token sequence by predicting and selecting tokens based on the decompressed ranks. Better compression can be achieved by directly using the probabilities produced by the LLM along with arithmetic coding.\n\n**III. Compression Using LLMs:**\n*   **Definitions:**\n    *   `s`: A sentence from the English language composed of `N_c` letters.\n    *   `X = [1, D]`: A dictionary of `D` tokens.\n    *   `x = x_1, x_2, ..., x_{N_T}`: A sequence of `N_T` tokens parsed from `s`.\n    *   `q_i(x_i) := Pr(X_i = x_i | x_{i-1}, x_{i-2}, ..., x_{i-M})`: The probability mass function for the next token `x_i` conditioned on the past `M` tokens, produced by a language model with memory `M`.\n    *   `r_i := \u03b3_i(x_i)`: The rank of the input token `x_i` at epoch `i` in the sorted list of probabilities.\n    *   `\u03c1 := N_b / N_c` bits/character: The compression ratio, where `N_b` is the total number of bits after compression.\n    *   `B_i`: A random variable identifying the number of characters contained in the `i`-th token.\n    *   `H_ub`: The asymptotic upper bound on the entropy rate `H(S)`.\n*   **Equations:**\n    *   `(1) m_i(s) = min n {length (T (s_n)) >= i}`: Defines `m_i(s)` as the (time) index during which the tokenizer outputs its `i`-th token for an input sequence `s`.\n    *   `b_i = m_i(s) - m_{i-1}(s)`: Expresses the number of characters contained in a token `i`.\n    *   `H(S) = H(X) / E[B]` (almost surely): Establishes the relationship between the entropy of the character sequence `H(S)` and the entropy of the token sequence `H(X)`, scaled by the expected number of characters per token `E[B]`.\n    *   `(5) H(X) <= lim_{N_T -> inf} (-1/N_T) sum_{i=1}^{N_T} log_2 q_i(X_i) = 1`: An upper bound on the entropy of the token sequence `H(X)`.\n    *   `(6) H(S) <= lim_{N_T -> inf} (-1/N_T) sum_{i=1}^{N_T} log_2 q_i(X_i) / E[B]`: The asymptotic upper bound on the entropy rate `H(S)`, denoted as `H_ub`.\n    *   `\u03c1_LLaMA+TbyT = (sum_{i=1}^{N_T} ceil(log_2(1/q_i(x_i)))) / (sum_{i=1}^{N_T} b_i)`: The compression ratio for the Token-by-Token Compression scheme.\n    *   `(7) \u03c1_LLM+AC \u2248 (sum_{i=1}^{N_T} log_2(1/q_i(x_i))) / (sum_{i=1}^{N_T} b_i)`: The expected compression ratio for the Arithmetic Coding scheme, which is nearly optimal.\n*   **Encoding Schemes:**\n    1.  **LLaMA+zlib:** Compresses the sequence of ranks `r_i` using the zlib algorithm.\n    2.  **LLaMA+TbyT (Token-by-Token Compression):** Uses a time-varying prefix-free code, where the codeword length for token `x_i` is `ceil(log_2(1/q_i(x_i)))`.\n    3.  **LLaMA+AC (Arithmetic Coding):** Employs arithmetic coding, using `q_i(x_i)` as the probability of token `x_i` at time `i`.\n\n**IV. Results:**\n*   **LLM and Tokenizer Used:** LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11], which produces a dictionary of size 32000.\n*   **Datasets:**\n    *   A 1MB section of the `text8` dataset (available from `http://mattmahoney.net/dc/text8.zip`).\n    *   A 100KB section of a recently released book [7] (J. Frank Dobie, Legends of Texas, Project Gutenberg, May 25, 2023).\n*   **Baselines:** State-of-the-art text compression schemes: ZPAQ (reported 1.4 bits/character on text8 [5]) and paq8h (reported 1.2 bits/character on text8 [12]). Standalone zlib was also run on the Project Gutenberg text, yielding 2.8 bits/character.\n*   **Key Findings (text8 dataset, 1MB section, M=511):**\n    *   Asymptotic upper bound on entropy (H_ub): 0.7093 bits/character.\n    *   LLaMA+AC compression ratio: 0.7101 bits/character.\n    *   LLaMA+zlib compression ratio: 1.0812 bits/character.\n    *   LLaMA+TbyT compression ratio: 0.845 bits/character.\n    *   LLaMA-7B based schemes (especially LLaMA+AC) substantially outperform ZPAQ and paq8h.\n*   **Key Findings (Project Gutenberg book, 100KB section, M=511):**\n    *   Asymptotic upper bound on entropy (H_ub): 0.8417 bits/character.\n    *   LLaMA+AC compression ratio: 0.8426 bits/character.\n    *   LLaMA+zlib compression ratio: 1.2465 bits/character.\n    *   LLaMA+TbyT compression ratio: 0.9711 bits/character.\n    *   LLaMA-7B based compressors are significantly better than standalone zlib (2.8 bpc).\n*   **Memory Dependence:** Compression performance improves with increasing LLM memory (M). Inference time scales approximately linearly with input memory length.\n*   **Caveats:** The `text8` dataset results might be optimistic because LLaMA-7B was likely trained on a corpus including Wikipedia articles, from which `text8` is derived. Comparisons with state-of-the-art algorithms were made using different input sizes (1MB/100KB for LLMZip vs. 100MB for baselines).\n\n**V. Acknowledgement:** The authors acknowledge Andreas Kirsch for email discussions about arithmetic coding.\n\n**References:** The paper cites 12 references, including foundational works by Shannon (1951) [3] and Cover and King (1978) [1] on English entropy estimation, information theory texts by Cover and Thomas (1999) [8] and MacKay (2003) [10], and works on data compression using adaptive coding (Cleary and Witten, 1984 [4]; Bell et al., 1989 [9]) and recurrent neural networks (Goyal et al., 2018 [5]). Recent works include LLaMA (Touvron et al., 2023 [6]), SentencePiece (Kudo and Richardson, 2018 [11]), and a 2023 paper by Lutati et al. [2]. The Project Gutenberg book [7] and `text8` results [12] are also referenced.", "duration_ms": 37766}
## Synopsis of LLMZip: Lossless Text Compression using Large Language Models

**II. Intuitive Explanation of the Main Idea:** The core idea is to leverage large language models (LLMs) for lossless text compression by predicting the next token in a sequence. Instead of directly compressing the text, the LLM generates a rank-ordered list of possible next tokens and their probabilities. The rank of the actual next token in this list is then compressed using a standard lossless compression algorithm (e.g., zip, arithmetic coding). During decompression, the same LLM is used to reconstruct the token sequence by predicting and selecting tokens based on the decompressed ranks. Better compression can be achieved by directly using the probabilities produced by the LLM along with arithmetic coding.

**III. Compression Using LLMs:**
*   **Definitions:**
    *   `s`: A sentence from the English language composed of `N_c` letters.
    *   `X = [1, D]`: A dictionary of `D` tokens.
    *   `x = x_1, x_2, ..., x_{N_T}`: A sequence of `N_T` tokens parsed from `s`.
    *   `q_i(x_i) := Pr(X_i = x_i | x_{i-1}, x_{i-2}, ..., x_{i-M})`: The probability mass function for the next token `x_i` conditioned on the past `M` tokens, produced by a language model with memory `M`.
    *   `r_i := γ_i(x_i)`: The rank of the input token `x_i` at epoch `i` in the sorted list of probabilities.
    *   `ρ := N_b / N_c` bits/character: The compression ratio, where `N_b` is the total number of bits after compression.
    *   `B_i`: A random variable identifying the number of characters contained in the `i`-th token.
    *   `H_ub`: The asymptotic upper bound on the entropy rate `H(S)`.
*   **Equations:**
    *   `(1) m_i(s) = min n {length (T (s_n)) >= i}`: Defines `m_i(s)` as the (time) index during which the tokenizer outputs its `i`-th token for an input sequence `s`.
    *   `b_i = m_i(s) - m_{i-1}(s)`: Expresses the number of characters contained in a token `i`.
    *   `H(S) = H(X) / E[B]` (almost surely): Establishes the relationship between the entropy of the character sequence `H(S)` and the entropy of the token sequence `H(X)`, scaled by the expected number of characters per token `E[B]`.
    *   `(5) H(X) <= lim_{N_T -> inf} (-1/N_T) sum_{i=1}^{N_T} log_2 q_i(X_i) = 1`: An upper bound on the entropy of the token sequence `H(X)`.
    *   `(6) H(S) <= lim_{N_T -> inf} (-1/N_T) sum_{i=1}^{N_T} log_2 q_i(X_i) / E[B]`: The asymptotic upper bound on the entropy rate `H(S)`, denoted as `H_ub`.
    *   `ρ_LLaMA+TbyT = (sum_{i=1}^{N_T} ceil(log_2(1/q_i(x_i)))) / (sum_{i=1}^{N_T} b_i)`: The compression ratio for the Token-by-Token Compression scheme.
    *   `(7) ρ_LLM+AC ≈ (sum_{i=1}^{N_T} log_2(1/q_i(x_i))) / (sum_{i=1}^{N_T} b_i)`: The expected compression ratio for the Arithmetic Coding scheme, which is nearly optimal.
*   **Encoding Schemes:**
    1.  **LLaMA+zlib:** Compresses the sequence of ranks `r_i` using the zlib algorithm.
    2.  **LLaMA+TbyT (Token-by-Token Compression):** Uses a time-varying prefix-free code, where the codeword length for token `x_i` is `ceil(log_2(1/q_i(x_i)))`.
    3.  **LLaMA+AC (Arithmetic Coding):** Employs arithmetic coding, using `q_i(x_i)` as the probability of token `x_i` at time `i`.

**IV. Results:**
*   **LLM and Tokenizer Used:** LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11], which produces a dictionary of size 32000.
*   **Datasets:**
    *   A 1MB section of the `text8` dataset (available from `http://mattmahoney.net/dc/text8.zip`).
    *   A 100KB section of a recently released book [7] (J. Frank Dobie, Legends of Texas, Project Gutenberg, May 25, 2023).
*   **Baselines:** State-of-the-art text compression schemes: ZPAQ (reported 1.4 bits/character on text8 [5]) and paq8h (reported 1.2 bits/character on text8 [12]). Standalone zlib was also run on the Project Gutenberg text, yielding 2.8 bits/character.
*   **Key Findings (text8 dataset, 1MB section, M=511):**
    *   Asymptotic upper bound on entropy (H_ub): 0.7093 bits/character.
    *   LLaMA+AC compression ratio: 0.7101 bits/character.
    *   LLaMA+zlib compression ratio: 1.0812 bits/character.
    *   LLaMA+TbyT compression ratio: 0.845 bits/character.
    *   LLaMA-7B based schemes (especially LLaMA+AC) substantially outperform ZPAQ and paq8h.
*   **Key Findings (Project Gutenberg book, 100KB section, M=511):**
    *   Asymptotic upper bound on entropy (H_ub): 0.8417 bits/character.
    *   LLaMA+AC compression ratio: 0.8426 bits/character.
    *   LLaMA+zlib compression ratio: 1.2465 bits/character.
    *   LLaMA+TbyT compression ratio: 0.9711 bits/character.
    *   LLaMA-7B based compressors are significantly better than standalone zlib (2.8 bpc).
*   **Memory Dependence:** Compression performance improves with increasing LLM memory (M). Inference time scales approximately linearly with input memory length.
*   **Caveats:** The `text8` dataset results might be optimistic because LLaMA-7B was likely trained on a corpus including Wikipedia articles, from which `text8` is derived. Comparisons with state-of-the-art algorithms were made using different input sizes (1MB/100KB for LLMZip vs. 100MB for baselines).

**V. Acknowledgement:** The authors acknowledge Andreas Kirsch for email discussions about arithmetic coding.

**References:** The paper cites 12 references, including foundational works by Shannon (1951) [3] and Cover and King (1978) [1] on English entropy estimation, information theory texts by Cover and Thomas (1999) [8] and MacKay (2003) [10], and works on data compression using adaptive coding (Cleary and Witten, 1984 [4]; Bell et al., 1989 [9]) and recurrent neural networks (Goyal et al., 2018 [5]). Recent works include LLaMA (Touvron et al., 2023 [6]), SentencePiece (Kudo and Richardson, 2018 [11]), and a 2023 paper by Lutati et al. [2]. The Project Gutenberg book [7] and `text8` results [12] are also referenced.
C:\tools\Anaconda3\Lib\site-packages\paramiko\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "cipher": algorithms.TripleDES,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.
  "class": algorithms.Blowfish,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "class": algorithms.TripleDES,
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\_internal\_generate_schema.py:628: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.
  warn(
- Reading configuration...
  - Reading registry from registry.yaml
  - Reading catalog from docker-mcp.yaml
  - Reading config from config.yaml
- Configuration read in 19.7695ms
- Watching registry at C:\Users\Noah\.docker\mcp\registry.yaml
- Watching config at C:\Users\Noah\.docker\mcp\config.yaml
- Those servers are enabled: arxiv-mcp-server, duckduckgo, wikipedia-mcp
- Using images:
  - mcp/arxiv-mcp-server@sha256:6dc6bba6dfed97f4ad6eb8d23a5c98ef5b7fa6184937d54b2d675801cd9dd29e
  - mcp/duckduckgo@sha256:68eb20db6109f5c312a695fc5ec3386ad15d93ffb765a0b4eb1baf4328dec14f
  - mcp/wikipedia-mcp@sha256:e90e3f6e3bb795f10d3d8d8ac1523f6d46d4f2ab5ac3b3a4720e1be879466472
> Images pulled in 62.4211ms
- Listing MCP tools...
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
  - Running mcp/duckduckgo with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=duckduckgo -l docker-mcp-transport=stdio]
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers:ro]
  > duckduckgo: (2 tools)
  > wikipedia-mcp: (10 tools) (9 resourceTemplates)
  > arxiv-mcp-server: (4 tools) (1 prompts)
> 16 tools listed in 3.0335592s
- Watching for configuration updates...
> Initialized in 3.1157498s
> Start stdio server
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\fields.py:1093: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'items', 'anyOf', 'enum', 'properties'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
  warn(
┌────────────────────────── Crew Execution Started ───────────────────────────┐
│                                                                             │
│  Crew Execution Started                                                     │
│  Name: crew                                                                 │
│  ID: 61317ccf-5d77-4e0a-9b5b-e45518aacc98                                   │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

🚀 Crew: crew
├── 📋 Task: 81f75889-bbc9-4f00-9326-8e6a0d86d45a
│   Status: Executing Task...
│   └── ✅ Reasoning Completed
├── 🔧 Using create_reasoning_plan from LLM available_function
├── 🔄 Tool Usage Started
├── 🔧 Using create_reasoning_plan from LLM available_function
└── ✅ Tool Usage Completed┌───────────────────────────── 🧠 Reasoning Plan ─────────────────────────────┐
│                                                                             │
│  1. **Identify the document:** The document is "LLMZip: Lossless Text       │
│  Compression using Large Language Models" (arXiv: 2306.04050v2).            │
│  2. **Search for the full document:** Use `search_papers` with the title    │
│  and arXiv ID to locate the paper.                                          │
│  3. **Retrieve the document content:** Use `read_paper` with the obtained   │
│  `paper_id` to access the full text of the paper.                           │
│  4. **Analyze the document structure:** Use `get_sections` to identify the  │
│  main sections of the paper, focusing on those that would logically follow  │
│  the Abstract and Introduction (e.g., Methodology, Experiments, Results,    │
│  Discussion, Conclusion, References).                                       │
│  5. **Extract relevant information from later sections:** For each          │
│  identified relevant section, use `extract_key_facts` to retrieve key       │
│  facts, equations, definitions, datasets, and references.                   │
│  6. **Synthesize the synopsis:** Combine the extracted information into a   │
│  concise textual synopsis, ensuring it is relevant to the document,         │
│  factual, and avoids hallucination. The synopsis will cover the LLMZip      │
│  algorithm, experimental setup, results, discussion, and any significant    │
│  definitions or equations.                                                  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool download_paper with arguments: {"check_status":false,"paper_id":"2306.04050"}
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
  > Calling tool download_paper took: 1.8264353s
  - Scanning tool call response for secrets...
  > No secret found in response.
[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Task:[00m [92mYou act as a deterministic research agent. Given the observed prefix of a document, use available MCP tools to retrieve information that is likely to appear later in the same document. Prefer authoritative sources. Produce a concise textual synopsis of the likely future content (facts, equations, definitions, datasets, references), suitable for conditioning a language model compressor. Maximize relevance to the document. Do not hallucinate; only include content supported by the tools.

Document prefix (UTF-8 text):
# LLMZip: Lossless Text Compression using Large Language Models

**Authors:** Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas Shakkottai  
**Affiliation:** Department of Electrical and Computer Engineering, Texas A&M University  
**Email:** {vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu  
**arXiv:** 2306.04050v2 [cs.IT] 26 Jun 2023

## Abstract

We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.

## I. INTRODUCTION

There are close connections between learning, prediction, and compression. The success of ChatGPT has captured the fascination of general public and brought the connection between learning and prediction to the fore. The main advance brought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in a paragraph based on knowing the past several words (tokens).

The connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the entropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past values can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many algorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of lossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very effective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor and every time there

Output: A concise synopsis (plain text).

Reasoning Plan:
1. **Identify the document:** The document is "LLMZip: Lossless Text Compression using Large Language Models" (arXiv: 2306.04050v2).
2. **Search for the full document:** Use `search_papers` with the title and arXiv ID to locate the paper.
3. **Retrieve the document content:** Use `read_paper` with the obtained `paper_id` to access the full text of the paper.
4. **Analyze the document structure:** Use `get_sections` to identify the main sections of the paper, focusing on those that would logically follow the Abstract and Introduction (e.g., Methodology, Experiments, Results, Discussion, Conclusion, References).
5. **Extract relevant information from later sections:** For each identified relevant section, use `extract_key_facts` to retrieve key facts, equations, definitions, datasets, and references.
6. **Synthesize the synopsis:** Combine the extracted information into a concise textual synopsis, ensuring it is relevant to the document, factual, and avoids hallucination. The synopsis will cover the LLMZip algorithm, experimental setup, results, discussion, and any significant definitions or equations.[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mAction: download_paper[00m
[95m## Using tool:[00m [92mdownload_paper[00m
[95m## Tool Input:[00m [92m
"{\"paper_id\": \"2306.04050\"}"[00m
[95m## Tool Output:[00m [92m
  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool get_sections with arguments: {"title":"LLMZip: Lossless Text Compression using Large Language Models"}
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
  > Calling tool get_sections took: 2.2550748s
  - Scanning tool call response for secrets...
  > No secret found in response.
{"status": "success", "message": "Paper already available", "resource_uri": "file:///app/papers/2306.04050.md", "pdf_uri": "https://arxiv.org/pdf/2306.04050.pdf", "content": "1\n\n# LLMZip: Lossless Text Compression using Large Language Models\n\n\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\n\n\nAbstract\n\n\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\n\n\nI. I NTRODUCTION\n\n\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\n\n\nII. I NTUITIVE EXPLANATION OF THE MAIN IDEA\n\n\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\n\nMy first attempt at writing a book\n\n\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032, \u2032 a \u2032, \u2032 book \u2032\n\n\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n\n\n2\n\n\nFigure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be\nwords such as [\u2032] reading [\u2032], [\u2032] writing [\u2032], [\u2032] driving [\u2032], [\u2032] cooking [\u2032] etc. The main idea is to compute the rank of the actual word in\nour sentence ( [\u2032] writing [\u2032] ) in this list and call it R 5 . We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for [\u2032] writing [\u2032] is R 5 = 1.\n\n\nTokens\n\n\n\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x5|x4 1)<br>5|Rank<br>computation|R5 = 1|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nreading 0.3\nwriting 0.2\ncycling 0.1\ndriving 0.05\n\n... ...\n\n\n\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\n\n\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, [\u2032] a [\u2032], and hence, the rank R 6 would be 0.\n\n\nTokens\n\n\n\n\n\n\u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x6|x5 2)<br>6|Rank<br>computation|R6 = 0|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nan 0.2\n\nthe 0.05\n\n... ...\n\n\n\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\n\n\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n\n\n1, 0, 0, . . .\n\n\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\n\n\nSequence of ranks\n\n|e of ranks|Col2|Col3|\n|---|---|---|\n|. ., rNT|Compression (zip)|Nb|\n||||\n\n\n\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\n\n\nWhen we wish to reconstruct the sequence, we first decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank R i during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\n\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n\n\n3\n\n\nIII. C OMPRESSION USING LLM S\n\n\nLet s denote a sentence from the English language composed of N c letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We first parse s into a sequence of N T tokens\ndenoted by x = x 1, x 2, . . ., x i\u22121, x i, x i+1, . . . x N T, where x i \u2208X . There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. x i \u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter X i .\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens x i\u2212M, x i\u2212M+1, . . ., x i\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nq i (x i ) := Pr(X i = x i |x i\u22121, x i\u22122, . . ., x i\u2212M ), \u2200x i \u2208X . The PMF vector q i := [q i (1), q i (2), . . ., q i (D)] [T] is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcq i . Let \u03b3 i : X \u2192X be a permutation on the integers\nfrom 1 to D such that\n\n\u02dc\nq i (\u03b3 i (j)) = q i (j), \u2200j \u2208X .\n\n\nThat is, \u03b3 i (j) is the rank of the token j at epoch i. We define the rank of the input sequence at epoch i as the rank of the\ntoken x i at epoch i, r i := \u03b3 i (x i ). The sequence {r i } [N] i=1 [T] [is compressed by a lossless compression algorithm (such as zlib) to]\nproduce N b bits which are the final bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors q i \u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 defined as\n\n\u03c1 := [N] [b] bits/character.\n\nN c\n\n\n\nLosseless\n\nCompression\n\n\n\nTokens . . ., x i\u2212M, . . ., x i\u22121\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n\nInput sentence s with N c characters\n\n\n\ni i i\u22121 Rank\n\nLLM\ncomputation\n\n\n\nq i = Pr(X i = j|x [i] i [\u2212] \u2212 [M] 1 [)] Rank r i Losseless N b bits\n\n\n\nFig. 4. Schematic showing the prediction at epoch i.\n\n\nA. Entropy bounds\n\n\nLet S \u2208S [\u221e] be a random process that represents language input. The nth character in the sequence is denoted by S n,\nwhereas the string of characters from the beginning to the nth character is expressed as S n . The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X 1, X 2, . . . using a variable-length mapping. In this sequence, X i is the ith\ntoken. The number of characters employed to generate X i depends on the realization of the random process and, as such,\nwe introduce random variable B i to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which B i \u2265 1 and B i is uniformly bounded, with B i < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded N T \u2192\u221e, the number of characters must also approach infinity N c \u2192\u221e. Formally, consider the tokenizer\nfunction T : S [N] \u2192X [N] operating on infinite symbol sequences; that is, T (s) = x where s is an infinite sequence in S [\u221e] . For\nnatural number, i \u2208 N, define m i : S [N] \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Specifically, suppose s is given, then\n\n\nm i (s) = min n [{][length (][T][ (][s] [n] [))][ \u2265] [i][}][ .] (1)\n\n\nWe note that, by construction, lim n\u2192\u221e length (T (s n )) = \u221e and, as such, m i (\u00b7) is well-defined. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple finite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (s n ) = T (s n+1 ). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (finite) series of input symbols in the restricted\nset for which T (y n ) = x i is s m i (s) . Given a fixed sequence s, we can express the number of characters contained in a token\n\nas\nb i = m i (s) \u2212 m i\u22121 (s)\n\n\n4\n\n\nwith initial condition m \u22121 = 0. Consequently, the number of characters embedded in the first N T tokens for a random input\nbecomes N c = [\ufffd] i [N] =1 [T] [B] [i] [.]\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {S k } [\u221e] k=1 [,]\n{B i } [\u221e] i=1 [, and][ {][X] [i] [}] [\u221e] i=1 [are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that]\n\n\n\n\u2212 [1]\n\n\n\n\n[1]\n\nn [log] [2] [ p] [S] [n] [(][S] [1] [, . . ., S] [n] [) =][ \u2212] n [1]\n\n\n\nn [log] [2] [ p] [S] [n] [(][S] [n] [)][ \u2192] [H][(][S][)] almost surely. (2)\n\n\n\nLet \u2126 S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\n\n\n\n[1]\n\ni [log] [2] [ p] [X] [i] [(][X] [1] [, . . ., X] [i] [) =][ \u2212] [1] i\n\n\n\n\u2212 [1]\n\n\n\ni [log] [2] [ p] [X] [i] [(][X] [i] [)][ \u2192] [H][(][X][)] almost surely. (3)\n\n\n\nDefine \u2126 X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\n\n\nm i (S)\nlim = E [B] almost surely. (4)\ni\u2192\u221e i\n\n\nSet \u2126 B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126 S \u2229 \u2126 X \u2229 \u2126 B, we deduce that\n\n\nH(S) = lim\nk\u2192\u221e [\u2212] k [1] [log] [2] [ p] [S] [k] [(][S] [k] [(][\u03c9][))]\n\n= lim i\u2192\u221e [\u2212] l [1] i log 2 p S li (S l i (\u03c9))\n\n\n= lim log 2 Pr (X i = T (S l i (\u03c9)))\ni\u2192\u221e [\u2212] l [1] i\n\n\n\n1\n\ni [log] [2] [ Pr (][X] [i] [ =][ x] [i] [) =][ H] E [(] B [X][)]\n\n\n\n1 1\n\n= \u2212\nE[B] i [lim] \u2192\u221e i\n\n\n\nE[B] [.]\n\n\n\nThe first equality follows from (2). The second equality is a consequence of the fact that {l i = m i (S(\u03c9))|i \u2208 N} is an infinite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\n\ntwo events,\n{\u03c9 \u2208 \u2126|X i (\u03c9) = x i } = {\u03c9 \u2208 \u2126|T (S m i (S(\u03c9)) ) = x i }.\n\n\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126 B . The sets \u2126 S, \u2126 X, and \u2126 B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\n\nH(S) = [H][(][X][)] almost surely.\n\nE[B]\n\n\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\n\n\n\nPr\n\n\ufffd\n\n\n\n\ufffd\n\n\n\nH(X) \u2264 lim\nN T \u2192\u221e [\u2212] N [1] T\n\n\n\nN T\n\ufffd\n\n\n\n\ufffd log 2 q i (X i )\n\n\ni=1\n\n\n\n= 1, (5)\n\n\n\nwhere q i (\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\n\n\n\n\n[1] N T\n\nN T \ufffd i=1 [log] 2 [q] [i] [(][X] [i] [)]\n\n\n\nH(S) \u2264 lim N T \u2192\u221e \u2212 N [1]\n\n\n\n. (6)\nE[B]\n\n\n\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by H ub . The\nnumerator in (6) represents the average number of bits required to represent the tokens X N T and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\n\n\nB. Encoding schemes\n\n\nWe consider three schemes for the lossless compression block in Fig. 3.\n\n\n5\n\n\n1) Compressing the ranks using zlib: The first scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1 LLaMA+zlib .\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token x i at epoch i by using a prefix-free code assuming q i to be the true distribution of\nthe tokens. A natural choice for a prefix-free code is a Huffman code. Instead, for simplicity, we use a prefix-free code where\n1\nthe codeword for the token x i is of length l i = \u2308log 2 q i (x i ) [\u2309][. A prefix-free code with this length for][ x] [i] [ is guaranteed to exist]\nsince this choice of lengths satisfies the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1 LLaMA+TbyT,\nis given by\n\n\n\nN T\n\ufffd\n\n\ni=1\n\n\n\n1\nlog 2\n\ufffd q i (x i )\n\n\n\n\ufffd\n\n\n\n\u03c1 LLaMA+TbyT =\n\n\n\n\ufffd Ni=1 T [b] [i] .\n\n\n\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use q i (x i ) as the probability of token x i at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1 LLM+AC . It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\n\n\n\u03c1 LLM+AC \u2248\n\n\n\nN T 1\n\ufffd i=1 log 2 q i (x i )\n\n\ufffd Ni=1 T [b] [i] . (7)\n\n\n\nClearly, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and \u03c1 LLM+AC also provide upper bounds on H(S). H ub, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and\n\u03c1 LLM+AC are estimated using a finite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\n\n\nIV. R ESULTS\n\n\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it difficult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\n[Nevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of](http://mattmahoney.net/dc/text8.zip)\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M ) is shown in Table II. As expected, the\ncompression performance improves with increasing M . We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n\n\n6\n\n\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\n\n\nV. A CKNOWLEDGEMENT\n\n\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\n\n\nTABLE I\n\nR ESULTS FOR 1MB OF TEXT FROM TEXT 8 DATASET\n\n\n\nBatch\nN c N T\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC ZPAQ pq8h\n\n\n\nNo. (bpc) file size (bits) (bpc) (bpc) (bpc) (bpc)\n\n\n1 466, 650 100, 000 0.6882 1.0513 0.8215 0.689\n\n\n2 461, 477 100, 000 0.6893 1.0558 0.8242 0.6901\n\n\n3 454, 599 100, 000 0.699 1.0681 0.8357 0.6999\n\n\n4 462, 755 100, 000 0.6748 1.0346 0.8093 0.6757\n\n\n5 453, 847 100, 000 0.7481 1.1265 0.8831 0.749\n\n\n6 458, 252 100, 000 0.7218 1.0957 0.8567 0.7227\n\n\n7 451, 036 100, 000 0.6959 1.0729 0.8353 0.6968\n\n\n8 447, 953 100, 000 0.7092 1.0896 0.8489 0.7101\n\n\n9 462, 665 100, 000 0.7394 1.1126 0.8713 0.7402\n\n\n10 449, 621 100, 000 0.7269 1.1046 0.8643 0.7277\n\n\nTotal 9, 137, 710 2, 000, 000 0.7093 1.0812 0.845 0.7101 1.4 [1] 1.2 [2]\n\n\nTABLE II\n\nC OMPRESSION PERFORMANCE OF THE LLM ON THE TEXT 8 DATASET, AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\n\n\n(bpc) file size (bits) (bpc) (bpc)\n\n\n\n31 4, 568, 855 1, 000, 000 0.9139 1.3159 1.0425 0.9145\n\n\n127 4, 568, 855 1, 000, 000 0.7511 1.1303 0.8847 0.752\n\n\n255 4, 568, 855 1, 000, 000 0.7242 1.0985 0.859 0.725\n\n\n511 4, 568, 855 1, 000, 000 0.7093 1.0812 0.845 0.7101\n\n\n1 This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2 This result is taken from [12] and it corresponds to the full 100MB dataset text8\n\n\n7\n\n\n\nTABLE III\n\nM EAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\nM\n\n(bpc) (bpc) (bpc) (bpc)\n\n\n31 0.9139 \u00b1 0.0263 1.3159 \u00b1 0.0329 1.0425 \u00b1 0.0262 0.9145 \u00b1 0.0263\n\n\n127 0.7511 \u00b1 0.0233 1.1303 \u00b1 0.0292 0.8847 \u00b1 0.0231 0.752 \u00b1 0.0233\n\n\n255 0.7242 \u00b1 0.0234 1.0985 \u00b1 0.0289 0.859 \u00b1 0.0232 0.725 \u00b1 0.0234\n\n\n511 0.7093 \u00b1 0.0228 1.0812 \u00b1 0.028 0.845 \u00b1 0.0226 0.7101 \u00b1 0.0228\n\n\nTABLE IV\n\nC OMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN P ROJECT G UTENBERG [7], AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC Standalone Zlib\n\n\n(bpc) (bpc) (bpc) (bpc) (bpc)\n\n\n\n31 508, 463 115, 000 1.0919 1.5316 1.2152 1.0924 2.80\n\n\n127 508, 463 115, 000 0.8973 1.3128 1.0235 0.8982 2.80\n\n\n255 508, 463 115, 000 0.8618 1.2684 0.9899 0.8627 2.80\n\n\n511 508, 463 115, 000 0.8417 1.2465 0.9711 0.8426 2.80\n\n\n8\n\n\nR EFERENCES\n\n\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR filters),\u201d 2023.\n\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and efficient foundation language models,\u201d\n2023.\n\n[7] J. Frank Dobie, Legends of Texas, United States, Texas Folk-Lore Society, 1924; Project Gutenberg, May 25, 2023, 2023,\n[https://www.gutenberg.org/ebooks/70859.](https://www.gutenberg.org/ebooks/70859)\n\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n\n\n"}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mThought: The paper has been downloaded and its content is available. Now I need to analyze the document structure to identify the main sections that follow the Abstract and Introduction. Then I will extract relevant information from those sections to create the synopsis.[00m
[95m## Using tool:[00m [92mget_sections[00m
[95m## Tool Input:[00m [92m
"{\"title\": \"LLMZip: Lossless Text Compression using Large Language Models\"}"[00m
[95m## Tool Output:[00m [92m
{"title":"LLMZip: Lossless Text Compression using Large Language Models","sections":[]}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Final Answer:[00m [92m
The document "LLMZip: Lossless Text Compression using Large Language Models" (arXiv: 2306.04050v2) details a novel approach to lossless text compression using Large Language Models (LLMs), specifically LLaMA-7B.

**Methodology (III. Compression Using LLMs):**
The core idea involves parsing an input sentence `s` into a sequence of `N_T` tokens `x = x_1, ..., x_{N_T}`. An LLM with memory `M` predicts the probability mass function `q_i(x_i)` for the next token `x_i` conditioned on the past `M` tokens. The rank `r_i` of the actual token `x_i` in the sorted PMF is then used.

Key definitions and equations include:
*   `m_i(s) = min n {length(T(s^n)) >= i}` (Equation 1): Defines the (time) index when the tokenizer outputs its `i`-th token.
*   `b_i = m_i(s) - m_{i-1}(s)`: Number of characters in the `i`-th token.
*   `N_c = sum_{i=1}^{N_T} B_i`: Total number of characters.
*   `ρ := N_b / N_c` (bits/character): The compression ratio.
*   Shannon-McMillan-Breiman Theorem (Equations 2 and 3) for entropy rates `H(S)` (characters) and `H(X)` (tokens).
*   Relationship: `H(S) = H(X) / E[B]`.
*   Asymptotic upper bound on `H(X)`: `H(X) <= lim_{N_T -> inf} (-1/N_T) sum_{i=1}^{N_T} log_2 q_i(X_i)` (Equation 5).
*   Asymptotic upper bound on `H(S)` (`H_ub`): `H(S) <= lim_{N_T -> inf} (-1/N_T) sum_{i=1}^{N_T} log_2 q_i(X_i) / E[B]` (Equation 6).

Three encoding schemes are considered for the lossless compression block:
1.  **LLaMA+zlib**: Compresses the sequence of ranks `r_i` using the zlib algorithm.
2.  **LLaMA+TbyT (Token-by-Token Compression)**: Uses a time-varying prefix-free code (e.g., Huffman code, satisfying Kraft inequality) where the codeword length for token `x_i` is `ceil(log_2(1/q_i(x_i)))`.
3.  **LLM+AC (Arithmetic Coding)**: Combines the LLM's output probabilities `q_i(x_i)` directly with arithmetic coding, which is known to be nearly optimal. The compression ratio `ρ_LLM+AC` is approximated by `(sum_{i=1}^{N_T} log_2(1/q_i(x_i))) / (sum_{i=1}^{N_T} b_i)` (Equation 7).

**Results (IV. Results):**
The study uses LLaMA-7B with a SentencePiece tokenizer (dictionary size 32000).
*   **Datasets**: A 1MB section of the `text8` dataset and a 100,000-token section from a recently published Project Gutenberg book [7].
*   **Baselines**: State-of-the-art text compression schemes like ZPAQ (1.4 bits/character) and paq8h (1.2 bits/character) on the full 100MB `text8` dataset.

**Key Findings:**
*   **On 1MB of text8 dataset (Table I):**
    *   LLaMA-7B with Arithmetic Coding (`ρ_LLM+AC`) achieved an average compression ratio of **0.7101 bits/character**.
    *   The estimated asymptotic upper bound on entropy (`H_ub`) was **0.7093 bits/character**.
    *   These results significantly outperform ZPAQ and paq8h.
    *   LLaMA+zlib (1.0812 bpc) and LLaMA+TbyT (0.845 bpc) also surpassed the baselines.
*   **Dependence on LLM Memory (M) (Table II):** Compression performance improves with increasing memory `M`. For `M=511`, `ρ_LLM+AC` was 0.7101 bpc, compared to 0.9145 bpc for `M=31`. Inference time scaled approximately linearly with memory length.
*   **On Project Gutenberg book (Table IV):**
    *   LLaMA-7B with Arithmetic Coding (`ρ_LLM+AC`) achieved **0.8426 bits/character** for `M=511`.
    *   The estimated `H_ub` was **0.8417 bits/character**.
    *   While slightly higher than `text8` results, these are still lower than previous entropy estimates [2] and better than state-of-the-art compressors on `text8`.
    *   Standalone zlib on this text yielded 2.8 bits/character, highlighting the substantial improvement from LLM-based compression.
*   **Caveats**: The LLaMA-7B model was not run on the entire 100MB `text8` dataset, and `text8` is derived from Wikipedia, which LLaMA-7B was likely trained on, potentially leading to optimistic results for `text8`. The estimates of compression ratio can show substantial variance depending on the input text.

**Conclusion**: The paper demonstrates that using LLaMA-7B as a predictor combined with arithmetic coding yields significantly better lossless text compression ratios and sharper estimates of the asymptotic upper bound on English entropy compared to existing methods and state-of-the-art compressors.[00m


🚀 Crew: crew
├── 📋 Task: 81f75889-bbc9-4f00-9326-8e6a0d86d45a
│   Assigned to: AI Assistant
│   Status: ✅ Completed
│   ├── ✅ Reasoning Completed
│   ├── 🔧 Used download_paper (1)
│   └── 🔧 Used get_sections (1)
├── 🔧 Using create_reasoning_plan from LLM available_function
├── 🔄 Tool Usage Started
├── 🔧 Using create_reasoning_plan from LLM available_function
└── ✅ Tool Usage Completed┌────────────────────────────── Task Completion ──────────────────────────────┐
│                                                                             │
│  Task Completed                                                             │
│  Name: 81f75889-bbc9-4f00-9326-8e6a0d86d45a                                 │
│  Agent: AI Assistant                                                        │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

┌────────────────────────────── Crew Completion ──────────────────────────────┐
│                                                                             │
│  Crew Execution Completed                                                   │
│  Name: crew                                                                 │
│  ID: 61317ccf-5d77-4e0a-9b5b-e45518aacc98                                   │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

AGENT_RESULT_JSON:{"final_text": "The document \"LLMZip: Lossless Text Compression using Large Language Models\" (arXiv: 2306.04050v2) details a novel approach to lossless text compression using Large Language Models (LLMs), specifically LLaMA-7B.\n\n**Methodology (III. Compression Using LLMs):**\nThe core idea involves parsing an input sentence `s` into a sequence of `N_T` tokens `x = x_1, ..., x_{N_T}`. An LLM with memory `M` predicts the probability mass function `q_i(x_i)` for the next token `x_i` conditioned on the past `M` tokens. The rank `r_i` of the actual token `x_i` in the sorted PMF is then used.\n\nKey definitions and equations include:\n*   `m_i(s) = min n {length(T(s^n)) >= i}` (Equation 1): Defines the (time) index when the tokenizer outputs its `i`-th token.\n*   `b_i = m_i(s) - m_{i-1}(s)`: Number of characters in the `i`-th token.\n*   `N_c = sum_{i=1}^{N_T} B_i`: Total number of characters.\n*   `\u03c1 := N_b / N_c` (bits/character): The compression ratio.\n*   Shannon-McMillan-Breiman Theorem (Equations 2 and 3) for entropy rates `H(S)` (characters) and `H(X)` (tokens).\n*   Relationship: `H(S) = H(X) / E[B]`.\n*   Asymptotic upper bound on `H(X)`: `H(X) <= lim_{N_T -> inf} (-1/N_T) sum_{i=1}^{N_T} log_2 q_i(X_i)` (Equation 5).\n*   Asymptotic upper bound on `H(S)` (`H_ub`): `H(S) <= lim_{N_T -> inf} (-1/N_T) sum_{i=1}^{N_T} log_2 q_i(X_i) / E[B]` (Equation 6).\n\nThree encoding schemes are considered for the lossless compression block:\n1.  **LLaMA+zlib**: Compresses the sequence of ranks `r_i` using the zlib algorithm.\n2.  **LLaMA+TbyT (Token-by-Token Compression)**: Uses a time-varying prefix-free code (e.g., Huffman code, satisfying Kraft inequality) where the codeword length for token `x_i` is `ceil(log_2(1/q_i(x_i)))`.\n3.  **LLM+AC (Arithmetic Coding)**: Combines the LLM's output probabilities `q_i(x_i)` directly with arithmetic coding, which is known to be nearly optimal. The compression ratio `\u03c1_LLM+AC` is approximated by `(sum_{i=1}^{N_T} log_2(1/q_i(x_i))) / (sum_{i=1}^{N_T} b_i)` (Equation 7).\n\n**Results (IV. Results):**\nThe study uses LLaMA-7B with a SentencePiece tokenizer (dictionary size 32000).\n*   **Datasets**: A 1MB section of the `text8` dataset and a 100,000-token section from a recently published Project Gutenberg book [7].\n*   **Baselines**: State-of-the-art text compression schemes like ZPAQ (1.4 bits/character) and paq8h (1.2 bits/character) on the full 100MB `text8` dataset.\n\n**Key Findings:**\n*   **On 1MB of text8 dataset (Table I):**\n    *   LLaMA-7B with Arithmetic Coding (`\u03c1_LLM+AC`) achieved an average compression ratio of **0.7101 bits/character**.\n    *   The estimated asymptotic upper bound on entropy (`H_ub`) was **0.7093 bits/character**.\n    *   These results significantly outperform ZPAQ and paq8h.\n    *   LLaMA+zlib (1.0812 bpc) and LLaMA+TbyT (0.845 bpc) also surpassed the baselines.\n*   **Dependence on LLM Memory (M) (Table II):** Compression performance improves with increasing memory `M`. For `M=511`, `\u03c1_LLM+AC` was 0.7101 bpc, compared to 0.9145 bpc for `M=31`. Inference time scaled approximately linearly with memory length.\n*   **On Project Gutenberg book (Table IV):**\n    *   LLaMA-7B with Arithmetic Coding (`\u03c1_LLM+AC`) achieved **0.8426 bits/character** for `M=511`.\n    *   The estimated `H_ub` was **0.8417 bits/character**.\n    *   While slightly higher than `text8` results, these are still lower than previous entropy estimates [2] and better than state-of-the-art compressors on `text8`.\n    *   Standalone zlib on this text yielded 2.8 bits/character, highlighting the substantial improvement from LLM-based compression.\n*   **Caveats**: The LLaMA-7B model was not run on the entire 100MB `text8` dataset, and `text8` is derived from Wikipedia, which LLaMA-7B was likely trained on, potentially leading to optimistic results for `text8`. The estimates of compression ratio can show substantial variance depending on the input text.\n\n**Conclusion**: The paper demonstrates that using LLaMA-7B as a predictor combined with arithmetic coding yields significantly better lossless text compression ratios and sharper estimates of the asymptotic upper bound on English entropy compared to existing methods and state-of-the-art compressors.", "duration_ms": 56724}
The document "LLMZip: Lossless Text Compression using Large Language Models" (arXiv: 2306.04050v2) details a novel approach to lossless text compression using Large Language Models (LLMs), specifically LLaMA-7B.

**Methodology (III. Compression Using LLMs):**
The core idea involves parsing an input sentence `s` into a sequence of `N_T` tokens `x = x_1, ..., x_{N_T}`. An LLM with memory `M` predicts the probability mass function `q_i(x_i)` for the next token `x_i` conditioned on the past `M` tokens. The rank `r_i` of the actual token `x_i` in the sorted PMF is then used.

Key definitions and equations include:
*   `m_i(s) = min n {length(T(s^n)) >= i}` (Equation 1): Defines the (time) index when the tokenizer outputs its `i`-th token.
*   `b_i = m_i(s) - m_{i-1}(s)`: Number of characters in the `i`-th token.
*   `N_c = sum_{i=1}^{N_T} B_i`: Total number of characters.
*   `ρ := N_b / N_c` (bits/character): The compression ratio.
*   Shannon-McMillan-Breiman Theorem (Equations 2 and 3) for entropy rates `H(S)` (characters) and `H(X)` (tokens).
*   Relationship: `H(S) = H(X) / E[B]`.
*   Asymptotic upper bound on `H(X)`: `H(X) <= lim_{N_T -> inf} (-1/N_T) sum_{i=1}^{N_T} log_2 q_i(X_i)` (Equation 5).
*   Asymptotic upper bound on `H(S)` (`H_ub`): `H(S) <= lim_{N_T -> inf} (-1/N_T) sum_{i=1}^{N_T} log_2 q_i(X_i) / E[B]` (Equation 6).

Three encoding schemes are considered for the lossless compression block:
1.  **LLaMA+zlib**: Compresses the sequence of ranks `r_i` using the zlib algorithm.
2.  **LLaMA+TbyT (Token-by-Token Compression)**: Uses a time-varying prefix-free code (e.g., Huffman code, satisfying Kraft inequality) where the codeword length for token `x_i` is `ceil(log_2(1/q_i(x_i)))`.
3.  **LLM+AC (Arithmetic Coding)**: Combines the LLM's output probabilities `q_i(x_i)` directly with arithmetic coding, which is known to be nearly optimal. The compression ratio `ρ_LLM+AC` is approximated by `(sum_{i=1}^{N_T} log_2(1/q_i(x_i))) / (sum_{i=1}^{N_T} b_i)` (Equation 7).

**Results (IV. Results):**
The study uses LLaMA-7B with a SentencePiece tokenizer (dictionary size 32000).
*   **Datasets**: A 1MB section of the `text8` dataset and a 100,000-token section from a recently published Project Gutenberg book [7].
*   **Baselines**: State-of-the-art text compression schemes like ZPAQ (1.4 bits/character) and paq8h (1.2 bits/character) on the full 100MB `text8` dataset.

**Key Findings:**
*   **On 1MB of text8 dataset (Table I):**
    *   LLaMA-7B with Arithmetic Coding (`ρ_LLM+AC`) achieved an average compression ratio of **0.7101 bits/character**.
    *   The estimated asymptotic upper bound on entropy (`H_ub`) was **0.7093 bits/character**.
    *   These results significantly outperform ZPAQ and paq8h.
    *   LLaMA+zlib (1.0812 bpc) and LLaMA+TbyT (0.845 bpc) also surpassed the baselines.
*   **Dependence on LLM Memory (M) (Table II):** Compression performance improves with increasing memory `M`. For `M=511`, `ρ_LLM+AC` was 0.7101 bpc, compared to 0.9145 bpc for `M=31`. Inference time scaled approximately linearly with memory length.
*   **On Project Gutenberg book (Table IV):**
    *   LLaMA-7B with Arithmetic Coding (`ρ_LLM+AC`) achieved **0.8426 bits/character** for `M=511`.
    *   The estimated `H_ub` was **0.8417 bits/character**.
    *   While slightly higher than `text8` results, these are still lower than previous entropy estimates [2] and better than state-of-the-art compressors on `text8`.
    *   Standalone zlib on this text yielded 2.8 bits/character, highlighting the substantial improvement from LLM-based compression.
*   **Caveats**: The LLaMA-7B model was not run on the entire 100MB `text8` dataset, and `text8` is derived from Wikipedia, which LLaMA-7B was likely trained on, potentially leading to optimistic results for `text8`. The estimates of compression ratio can show substantial variance depending on the input text.

**Conclusion**: The paper demonstrates that using LLaMA-7B as a predictor combined with arithmetic coding yields significantly better lossless text compression ratios and sharper estimates of the asymptotic upper bound on English entropy compared to existing methods and state-of-the-art compressors.
