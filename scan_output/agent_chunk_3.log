C:\tools\Anaconda3\Lib\site-packages\paramiko\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "cipher": algorithms.TripleDES,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.
  "class": algorithms.Blowfish,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "class": algorithms.TripleDES,
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\_internal\_generate_schema.py:628: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.
  warn(
- Reading configuration...
  - Reading registry from registry.yaml
  - Reading catalog from docker-mcp.yaml
  - Reading config from config.yaml
- Configuration read in 29.1734ms
- Watching registry at C:\Users\Noah\.docker\mcp\registry.yaml
- Watching config at C:\Users\Noah\.docker\mcp\config.yaml
- Those servers are enabled: arxiv-mcp-server, duckduckgo, wikipedia-mcp
- Using images:
  - mcp/arxiv-mcp-server@sha256:6dc6bba6dfed97f4ad6eb8d23a5c98ef5b7fa6184937d54b2d675801cd9dd29e
  - mcp/duckduckgo@sha256:68eb20db6109f5c312a695fc5ec3386ad15d93ffb765a0b4eb1baf4328dec14f
  - mcp/wikipedia-mcp@sha256:e90e3f6e3bb795f10d3d8d8ac1523f6d46d4f2ab5ac3b3a4720e1be879466472
> Images pulled in 75.6831ms
- Listing MCP tools...
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers:ro]
  - Running mcp/duckduckgo with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=duckduckgo -l docker-mcp-transport=stdio]
  > duckduckgo: (2 tools)
  > wikipedia-mcp: (10 tools) (9 resourceTemplates)
  > arxiv-mcp-server: (4 tools) (1 prompts)
> 16 tools listed in 2.9880726s
- Watching for configuration updates...
> Initialized in 3.0939313s
> Start stdio server
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\fields.py:1093: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'items', 'anyOf', 'enum', 'properties'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
  warn(
┌────────────────────────── Crew Execution Started ───────────────────────────┐
│                                                                             │
│  Crew Execution Started                                                     │
│  Name: crew                                                                 │
│  ID: 3819aaf2-f78a-44f2-819e-544f94a0dc7d                                   │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

🚀 Crew: crew
├── 📋 Task: e8e46cec-cf09-4581-84e1-fa7f21ec2198
│   Status: Executing Task...
│   └── ✅ Reasoning Completed
├── 🔧 Using create_reasoning_plan from LLM available_function
├── 🔄 Tool Usage Started
├── 🔧 Using create_reasoning_plan from LLM available_function
└── ✅ Tool Usage Completed┌───────────────────────────── 🧠 Reasoning Plan ─────────────────────────────┐
│                                                                             │
│  1. Analyze the document prefix: I will carefully read the provided         │
│  document prefix to identify the paper's core topic (lossless text          │
│  compression using LLMs), the specific LLM used (LLaMA-7B), the proposed    │
│  method (LLMZip, combining LLaMA-7B with arithmetic coding), key findings   │
│  (new entropy estimates, improved compression ratios), and the datasets     │
│  mentioned (text8, [7]). I will also note the stated goal of the paper.     │
│  2. Access the full document: I will use the `read_paper` tool with the     │
│  provided arXiv ID (2306.04050v2) to retrieve the complete content of the   │
│  paper. This is the most direct and authoritative way to obtain             │
│  information that will appear later in the document.                        │
│  3. Identify key sections for information extraction: Once the full paper   │
│  is retrieved, I will identify the sections that typically follow the       │
│  introduction and intuitive explanation. These are expected to include:     │
│      *   Detailed methodology of LLMZip (e.g., "III. LLMZip: The Proposed   │
│  Scheme").                                                                  │
│      *   Experimental setup (e.g., "IV. Experimental Setup," detailing      │
│  datasets, evaluation metrics, and implementation).                         │
│      *   Results and discussion (e.g., "V. Results," presenting             │
│  quantitative findings, tables, figures, and comparisons).                  │
│      *   Conclusion and future work (e.g., "VI. Conclusion").               │
│      *   References.                                                        │
│  4. Extract relevant content: I will systematically read through these      │
│  identified sections to extract specific facts, equations, definitions,     │
│  details about datasets, and references. I will focus on:                   │
│      *   The precise algorithm or scheme of LLMZip.                         │
│      *   Any mathematical formulations or key equations.                    │
│      *   Detailed descriptions of the text8 dataset and the dataset from    │
│  [7].                                                                       │
│      *   The exact entropy estimates and compression ratios achieved by     │
│  LLMZip.                                                                    │
│      *   The specific performance comparisons against BSC, ZPAQ, and        │
│  paq8h.                                                                     │
│      *   Any new definitions introduced.                                    │
│      *   Important references that are central to the paper's claims or     │
│  background.                                                                │
│  5. Synthesize the concise textual synopsis: I will compile the extracted   │
│  information into a concise textual synopsis. This synopsis will be         │
│  structured to provide a clear overview of the paper's future content,      │
│  focusing on the elements requested (facts, equations, definitions,         │
│  datasets, references). I will ensure that the synopsis is highly relevant  │
│  to the document's theme and strictly avoids any information not supported  │
│  by the retrieved paper content.                                            │
│  6. Review and refine: I will review the generated synopsis to ensure it    │
│  is concise, accurate, comprehensive within the given constraints, and      │
│  directly addresses all aspects of the prompt, particularly the             │
│  requirement to avoid hallucination and maximize relevance.                 │
│                                                                             │
│  Challenges and Mitigation:                                                 │
│  *   **Tool output format:** If `read_paper` returns the entire paper as a  │
│  single string, I will manually parse it to identify sections and extract   │
│  information. If it returns a structured object, I will leverage that       │
│  structure.                                                                 │
│  *   **Information overload:** The full paper might contain a lot of        │
│  detail. I will focus on extracting only the most critical and              │
│  summary-level information relevant to the synopsis, prioritizing key       │
│  findings, methods, and comparative results.                                │
│  *   **Ensuring conciseness:** I will use bullet points or short,           │
│  declarative sentences to present the information efficiently, avoiding     │
│  verbose explanations.                                                      │
│                                                                             │
│  Strategic Tool Usage:                                                      │
│  *   `read_paper(paper_id='2306.04050v2')`: This is the primary and most    │
│  crucial tool. It will be used once at the beginning to get the full        │
│  document content. No other tools are anticipated to be necessary if        │
│  `read_paper` provides the complete text, as the task specifically asks     │
│  for content *from the same document*. If `read_paper` fails or provides    │
│  insufficient detail, I would then consider `search_papers` or              │
│  `get_sections` as fallback, but the initial plan relies solely on          │
│  `read_paper` for direct access to the source.                              │
│                                                                             │
│  Expected Outcome: A concise textual synopsis (plain text) detailing the    │
│  likely future content of the "LLMZip" paper, including its detailed        │
│  methodology, experimental setup, quantitative results (entropy estimates,  │
│  compression ratios), comparisons with other compression schemes, and key   │
│  references, all derived directly from the full paper content. This will    │
│  directly fulfill the task requirements and align with my goal of           │
│  providing accurate and efficient information.                              │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Task:[00m [92mYou act as a deterministic research agent. Given the observed prefix of a document, use available MCP tools to retrieve information that is likely to appear later in the same document. Prefer authoritative sources. Produce a concise textual synopsis of the likely future content (facts, equations, definitions, datasets, references), suitable for conditioning a language model compressor. Maximize relevance to the document. Do not hallucinate; only include content supported by the tools.

Document prefix (UTF-8 text):
# LLMZip: Lossless Text Compression using Large Language Models

**Authors:** Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas Shakkottai  
**Affiliation:** Department of Electrical and Computer Engineering, Texas A&M University  
**Email:** {vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu  
**arXiv:** 2306.04050v2 [cs.IT] 26 Jun 2023

## Abstract

We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.

## I. INTRODUCTION

There are close connections between learning, prediction, and compression. The success of ChatGPT has captured the fascination of general public and brought the connection between learning and prediction to the fore. The main advance brought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in a paragraph based on knowing the past several words (tokens).

The connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the entropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past values can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many algorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of lossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very effective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor and every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression performance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved results for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ for text compression.

It is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the entropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this paper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the entropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates provided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of the text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for compression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression ratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio obtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.

## II. INTUITIVE EXPLANATION OF THE MAIN IDEA

We will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3] for estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable length and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence that reads as

My first attempt at writing a book

Our goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can be reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)

′My′, ′first′, ′attempt′, ′at′, ′writing′, ′a′, ′book′

A language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the past M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in Figure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be words such as ′reading′,′ writing′,′ driving′,′ cooking′ etc. The main idea is to compute the rank of the actual word in our sentence (′writing′) in this list and call it R5. We will assume that the ranks start at 0 i.e., the most likely word has rank 0, the second most likely word has rank 1, and so on. In this example, the rank for ′writing′ is R5 = 1.

**Fig. 1.** Schematic showing the prediction at epoch 5 for a language model with memory 4.

| Next word | Probability |
|-----------|-------------|
| reading  | 0.3        |
| writing  | 0.2        |
| cycling  | 0.1        |
| driving  | 0.05       |
| ...      | ...        |

R5 = 1

Then, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2 through 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same word in the sentence that we wish to encode, ′a′, and hence, the rank R6 would be 0.

**Fig. 2.** Schematic showing the prediction at epoch 6 for a language model with memory 4.

| Next word | Probability |
|-----------|-------------|
| a        | 0.7        |
| an       | 0.2        |
| the      | 0.05       |
| ...

Output: A concise synopsis (plain text).

Reasoning Plan:
1. Analyze the document prefix: I will carefully read the provided document prefix to identify the paper's core topic (lossless text compression using LLMs), the specific LLM used (LLaMA-7B), the proposed method (LLMZip, combining LLaMA-7B with arithmetic coding), key findings (new entropy estimates, improved compression ratios), and the datasets mentioned (text8, [7]). I will also note the stated goal of the paper.
2. Access the full document: I will use the `read_paper` tool with the provided arXiv ID (2306.04050v2) to retrieve the complete content of the paper. This is the most direct and authoritative way to obtain information that will appear later in the document.
3. Identify key sections for information extraction: Once the full paper is retrieved, I will identify the sections that typically follow the introduction and intuitive explanation. These are expected to include:
    *   Detailed methodology of LLMZip (e.g., "III. LLMZip: The Proposed Scheme").
    *   Experimental setup (e.g., "IV. Experimental Setup," detailing datasets, evaluation metrics, and implementation).
    *   Results and discussion (e.g., "V. Results," presenting quantitative findings, tables, figures, and comparisons).
    *   Conclusion and future work (e.g., "VI. Conclusion").
    *   References.
4. Extract relevant content: I will systematically read through these identified sections to extract specific facts, equations, definitions, details about datasets, and references. I will focus on:
    *   The precise algorithm or scheme of LLMZip.
    *   Any mathematical formulations or key equations.
    *   Detailed descriptions of the text8 dataset and the dataset from [7].
    *   The exact entropy estimates and compression ratios achieved by LLMZip.
    *   The specific performance comparisons against BSC, ZPAQ, and paq8h.
    *   Any new definitions introduced.
    *   Important references that are central to the paper's claims or background.
5. Synthesize the concise textual synopsis: I will compile the extracted information into a concise textual synopsis. This synopsis will be structured to provide a clear overview of the paper's future content, focusing on the elements requested (facts, equations, definitions, datasets, references). I will ensure that the synopsis is highly relevant to the document's theme and strictly avoids any information not supported by the retrieved paper content.
6. Review and refine: I will review the generated synopsis to ensure it is concise, accurate, comprehensive within the given constraints, and directly addresses all aspects of the prompt, particularly the requirement to avoid hallucination and maximize relevance.

Challenges and Mitigation:
*   **Tool output format:** If `read_paper` returns the entire paper as a single string, I will manually parse it to identify sections and extract information. If it returns a structured object, I will leverage that structure.
*   **Information overload:** The full paper might contain a lot of detail. I will focus on extracting only the most critical and summary-level information relevant to the synopsis, prioritizing key findings, methods, and comparative results.
*   **Ensuring conciseness:** I will use bullet points or short, declarative sentences to present the information efficiently, avoiding verbose explanations.

Strategic Tool Usage:
*   `read_paper(paper_id='2306.04050v2')`: This is the primary and most crucial tool. It will be used once at the beginning to get the full document content. No other tools are anticipated to be necessary if `read_paper` provides the complete text, as the task specifically asks for content *from the same document*. If `read_paper` fails or provides insufficient detail, I would then consider `search_papers` or `get_sections` as fallback, but the initial plan relies solely on `read_paper` for direct access to the source.

Expected Outcome: A concise textual synopsis (plain text) detailing the likely future content of the "LLMZip" paper, including its detailed methodology, experimental setup, quantitative results (entropy estimates, compression ratios), comparisons with other compression schemes, and key references, all derived directly from the full paper content. This will directly fulfill the task requirements and align with my goal of providing accurate and efficient information.
  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool download_paper with arguments: {"check_status":false,"paper_id":"2306.04050v2"}
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
  > Calling tool download_paper took: 1.6574473s
  - Scanning tool call response for secrets...
  > No secret found in response.
[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mAction: download_paper[00m
[95m## Using tool:[00m [92mdownload_paper[00m
[95m## Tool Input:[00m [92m
"{\"paper_id\": \"2306.04050v2\"}"[00m
[95m## Tool Output:[00m [92m
{"status": "success", "message": "Paper already available", "resource_uri": "file:///app/papers/2306.04050v2.md", "pdf_uri": "https://arxiv.org/pdf/2306.04050v2.pdf", "content": "1\n\n# LLMZip: Lossless Text Compression using Large Language Models\n\n\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\n\n\nAbstract\n\n\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\n\n\nI. I NTRODUCTION\n\n\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\n\n\nII. I NTUITIVE EXPLANATION OF THE MAIN IDEA\n\n\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\n\nMy first attempt at writing a book\n\n\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032, \u2032 a \u2032, \u2032 book \u2032\n\n\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n\n\n2\n\n\nFigure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be\nwords such as [\u2032] reading [\u2032], [\u2032] writing [\u2032], [\u2032] driving [\u2032], [\u2032] cooking [\u2032] etc. The main idea is to compute the rank of the actual word in\nour sentence ( [\u2032] writing [\u2032] ) in this list and call it R 5 . We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for [\u2032] writing [\u2032] is R 5 = 1.\n\n\nTokens\n\n\n\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x5|x4 1)<br>5|Rank<br>computation|R5 = 1|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nreading 0.3\nwriting 0.2\ncycling 0.1\ndriving 0.05\n\n... ...\n\n\n\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\n\n\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, [\u2032] a [\u2032], and hence, the rank R 6 would be 0.\n\n\nTokens\n\n\n\n\n\n\u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x6|x5 2)<br>6|Rank<br>computation|R6 = 0|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nan 0.2\n\nthe 0.05\n\n... ...\n\n\n\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\n\n\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n\n\n1, 0, 0, . . .\n\n\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\n\n\nSequence of ranks\n\n|e of ranks|Col2|Col3|\n|---|---|---|\n|. ., rNT|Compression (zip)|Nb|\n||||\n\n\n\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\n\n\nWhen we wish to reconstruct the sequence, we first decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank R i during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\n\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n\n\n3\n\n\nIII. C OMPRESSION USING LLM S\n\n\nLet s denote a sentence from the English language composed of N c letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We first parse s into a sequence of N T tokens\ndenoted by x = x 1, x 2, . . ., x i\u22121, x i, x i+1, . . . x N T, where x i \u2208X . There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. x i \u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter X i .\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens x i\u2212M, x i\u2212M+1, . . ., x i\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nq i (x i ) := Pr(X i = x i |x i\u22121, x i\u22122, . . ., x i\u2212M ), \u2200x i \u2208X . The PMF vector q i := [q i (1), q i (2), . . ., q i (D)] [T] is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcq i . Let \u03b3 i : X \u2192X be a permutation on the integers\nfrom 1 to D such that\n\n\u02dc\nq i (\u03b3 i (j)) = q i (j), \u2200j \u2208X .\n\n\nThat is, \u03b3 i (j) is the rank of the token j at epoch i. We define the rank of the input sequence at epoch i as the rank of the\ntoken x i at epoch i, r i := \u03b3 i (x i ). The sequence {r i } [N] i=1 [T] [is compressed by a lossless compression algorithm (such as zlib) to]\nproduce N b bits which are the final bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors q i \u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 defined as\n\n\u03c1 := [N] [b] bits/character.\n\nN c\n\n\n\nLosseless\n\nCompression\n\n\n\nTokens . . ., x i\u2212M, . . ., x i\u22121\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n\nInput sentence s with N c characters\n\n\n\ni i i\u22121 Rank\n\nLLM\ncomputation\n\n\n\nq i = Pr(X i = j|x [i] i [\u2212] \u2212 [M] 1 [)] Rank r i Losseless N b bits\n\n\n\nFig. 4. Schematic showing the prediction at epoch i.\n\n\nA. Entropy bounds\n\n\nLet S \u2208S [\u221e] be a random process that represents language input. The nth character in the sequence is denoted by S n,\nwhereas the string of characters from the beginning to the nth character is expressed as S n . The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X 1, X 2, . . . using a variable-length mapping. In this sequence, X i is the ith\ntoken. The number of characters employed to generate X i depends on the realization of the random process and, as such,\nwe introduce random variable B i to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which B i \u2265 1 and B i is uniformly bounded, with B i < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded N T \u2192\u221e, the number of characters must also approach infinity N c \u2192\u221e. Formally, consider the tokenizer\nfunction T : S [N] \u2192X [N] operating on infinite symbol sequences; that is, T (s) = x where s is an infinite sequence in S [\u221e] . For\nnatural number, i \u2208 N, define m i : S [N] \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Specifically, suppose s is given, then\n\n\nm i (s) = min n [{][length (][T][ (][s] [n] [))][ \u2265] [i][}][ .] (1)\n\n\nWe note that, by construction, lim n\u2192\u221e length (T (s n )) = \u221e and, as such, m i (\u00b7) is well-defined. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple finite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (s n ) = T (s n+1 ). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (finite) series of input symbols in the restricted\nset for which T (y n ) = x i is s m i (s) . Given a fixed sequence s, we can express the number of characters contained in a token\n\nas\nb i = m i (s) \u2212 m i\u22121 (s)\n\n\n4\n\n\nwith initial condition m \u22121 = 0. Consequently, the number of characters embedded in the first N T tokens for a random input\nbecomes N c = [\ufffd] i [N] =1 [T] [B] [i] [.]\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {S k } [\u221e] k=1 [,]\n{B i } [\u221e] i=1 [, and][ {][X] [i] [}] [\u221e] i=1 [are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that]\n\n\n\n\u2212 [1]\n\n\n\n\n[1]\n\nn [log] [2] [ p] [S] [n] [(][S] [1] [, . . ., S] [n] [) =][ \u2212] n [1]\n\n\n\nn [log] [2] [ p] [S] [n] [(][S] [n] [)][ \u2192] [H][(][S][)] almost surely. (2)\n\n\n\nLet \u2126 S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\n\n\n\n[1]\n\ni [log] [2] [ p] [X] [i] [(][X] [1] [, . . ., X] [i] [) =][ \u2212] [1] i\n\n\n\n\u2212 [1]\n\n\n\ni [log] [2] [ p] [X] [i] [(][X] [i] [)][ \u2192] [H][(][X][)] almost surely. (3)\n\n\n\nDefine \u2126 X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\n\n\nm i (S)\nlim = E [B] almost surely. (4)\ni\u2192\u221e i\n\n\nSet \u2126 B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126 S \u2229 \u2126 X \u2229 \u2126 B, we deduce that\n\n\nH(S) = lim\nk\u2192\u221e [\u2212] k [1] [log] [2] [ p] [S] [k] [(][S] [k] [(][\u03c9][))]\n\n= lim i\u2192\u221e [\u2212] l [1] i log 2 p S li (S l i (\u03c9))\n\n\n= lim log 2 Pr (X i = T (S l i (\u03c9)))\ni\u2192\u221e [\u2212] l [1] i\n\n\n\n1\n\ni [log] [2] [ Pr (][X] [i] [ =][ x] [i] [) =][ H] E [(] B [X][)]\n\n\n\n1 1\n\n= \u2212\nE[B] i [lim] \u2192\u221e i\n\n\n\nE[B] [.]\n\n\n\nThe first equality follows from (2). The second equality is a consequence of the fact that {l i = m i (S(\u03c9))|i \u2208 N} is an infinite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\n\ntwo events,\n{\u03c9 \u2208 \u2126|X i (\u03c9) = x i } = {\u03c9 \u2208 \u2126|T (S m i (S(\u03c9)) ) = x i }.\n\n\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126 B . The sets \u2126 S, \u2126 X, and \u2126 B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\n\nH(S) = [H][(][X][)] almost surely.\n\nE[B]\n\n\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\n\n\n\nPr\n\n\ufffd\n\n\n\n\ufffd\n\n\n\nH(X) \u2264 lim\nN T \u2192\u221e [\u2212] N [1] T\n\n\n\nN T\n\ufffd\n\n\n\n\ufffd log 2 q i (X i )\n\n\ni=1\n\n\n\n= 1, (5)\n\n\n\nwhere q i (\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\n\n\n\n\n[1] N T\n\nN T \ufffd i=1 [log] 2 [q] [i] [(][X] [i] [)]\n\n\n\nH(S) \u2264 lim N T \u2192\u221e \u2212 N [1]\n\n\n\n. (6)\nE[B]\n\n\n\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by H ub . The\nnumerator in (6) represents the average number of bits required to represent the tokens X N T and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\n\n\nB. Encoding schemes\n\n\nWe consider three schemes for the lossless compression block in Fig. 3.\n\n\n5\n\n\n1) Compressing the ranks using zlib: The first scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1 LLaMA+zlib .\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token x i at epoch i by using a prefix-free code assuming q i to be the true distribution of\nthe tokens. A natural choice for a prefix-free code is a Huffman code. Instead, for simplicity, we use a prefix-free code where\n1\nthe codeword for the token x i is of length l i = \u2308log 2 q i (x i ) [\u2309][. A prefix-free code with this length for][ x] [i] [ is guaranteed to exist]\nsince this choice of lengths satisfies the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1 LLaMA+TbyT,\nis given by\n\n\n\nN T\n\ufffd\n\n\ni=1\n\n\n\n1\nlog 2\n\ufffd q i (x i )\n\n\n\n\ufffd\n\n\n\n\u03c1 LLaMA+TbyT =\n\n\n\n\ufffd Ni=1 T [b] [i] .\n\n\n\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use q i (x i ) as the probability of token x i at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1 LLM+AC . It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\n\n\n\u03c1 LLM+AC \u2248\n\n\n\nN T 1\n\ufffd i=1 log 2 q i (x i )\n\n\ufffd Ni=1 T [b] [i] . (7)\n\n\n\nClearly, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and \u03c1 LLM+AC also provide upper bounds on H(S). H ub, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and\n\u03c1 LLM+AC are estimated using a finite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\n\n\nIV. R ESULTS\n\n\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it difficult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\n[Nevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of](http://mattmahoney.net/dc/text8.zip)\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M ) is shown in Table II. As expected, the\ncompression performance improves with increasing M . We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n\n\n6\n\n\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\n\n\nV. A CKNOWLEDGEMENT\n\n\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\n\n\nTABLE I\n\nR ESULTS FOR 1MB OF TEXT FROM TEXT 8 DATASET\n\n\n\nBatch\nN c N T\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC ZPAQ pq8h\n\n\n\nNo. (bpc) file size (bits) (bpc) (bpc) (bpc) (bpc)\n\n\n1 466, 650 100, 000 0.6882 1.0513 0.8215 0.689\n\n\n2 461, 477 100, 000 0.6893 1.0558 0.8242 0.6901\n\n\n3 454, 599 100, 000 0.699 1.0681 0.8357 0.6999\n\n\n4 462, 755 100, 000 0.6748 1.0346 0.8093 0.6757\n\n\n5 453, 847 100, 000 0.7481 1.1265 0.8831 0.749\n\n\n6 458, 252 100, 000 0.7218 1.0957 0.8567 0.7227\n\n\n7 451, 036 100, 000 0.6959 1.0729 0.8353 0.6968\n\n\n8 447, 953 100, 000 0.7092 1.0896 0.8489 0.7101\n\n\n9 462, 665 100, 000 0.7394 1.1126 0.8713 0.7402\n\n\n10 449, 621 100, 000 0.7269 1.1046 0.8643 0.7277\n\n\nTotal 9, 137, 710 2, 000, 000 0.7093 1.0812 0.845 0.7101 1.4 [1] 1.2 [2]\n\n\nTABLE II\n\nC OMPRESSION PERFORMANCE OF THE LLM ON THE TEXT 8 DATASET, AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\n\n\n(bpc) file size (bits) (bpc) (bpc)\n\n\n\n31 4, 568, 855 1, 000, 000 0.9139 1.3159 1.0425 0.9145\n\n\n127 4, 568, 855 1, 000, 000 0.7511 1.1303 0.8847 0.752\n\n\n255 4, 568, 855 1, 000, 000 0.7242 1.0985 0.859 0.725\n\n\n511 4, 568, 855 1, 000, 000 0.7093 1.0812 0.845 0.7101\n\n\n1 This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2 This result is taken from [12] and it corresponds to the full 100MB dataset text8\n\n\n7\n\n\n\nTABLE III\n\nM EAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\nM\n\n(bpc) (bpc) (bpc) (bpc)\n\n\n31 0.9139 \u00b1 0.0263 1.3159 \u00b1 0.0329 1.0425 \u00b1 0.0262 0.9145 \u00b1 0.0263\n\n\n127 0.7511 \u00b1 0.0233 1.1303 \u00b1 0.0292 0.8847 \u00b1 0.0231 0.752 \u00b1 0.0233\n\n\n255 0.7242 \u00b1 0.0234 1.0985 \u00b1 0.0289 0.859 \u00b1 0.0232 0.725 \u00b1 0.0234\n\n\n511 0.7093 \u00b1 0.0228 1.0812 \u00b1 0.028 0.845 \u00b1 0.0226 0.7101 \u00b1 0.0228\n\n\nTABLE IV\n\nC OMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN P ROJECT G UTENBERG [7], AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC Standalone Zlib\n\n\n(bpc) (bpc) (bpc) (bpc) (bpc)\n\n\n\n31 508, 463 115, 000 1.0919 1.5316 1.2152 1.0924 2.80\n\n\n127 508, 463 115, 000 0.8973 1.3128 1.0235 0.8982 2.80\n\n\n255 508, 463 115, 000 0.8618 1.2684 0.9899 0.8627 2.80\n\n\n511 508, 463 115, 000 0.8417 1.2465 0.9711 0.8426 2.80\n\n\n8\n\n\nR EFERENCES\n\n\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR filters),\u201d 2023.\n\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and efficient foundation language models,\u201d\n2023.\n\n[7] J. Frank Dobie, Legends of Texas, United States, Texas Folk-Lore Society, 1924; Project Gutenberg, May 25, 2023, 2023,\n[https://www.gutenberg.org/ebooks/70859.](https://www.gutenberg.org/ebooks/70859)\n\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n\n\n"}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Final Answer:[00m [92m

Thought: The paper "LLMZip: Lossless Text Compression using Large Language Models" (arXiv:2306.04050v2) has been successfully downloaded. I now have the full content of the document. My next step is to extract the information that is likely to appear later in the document, specifically focusing on facts, equations, definitions, datasets, and references, to create a concise textual synopsis.

Based on the document prefix and the full content, the likely future content will include:

*   **III. COMPRESSION USING LLMS**: This section details the formal methodology of LLMZip, including the definition of tokens, the role of the language model in producing probability mass functions (PMFs), and the compression of ranks. It also introduces the compression ratio metric.
    *   **A. Entropy bounds**: This subsection provides the theoretical framework for entropy estimation, including equations relating the entropy of characters (H(S)) to the entropy of tokens (H(X)) and the average number of characters per token (E[B]). It also presents the asymptotic upper bound on the entropy rate H(S) (Equation 6).
    *   **B. Encoding schemes**: This subsection describes the three specific compression schemes considered:
        1.  **Compressing the ranks using zlib (LLaMA+zlib)**.
        2.  **Token-by-Token Compression (LLaMA+TbyT)**, including its compression ratio formula.
        3.  **Arithmetic Coding (LLM+AC)**, highlighting its near-optimality and its compression ratio formula (Equation 7).
*   **IV. RESULTS**: This section presents the experimental findings.
    *   **LLaMA-7B and SentencePiece tokenizer**: Details on the specific LLM and tokenizer used.
    *   **Datasets**: Discussion of the text8 dataset and a recently released book from Project Gutenberg [7].
    *   **Baselines**: Comparison with state-of-the-art text compression schemes like ZPAQ and paq8h.
    *   **Tables**: Tables I, II, III, and IV present quantitative results:
        *   Table I: Results for 1MB of text from the text8 dataset, showing H_ub, compression ratios for LLaMA+zlib, LLaMA+TbyT, LLaMA+AC, and comparisons with ZPAQ and pq8h.
        *   Table II: Compression performance as a function of LLM memory (M) on the text8 dataset.
        *   Table III: Mean and standard deviation of entropy bounds and compression ratios over multiple batches.
        *   Table IV: Compression performance on the Project Gutenberg book [7], including a comparison with standalone zlib.
    *   **Key findings**: Specific entropy estimates (e.g., 0.709 bits/character for text8, 0.85 bits/character for the book) and compression ratios (e.g., 0.7101 bits/character for LLaMA+AC on text8, 0.8426 bits/character on the book).
    *   **Limitations**: Discussion on the potential optimism of text8 results due to LLaMA-7B's training corpus and the differing input sizes for comparisons.
*   **V. ACKNOWLEDGEMENT**: A brief section acknowledging contributions.
*   **REFERENCES**: A list of cited works.

I will now synthesize this information into a concise textual synopsis.The document "LLMZip: Lossless Text Compression using Large Language Models" will further detail its methodology, experimental results, and conclusions.

> Stop watching for updates
**Methodology (III. Compression Using LLMs):**
The paper will formally define the compression process. It will explain how a sentence `s` of `N_c` characters is parsed into `N_T` tokens `x = x_1, ..., x_{N_T}` using a dictionary `X`. A language model with memory `M` predicts the next token `x_i` by producing a probability mass function `q_i(x_i) := Pr(X_i = x_i | x_{i-1}, ..., x_{i-M})`. The rank `r_i` of the actual token `x_i` in the sorted PMF `~q_i` is then computed. The compression ratio `ρ` is defined as `N_b / N_c` bits/character, where `N_b` is the number of bits after compression.

**Entropy Bounds (III.A. Entropy bounds):**
The paper will establish a theoretical connection between the entropy of characters `H(S)` and the entropy of tokens `H(X)`. Key equations will include:
*   `m_i(s) = min n {length(T(s^n)) >= i}` (Equation 1), defining the character index for the i-th token.
*   `H(S) = H(X) / E[B]` almost surely, where `E[B]` is the average number of characters per token.
*   An asymptotic upper bound on the entropy rate `H(S)`: `H_ub <= lim_{N_T->∞} (-1/N_T) Σ_{i=1}^{N_T} log_2 q_i(X_i) / E[B]` (Equation 6). This bound is compared to previous estimates like 1.3 bits/character by Cover and King [1].

**Encoding Schemes (III.B. Encoding schemes):**
Three specific lossless compression schemes will be described:
1.  **LLaMA+zlib**: Compressing the sequence of ranks `r_i` using the zlib algorithm.
2.  **LLaMA+TbyT (Token-by-Token Compression)**: Using a time-varying prefix-free code (e.g., Huffman code) with codeword length `l_i = ⌈log_2(1/q_i(x_i))⌉`. The compression ratio `ρ_LLaMA+TbyT` is given by `(Σ_{i=1}^{N_T} ⌈log_2(1/q_i(x_i))⌉) / (Σ_{i=1}^{N_T} b_i)`.
3.  **LLM+AC (Arithmetic Coding)**: Combining the LLM's output probabilities `q_i(x_i)` with arithmetic coding, which is expected to be nearly optimal. The compression ratio `ρ_LLM+AC` is approximated by `(Σ_{i=1}^{N_T} log_2(1/q_i(x_i))) / (Σ_{i=1}^{N_T} b_i)` (Equation 7).

**Results (IV. Results):**
This section will present the empirical performance of LLMZip.
*   **LLM and Tokenizer**: LLaMA-7B [6] is used as the language model, and SentencePiece tokenizer [11] with a dictionary size of 32,000.
*   **Datasets**: Experiments are conducted on the `text8` dataset (derived from Wikipedia) and a recently published book from Project Gutenberg [7] (released May 25, 2023).
*   **Baselines**: Performance is compared against state-of-the-art text compression schemes like ZPAQ [5] (1.4 bpc on text8) and paq8h [12] (1.2 bpc on text8). Standalone zlib is also used for comparison (2.8 bpc on the Project Gutenberg book).
*   **Key Findings**:
    *   **text8 dataset (1MB section)**: The asymptotic upper bound on entropy `H_ub` is estimated at 0.709 bits/character. LLaMA+AC achieves a compression ratio of 0.7101 bits/character, outperforming ZPAQ and paq8h. LLaMA+zlib and LLaMA+TbyT also show improved performance.
    *   **Project Gutenberg book (100KB section)**: `H_ub` is estimated at 0.8417 bits/character. LLaMA+AC achieves 0.8426 bits/character.
    *   **Memory (M) dependence**: Compression performance improves with increasing LLM memory (M), as shown in Table II.
    *   **Variance**: Results show variance depending on input text, and comparisons are made with different input sizes.
    *   **Optimism**: Results on text8 might be optimistic due to LLaMA-7B's training on Wikipedia.
*   **Tables**:
    *   **Table I**: Detailed results for 1MB of text from the text8 dataset, showing `N_c`, `N_T`, `H_ub`, `ρ_LLaMA+zlib`, `ρ_LLaMA+TbyT`, `ρ_LLM+AC`, and baseline `ZPAQ` and `pq8h` values.
    *   **Table II**: Compression performance on the text8 dataset as a function of LLM memory `M` (31, 127, 255, 511 tokens).
    *   **Table III**: Mean and standard deviation of entropy bounds and compression ratios over 10 batches of 100,000 tokens for different `M`.
    *   **Table IV**: Compression performance on the Project Gutenberg book [7] as a function of LLM memory `M`, including standalone Zlib results.

**Acknowledgement (V. Acknowledgement):**
The authors will acknowledge Andreas Kirsch for discussions on arithmetic coding.

**References:**
A list of 12 references will be provided, including foundational works by Shannon [3], Cover and King [1], and works on arithmetic coding [4, 9], RNN-based compression [5], LLaMA [6], SentencePiece [11], and the datasets used [7, 12].[00m


🚀 Crew: crew
├── 📋 Task: e8e46cec-cf09-4581-84e1-fa7f21ec2198
│   Assigned to: AI Assistant
│   Status: ✅ Completed
│   ├── ✅ Reasoning Completed
│   └── 🔧 Used download_paper (1)
├── 🔧 Using create_reasoning_plan from LLM available_function
├── 🔄 Tool Usage Started
├── 🔧 Using create_reasoning_plan from LLM available_function
└── ✅ Tool Usage Completed┌────────────────────────────── Task Completion ──────────────────────────────┐
│                                                                             │
│  Task Completed                                                             │
│  Name: e8e46cec-cf09-4581-84e1-fa7f21ec2198                                 │
│  Agent: AI Assistant                                                        │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

┌────────────────────────────── Crew Completion ──────────────────────────────┐
│                                                                             │
│  Crew Execution Completed                                                   │
│  Name: crew                                                                 │
│  ID: 3819aaf2-f78a-44f2-819e-544f94a0dc7d                                   │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

AGENT_RESULT_JSON:{"final_text": "\nThought: The paper \"LLMZip: Lossless Text Compression using Large Language Models\" (arXiv:2306.04050v2) has been successfully downloaded. I now have the full content of the document. My next step is to extract the information that is likely to appear later in the document, specifically focusing on facts, equations, definitions, datasets, and references, to create a concise textual synopsis.\n\nBased on the document prefix and the full content, the likely future content will include:\n\n*   **III. COMPRESSION USING LLMS**: This section details the formal methodology of LLMZip, including the definition of tokens, the role of the language model in producing probability mass functions (PMFs), and the compression of ranks. It also introduces the compression ratio metric.\n    *   **A. Entropy bounds**: This subsection provides the theoretical framework for entropy estimation, including equations relating the entropy of characters (H(S)) to the entropy of tokens (H(X)) and the average number of characters per token (E[B]). It also presents the asymptotic upper bound on the entropy rate H(S) (Equation 6).\n    *   **B. Encoding schemes**: This subsection describes the three specific compression schemes considered:\n        1.  **Compressing the ranks using zlib (LLaMA+zlib)**.\n        2.  **Token-by-Token Compression (LLaMA+TbyT)**, including its compression ratio formula.\n        3.  **Arithmetic Coding (LLM+AC)**, highlighting its near-optimality and its compression ratio formula (Equation 7).\n*   **IV. RESULTS**: This section presents the experimental findings.\n    *   **LLaMA-7B and SentencePiece tokenizer**: Details on the specific LLM and tokenizer used.\n    *   **Datasets**: Discussion of the text8 dataset and a recently released book from Project Gutenberg [7].\n    *   **Baselines**: Comparison with state-of-the-art text compression schemes like ZPAQ and paq8h.\n    *   **Tables**: Tables I, II, III, and IV present quantitative results:\n        *   Table I: Results for 1MB of text from the text8 dataset, showing H_ub, compression ratios for LLaMA+zlib, LLaMA+TbyT, LLaMA+AC, and comparisons with ZPAQ and pq8h.\n        *   Table II: Compression performance as a function of LLM memory (M) on the text8 dataset.\n        *   Table III: Mean and standard deviation of entropy bounds and compression ratios over multiple batches.\n        *   Table IV: Compression performance on the Project Gutenberg book [7], including a comparison with standalone zlib.\n    *   **Key findings**: Specific entropy estimates (e.g., 0.709 bits/character for text8, 0.85 bits/character for the book) and compression ratios (e.g., 0.7101 bits/character for LLaMA+AC on text8, 0.8426 bits/character on the book).\n    *   **Limitations**: Discussion on the potential optimism of text8 results due to LLaMA-7B's training corpus and the differing input sizes for comparisons.\n*   **V. ACKNOWLEDGEMENT**: A brief section acknowledging contributions.\n*   **REFERENCES**: A list of cited works.\n\nI will now synthesize this information into a concise textual synopsis.The document \"LLMZip: Lossless Text Compression using Large Language Models\" will further detail its methodology, experimental results, and conclusions.\n\n**Methodology (III. Compression Using LLMs):**\nThe paper will formally define the compression process. It will explain how a sentence `s` of `N_c` characters is parsed into `N_T` tokens `x = x_1, ..., x_{N_T}` using a dictionary `X`. A language model with memory `M` predicts the next token `x_i` by producing a probability mass function `q_i(x_i) := Pr(X_i = x_i | x_{i-1}, ..., x_{i-M})`. The rank `r_i` of the actual token `x_i` in the sorted PMF `~q_i` is then computed. The compression ratio `\u03c1` is defined as `N_b / N_c` bits/character, where `N_b` is the number of bits after compression.\n\n**Entropy Bounds (III.A. Entropy bounds):**\nThe paper will establish a theoretical connection between the entropy of characters `H(S)` and the entropy of tokens `H(X)`. Key equations will include:\n*   `m_i(s) = min n {length(T(s^n)) >= i}` (Equation 1), defining the character index for the i-th token.\n*   `H(S) = H(X) / E[B]` almost surely, where `E[B]` is the average number of characters per token.\n*   An asymptotic upper bound on the entropy rate `H(S)`: `H_ub <= lim_{N_T->\u221e} (-1/N_T) \u03a3_{i=1}^{N_T} log_2 q_i(X_i) / E[B]` (Equation 6). This bound is compared to previous estimates like 1.3 bits/character by Cover and King [1].\n\n**Encoding Schemes (III.B. Encoding schemes):**\nThree specific lossless compression schemes will be described:\n1.  **LLaMA+zlib**: Compressing the sequence of ranks `r_i` using the zlib algorithm.\n2.  **LLaMA+TbyT (Token-by-Token Compression)**: Using a time-varying prefix-free code (e.g., Huffman code) with codeword length `l_i = \u2308log_2(1/q_i(x_i))\u2309`. The compression ratio `\u03c1_LLaMA+TbyT` is given by `(\u03a3_{i=1}^{N_T} \u2308log_2(1/q_i(x_i))\u2309) / (\u03a3_{i=1}^{N_T} b_i)`.\n3.  **LLM+AC (Arithmetic Coding)**: Combining the LLM's output probabilities `q_i(x_i)` with arithmetic coding, which is expected to be nearly optimal. The compression ratio `\u03c1_LLM+AC` is approximated by `(\u03a3_{i=1}^{N_T} log_2(1/q_i(x_i))) / (\u03a3_{i=1}^{N_T} b_i)` (Equation 7).\n\n**Results (IV. Results):**\nThis section will present the empirical performance of LLMZip.\n*   **LLM and Tokenizer**: LLaMA-7B [6] is used as the language model, and SentencePiece tokenizer [11] with a dictionary size of 32,000.\n*   **Datasets**: Experiments are conducted on the `text8` dataset (derived from Wikipedia) and a recently published book from Project Gutenberg [7] (released May 25, 2023).\n*   **Baselines**: Performance is compared against state-of-the-art text compression schemes like ZPAQ [5] (1.4 bpc on text8) and paq8h [12] (1.2 bpc on text8). Standalone zlib is also used for comparison (2.8 bpc on the Project Gutenberg book).\n*   **Key Findings**:\n    *   **text8 dataset (1MB section)**: The asymptotic upper bound on entropy `H_ub` is estimated at 0.709 bits/character. LLaMA+AC achieves a compression ratio of 0.7101 bits/character, outperforming ZPAQ and paq8h. LLaMA+zlib and LLaMA+TbyT also show improved performance.\n    *   **Project Gutenberg book (100KB section)**: `H_ub` is estimated at 0.8417 bits/character. LLaMA+AC achieves 0.8426 bits/character.\n    *   **Memory (M) dependence**: Compression performance improves with increasing LLM memory (M), as shown in Table II.\n    *   **Variance**: Results show variance depending on input text, and comparisons are made with different input sizes.\n    *   **Optimism**: Results on text8 might be optimistic due to LLaMA-7B's training on Wikipedia.\n*   **Tables**:\n    *   **Table I**: Detailed results for 1MB of text from the text8 dataset, showing `N_c`, `N_T`, `H_ub`, `\u03c1_LLaMA+zlib`, `\u03c1_LLaMA+TbyT`, `\u03c1_LLM+AC`, and baseline `ZPAQ` and `pq8h` values.\n    *   **Table II**: Compression performance on the text8 dataset as a function of LLM memory `M` (31, 127, 255, 511 tokens).\n    *   **Table III**: Mean and standard deviation of entropy bounds and compression ratios over 10 batches of 100,000 tokens for different `M`.\n    *   **Table IV**: Compression performance on the Project Gutenberg book [7] as a function of LLM memory `M`, including standalone Zlib results.\n\n**Acknowledgement (V. Acknowledgement):**\nThe authors will acknowledge Andreas Kirsch for discussions on arithmetic coding.\n\n**References:**\nA list of 12 references will be provided, including foundational works by Shannon [3], Cover and King [1], and works on arithmetic coding [4, 9], RNN-based compression [5], LLaMA [6], SentencePiece [11], and the datasets used [7, 12].", "duration_ms": 32802}

Thought: The paper "LLMZip: Lossless Text Compression using Large Language Models" (arXiv:2306.04050v2) has been successfully downloaded. I now have the full content of the document. My next step is to extract the information that is likely to appear later in the document, specifically focusing on facts, equations, definitions, datasets, and references, to create a concise textual synopsis.

Based on the document prefix and the full content, the likely future content will include:

*   **III. COMPRESSION USING LLMS**: This section details the formal methodology of LLMZip, including the definition of tokens, the role of the language model in producing probability mass functions (PMFs), and the compression of ranks. It also introduces the compression ratio metric.
    *   **A. Entropy bounds**: This subsection provides the theoretical framework for entropy estimation, including equations relating the entropy of characters (H(S)) to the entropy of tokens (H(X)) and the average number of characters per token (E[B]). It also presents the asymptotic upper bound on the entropy rate H(S) (Equation 6).
    *   **B. Encoding schemes**: This subsection describes the three specific compression schemes considered:
        1.  **Compressing the ranks using zlib (LLaMA+zlib)**.
        2.  **Token-by-Token Compression (LLaMA+TbyT)**, including its compression ratio formula.
        3.  **Arithmetic Coding (LLM+AC)**, highlighting its near-optimality and its compression ratio formula (Equation 7).
*   **IV. RESULTS**: This section presents the experimental findings.
    *   **LLaMA-7B and SentencePiece tokenizer**: Details on the specific LLM and tokenizer used.
    *   **Datasets**: Discussion of the text8 dataset and a recently released book from Project Gutenberg [7].
    *   **Baselines**: Comparison with state-of-the-art text compression schemes like ZPAQ and paq8h.
    *   **Tables**: Tables I, II, III, and IV present quantitative results:
        *   Table I: Results for 1MB of text from the text8 dataset, showing H_ub, compression ratios for LLaMA+zlib, LLaMA+TbyT, LLaMA+AC, and comparisons with ZPAQ and pq8h.
        *   Table II: Compression performance as a function of LLM memory (M) on the text8 dataset.
        *   Table III: Mean and standard deviation of entropy bounds and compression ratios over multiple batches.
        *   Table IV: Compression performance on the Project Gutenberg book [7], including a comparison with standalone zlib.
    *   **Key findings**: Specific entropy estimates (e.g., 0.709 bits/character for text8, 0.85 bits/character for the book) and compression ratios (e.g., 0.7101 bits/character for LLaMA+AC on text8, 0.8426 bits/character on the book).
    *   **Limitations**: Discussion on the potential optimism of text8 results due to LLaMA-7B's training corpus and the differing input sizes for comparisons.
*   **V. ACKNOWLEDGEMENT**: A brief section acknowledging contributions.
*   **REFERENCES**: A list of cited works.

I will now synthesize this information into a concise textual synopsis.The document "LLMZip: Lossless Text Compression using Large Language Models" will further detail its methodology, experimental results, and conclusions.

**Methodology (III. Compression Using LLMs):**
The paper will formally define the compression process. It will explain how a sentence `s` of `N_c` characters is parsed into `N_T` tokens `x = x_1, ..., x_{N_T}` using a dictionary `X`. A language model with memory `M` predicts the next token `x_i` by producing a probability mass function `q_i(x_i) := Pr(X_i = x_i | x_{i-1}, ..., x_{i-M})`. The rank `r_i` of the actual token `x_i` in the sorted PMF `~q_i` is then computed. The compression ratio `ρ` is defined as `N_b / N_c` bits/character, where `N_b` is the number of bits after compression.

**Entropy Bounds (III.A. Entropy bounds):**
The paper will establish a theoretical connection between the entropy of characters `H(S)` and the entropy of tokens `H(X)`. Key equations will include:
*   `m_i(s) = min n {length(T(s^n)) >= i}` (Equation 1), defining the character index for the i-th token.
*   `H(S) = H(X) / E[B]` almost surely, where `E[B]` is the average number of characters per token.
*   An asymptotic upper bound on the entropy rate `H(S)`: `H_ub <= lim_{N_T->∞} (-1/N_T) Σ_{i=1}^{N_T} log_2 q_i(X_i) / E[B]` (Equation 6). This bound is compared to previous estimates like 1.3 bits/character by Cover and King [1].

**Encoding Schemes (III.B. Encoding schemes):**
Three specific lossless compression schemes will be described:
1.  **LLaMA+zlib**: Compressing the sequence of ranks `r_i` using the zlib algorithm.
2.  **LLaMA+TbyT (Token-by-Token Compression)**: Using a time-varying prefix-free code (e.g., Huffman code) with codeword length `l_i = ⌈log_2(1/q_i(x_i))⌉`. The compression ratio `ρ_LLaMA+TbyT` is given by `(Σ_{i=1}^{N_T} ⌈log_2(1/q_i(x_i))⌉) / (Σ_{i=1}^{N_T} b_i)`.
3.  **LLM+AC (Arithmetic Coding)**: Combining the LLM's output probabilities `q_i(x_i)` with arithmetic coding, which is expected to be nearly optimal. The compression ratio `ρ_LLM+AC` is approximated by `(Σ_{i=1}^{N_T} log_2(1/q_i(x_i))) / (Σ_{i=1}^{N_T} b_i)` (Equation 7).

**Results (IV. Results):**
This section will present the empirical performance of LLMZip.
*   **LLM and Tokenizer**: LLaMA-7B [6] is used as the language model, and SentencePiece tokenizer [11] with a dictionary size of 32,000.
*   **Datasets**: Experiments are conducted on the `text8` dataset (derived from Wikipedia) and a recently published book from Project Gutenberg [7] (released May 25, 2023).
*   **Baselines**: Performance is compared against state-of-the-art text compression schemes like ZPAQ [5] (1.4 bpc on text8) and paq8h [12] (1.2 bpc on text8). Standalone zlib is also used for comparison (2.8 bpc on the Project Gutenberg book).
*   **Key Findings**:
    *   **text8 dataset (1MB section)**: The asymptotic upper bound on entropy `H_ub` is estimated at 0.709 bits/character. LLaMA+AC achieves a compression ratio of 0.7101 bits/character, outperforming ZPAQ and paq8h. LLaMA+zlib and LLaMA+TbyT also show improved performance.
    *   **Project Gutenberg book (100KB section)**: `H_ub` is estimated at 0.8417 bits/character. LLaMA+AC achieves 0.8426 bits/character.
    *   **Memory (M) dependence**: Compression performance improves with increasing LLM memory (M), as shown in Table II.
    *   **Variance**: Results show variance depending on input text, and comparisons are made with different input sizes.
    *   **Optimism**: Results on text8 might be optimistic due to LLaMA-7B's training on Wikipedia.
*   **Tables**:
    *   **Table I**: Detailed results for 1MB of text from the text8 dataset, showing `N_c`, `N_T`, `H_ub`, `ρ_LLaMA+zlib`, `ρ_LLaMA+TbyT`, `ρ_LLM+AC`, and baseline `ZPAQ` and `pq8h` values.
    *   **Table II**: Compression performance on the text8 dataset as a function of LLM memory `M` (31, 127, 255, 511 tokens).
    *   **Table III**: Mean and standard deviation of entropy bounds and compression ratios over 10 batches of 100,000 tokens for different `M`.
    *   **Table IV**: Compression performance on the Project Gutenberg book [7] as a function of LLM memory `M`, including standalone Zlib results.

**Acknowledgement (V. Acknowledgement):**
The authors will acknowledge Andreas Kirsch for discussions on arithmetic coding.

**References:**
A list of 12 references will be provided, including foundational works by Shannon [3], Cover and King [1], and works on arithmetic coding [4, 9], RNN-based compression [5], LLaMA [6], SentencePiece [11], and the datasets used [7, 12].
C:\tools\Anaconda3\Lib\site-packages\paramiko\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "cipher": algorithms.TripleDES,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.
  "class": algorithms.Blowfish,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "class": algorithms.TripleDES,
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\_internal\_generate_schema.py:628: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.
  warn(
- Reading configuration...
  - Reading registry from registry.yaml
  - Reading catalog from docker-mcp.yaml
  - Reading config from config.yaml
- Configuration read in 15.8328ms
- Watching registry at C:\Users\Noah\.docker\mcp\registry.yaml
- Watching config at C:\Users\Noah\.docker\mcp\config.yaml
- Those servers are enabled: arxiv-mcp-server, duckduckgo, wikipedia-mcp
- Using images:
  - mcp/arxiv-mcp-server@sha256:6dc6bba6dfed97f4ad6eb8d23a5c98ef5b7fa6184937d54b2d675801cd9dd29e
  - mcp/duckduckgo@sha256:68eb20db6109f5c312a695fc5ec3386ad15d93ffb765a0b4eb1baf4328dec14f
  - mcp/wikipedia-mcp@sha256:e90e3f6e3bb795f10d3d8d8ac1523f6d46d4f2ab5ac3b3a4720e1be879466472
> Images pulled in 78.0947ms
- Listing MCP tools...
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
  - Running mcp/duckduckgo with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=duckduckgo -l docker-mcp-transport=stdio]
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers:ro]
  > duckduckgo: (2 tools)
  > wikipedia-mcp: (10 tools) (9 resourceTemplates)
  > arxiv-mcp-server: (4 tools) (1 prompts)
> 16 tools listed in 2.6569698s
- Watching for configuration updates...
> Initialized in 2.7508973s
> Start stdio server
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\fields.py:1093: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'items', 'anyOf', 'enum', 'properties'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
  warn(
┌────────────────────────── Crew Execution Started ───────────────────────────┐
│                                                                             │
│  Crew Execution Started                                                     │
│  Name: crew                                                                 │
│  ID: 7cdf0cfa-08e9-4852-a845-f785163dace1                                   │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

🚀 Crew: crew
├── 📋 Task: 641b2f3e-e8d5-4289-80b2-840f8c9cbb9d
│   Status: Executing Task...
│   └── ✅ Reasoning Completed
├── 🔧 Using create_reasoning_plan from LLM available_function
├── 🔄 Tool Usage Started
├── 🔧 Using create_reasoning_plan from LLM available_function
└── ✅ Tool Usage Completed┌───────────────────────────── 🧠 Reasoning Plan ─────────────────────────────┐
│                                                                             │
│  1. Analyze the document prefix: I will carefully read the provided         │
│  document prefix to identify the paper's core topic (lossless text          │
│  compression using LLMs), the specific LLM used (LLaMA-7B), the proposed    │
│  method (LLMZip, combining LLaMA-7B with arithmetic coding), key findings   │
│  (new entropy estimates, improved compression ratios), and the datasets     │
│  mentioned (text8, [7]). I will also note the stated goal of the paper.     │
│  2. Access the full document: I will use the `read_paper` tool with the     │
│  provided arXiv ID (2306.04050v2) to retrieve the complete content of the   │
│  paper. This is the most direct and authoritative way to obtain             │
│  information that will appear later in the document.                        │
│  3. Identify key sections for information extraction: Once the full paper   │
│  is retrieved, I will identify the sections that typically follow the       │
│  introduction and intuitive explanation. These are expected to include:     │
│      *   Detailed methodology of LLMZip (e.g., "III. LLMZip: The Proposed   │
│  Scheme").                                                                  │
│      *   Experimental setup (e.g., "IV. Experimental Setup," detailing      │
│  datasets, evaluation metrics, and implementation).                         │
│      *   Results and discussion (e.g., "V. Results," presenting             │
│  quantitative findings, tables, figures, and comparisons).                  │
│      *   Conclusion and future work (e.g., "VI. Conclusion").               │
│      *   References.                                                        │
│  4. Extract relevant content: I will systematically read through these      │
│  identified sections to extract specific facts, equations, definitions,     │
│  details about datasets, and references. I will focus on:                   │
│      *   The precise algorithm or scheme of LLMZip.                         │
│      *   Any mathematical formulations or key equations.                    │
│      *   Detailed descriptions of the text8 dataset and the dataset from    │
│  [7].                                                                       │
│      *   The exact entropy estimates and compression ratios achieved by     │
│  LLMZip.                                                                    │
│      *   The specific performance comparisons against BSC, ZPAQ, and        │
│  paq8h.                                                                     │
│      *   Any new definitions introduced.                                    │
│      *   Important references that are central to the paper's claims or     │
│  background.                                                                │
│  5. Synthesize the concise textual synopsis: I will compile the extracted   │
│  information into a concise textual synopsis. This synopsis will be         │
│  structured to provide a clear overview of the paper's future content,      │
│  focusing on the elements requested (facts, equations, definitions,         │
│  datasets, references). I will ensure that the synopsis is highly relevant  │
│  to the document's theme and strictly avoids any information not supported  │
│  by the retrieved paper content.                                            │
│  6. Review and refine: I will review the generated synopsis to ensure it    │
│  is concise, accurate, comprehensive within the given constraints, and      │
│  directly addresses all aspects of the prompt, particularly the             │
│  requirement to avoid hallucination and maximize relevance.                 │
│                                                                             │
│  Challenges and Mitigation:                                                 │
│  *   **Tool output format:** If `read_paper` returns the entire paper as a  │
│  single string, I will manually parse it to identify sections and extract   │
│  information. If it returns a structured object, I will leverage that       │
│  structure.                                                                 │
│  *   **Information overload:** The full paper might contain a lot of        │
│  detail. I will focus on extracting only the most critical and              │
│  summary-level information relevant to the synopsis, prioritizing key       │
│  findings, methods, and comparative results.                                │
│  *   **Ensuring conciseness:** I will use bullet points or short,           │
│  declarative sentences to present the information efficiently, avoiding     │
│  verbose explanations.                                                      │
│                                                                             │
│  Strategic Tool Usage:                                                      │
│  *   `read_paper(paper_id='2306.04050v2')`: This is the primary and most    │
│  crucial tool. It will be used once at the beginning to get the full        │
│  document content. No other tools are anticipated to be necessary if        │
│  `read_paper` provides the complete text, as the task specifically asks     │
│  for content *from the same document*. If `read_paper` fails or provides    │
│  insufficient detail, I would then consider `search_papers` or              │
│  `get_sections` as fallback, but the initial plan relies solely on          │
│  `read_paper` for direct access to the source.                              │
│                                                                             │
│  Expected Outcome: A concise textual synopsis (plain text) detailing the    │
│  likely future content of the "LLMZip" paper, including its detailed        │
│  methodology, experimental setup, quantitative results (entropy estimates,  │
│  compression ratios), comparisons with other compression schemes, and key   │
│  references, all derived directly from the full paper content. This will    │
│  directly fulfill the task requirements and align with my goal of           │
│  providing accurate and efficient information.                              │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Task:[00m [92mYou act as a deterministic research agent. Given the observed prefix of a document, use available MCP tools to retrieve information that is likely to appear later in the same document. Prefer authoritative sources. Produce a concise textual synopsis of the likely future content (facts, equations, definitions, datasets, references), suitable for conditioning a language model compressor. Maximize relevance to the document. Do not hallucinate; only include content supported by the tools.

Document prefix (UTF-8 text):
# LLMZip: Lossless Text Compression using Large Language Models

**Authors:** Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas Shakkottai  
**Affiliation:** Department of Electrical and Computer Engineering, Texas A&M University  
**Email:** {vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu  
**arXiv:** 2306.04050v2 [cs.IT] 26 Jun 2023

## Abstract

We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.

## I. INTRODUCTION

There are close connections between learning, prediction, and compression. The success of ChatGPT has captured the fascination of general public and brought the connection between learning and prediction to the fore. The main advance brought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in a paragraph based on knowing the past several words (tokens).

The connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the entropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past values can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many algorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of lossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very effective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor and every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression performance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved results for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ for text compression.

It is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the entropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this paper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the entropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates provided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of the text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for compression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression ratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio obtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.

## II. INTUITIVE EXPLANATION OF THE MAIN IDEA

We will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3] for estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable length and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence that reads as

My first attempt at writing a book

Our goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can be reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)

′My′, ′first′, ′attempt′, ′at′, ′writing′, ′a′, ′book′

A language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the past M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in Figure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be words such as ′reading′,′ writing′,′ driving′,′ cooking′ etc. The main idea is to compute the rank of the actual word in our sentence (′writing′) in this list and call it R5. We will assume that the ranks start at 0 i.e., the most likely word has rank 0, the second most likely word has rank 1, and so on. In this example, the rank for ′writing′ is R5 = 1.

**Fig. 1.** Schematic showing the prediction at epoch 5 for a language model with memory 4.

| Next word | Probability |
|-----------|-------------|
| reading  | 0.3        |
| writing  | 0.2        |
| cycling  | 0.1        |
| driving  | 0.05       |
| ...      | ...        |

R5 = 1

Then, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2 through 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same word in the sentence that we wish to encode, ′a′, and hence, the rank R6 would be 0.

**Fig. 2.** Schematic showing the prediction at epoch 6 for a language model with memory 4.

| Next word | Probability |
|-----------|-------------|
| a        | 0.7        |
| an       | 0.2        |
| the      | 0.05       |
| ...

Output: A concise synopsis (plain text).

Reasoning Plan:
1. Analyze the document prefix: I will carefully read the provided document prefix to identify the paper's core topic (lossless text compression using LLMs), the specific LLM used (LLaMA-7B), the proposed method (LLMZip, combining LLaMA-7B with arithmetic coding), key findings (new entropy estimates, improved compression ratios), and the datasets mentioned (text8, [7]). I will also note the stated goal of the paper.
2. Access the full document: I will use the `read_paper` tool with the provided arXiv ID (2306.04050v2) to retrieve the complete content of the paper. This is the most direct and authoritative way to obtain information that will appear later in the document.
3. Identify key sections for information extraction: Once the full paper is retrieved, I will identify the sections that typically follow the introduction and intuitive explanation. These are expected to include:
    *   Detailed methodology of LLMZip (e.g., "III. LLMZip: The Proposed Scheme").
    *   Experimental setup (e.g., "IV. Experimental Setup," detailing datasets, evaluation metrics, and implementation).
    *   Results and discussion (e.g., "V. Results," presenting quantitative findings, tables, figures, and comparisons).
    *   Conclusion and future work (e.g., "VI. Conclusion").
    *   References.
4. Extract relevant content: I will systematically read through these identified sections to extract specific facts, equations, definitions, details about datasets, and references. I will focus on:
    *   The precise algorithm or scheme of LLMZip.
    *   Any mathematical formulations or key equations.
    *   Detailed descriptions of the text8 dataset and the dataset from [7].
    *   The exact entropy estimates and compression ratios achieved by LLMZip.
    *   The specific performance comparisons against BSC, ZPAQ, and paq8h.
    *   Any new definitions introduced.
    *   Important references that are central to the paper's claims or background.
5. Synthesize the concise textual synopsis: I will compile the extracted information into a concise textual synopsis. This synopsis will be structured to provide a clear overview of the paper's future content, focusing on the elements requested (facts, equations, definitions, datasets, references). I will ensure that the synopsis is highly relevant to the document's theme and strictly avoids any information not supported by the retrieved paper content.
6. Review and refine: I will review the generated synopsis to ensure it is concise, accurate, comprehensive within the given constraints, and directly addresses all aspects of the prompt, particularly the requirement to avoid hallucination and maximize relevance.

Challenges and Mitigation:
*   **Tool output format:** If `read_paper` returns the entire paper as a single string, I will manually parse it to identify sections and extract information. If it returns a structured object, I will leverage that structure.
*   **Information overload:** The full paper might contain a lot of detail. I will focus on extracting only the most critical and summary-level information relevant to the synopsis, prioritizing key findings, methods, and comparative results.
*   **Ensuring conciseness:** I will use bullet points or short, declarative sentences to present the information efficiently, avoiding verbose explanations.

Strategic Tool Usage:
*   `read_paper(paper_id='2306.04050v2')`: This is the primary and most crucial tool. It will be used once at the beginning to get the full document content. No other tools are anticipated to be necessary if `read_paper` provides the complete text, as the task specifically asks for content *from the same document*. If `read_paper` fails or provides insufficient detail, I would then consider `search_papers` or `get_sections` as fallback, but the initial plan relies solely on `read_paper` for direct access to the source.

Expected Outcome: A concise textual synopsis (plain text) detailing the likely future content of the "LLMZip" paper, including its detailed methodology, experimental setup, quantitative results (entropy estimates, compression ratios), comparisons with other compression schemes, and key references, all derived directly from the full paper content. This will directly fulfill the task requirements and align with my goal of providing accurate and efficient information.
  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool download_paper with arguments: {"check_status":false,"paper_id":"2306.04050v2"}
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
  > Calling tool download_paper took: 1.5144575s
  - Scanning tool call response for secrets...
  > No secret found in response.
[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mAction: download_paper[00m
[95m## Using tool:[00m [92mdownload_paper[00m
[95m## Tool Input:[00m [92m
"{\"paper_id\": \"2306.04050v2\"}"[00m
[95m## Tool Output:[00m [92m
{"status": "success", "message": "Paper already available", "resource_uri": "file:///app/papers/2306.04050v2.md", "pdf_uri": "https://arxiv.org/pdf/2306.04050v2.pdf", "content": "1\n\n# LLMZip: Lossless Text Compression using Large Language Models\n\n\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\n\n\nAbstract\n\n\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\n\n\nI. I NTRODUCTION\n\n\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\n\n\nII. I NTUITIVE EXPLANATION OF THE MAIN IDEA\n\n\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\n\nMy first attempt at writing a book\n\n\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032, \u2032 a \u2032, \u2032 book \u2032\n\n\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n\n\n2\n\n\nFigure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be\nwords such as [\u2032] reading [\u2032], [\u2032] writing [\u2032], [\u2032] driving [\u2032], [\u2032] cooking [\u2032] etc. The main idea is to compute the rank of the actual word in\nour sentence ( [\u2032] writing [\u2032] ) in this list and call it R 5 . We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for [\u2032] writing [\u2032] is R 5 = 1.\n\n\nTokens\n\n\n\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x5|x4 1)<br>5|Rank<br>computation|R5 = 1|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nreading 0.3\nwriting 0.2\ncycling 0.1\ndriving 0.05\n\n... ...\n\n\n\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\n\n\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, [\u2032] a [\u2032], and hence, the rank R 6 would be 0.\n\n\nTokens\n\n\n\n\n\n\u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x6|x5 2)<br>6|Rank<br>computation|R6 = 0|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nan 0.2\n\nthe 0.05\n\n... ...\n\n\n\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\n\n\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n\n\n1, 0, 0, . . .\n\n\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\n\n\nSequence of ranks\n\n|e of ranks|Col2|Col3|\n|---|---|---|\n|. ., rNT|Compression (zip)|Nb|\n||||\n\n\n\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\n\n\nWhen we wish to reconstruct the sequence, we first decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank R i during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\n\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n\n\n3\n\n\nIII. C OMPRESSION USING LLM S\n\n\nLet s denote a sentence from the English language composed of N c letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We first parse s into a sequence of N T tokens\ndenoted by x = x 1, x 2, . . ., x i\u22121, x i, x i+1, . . . x N T, where x i \u2208X . There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. x i \u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter X i .\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens x i\u2212M, x i\u2212M+1, . . ., x i\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nq i (x i ) := Pr(X i = x i |x i\u22121, x i\u22122, . . ., x i\u2212M ), \u2200x i \u2208X . The PMF vector q i := [q i (1), q i (2), . . ., q i (D)] [T] is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcq i . Let \u03b3 i : X \u2192X be a permutation on the integers\nfrom 1 to D such that\n\n\u02dc\nq i (\u03b3 i (j)) = q i (j), \u2200j \u2208X .\n\n\nThat is, \u03b3 i (j) is the rank of the token j at epoch i. We define the rank of the input sequence at epoch i as the rank of the\ntoken x i at epoch i, r i := \u03b3 i (x i ). The sequence {r i } [N] i=1 [T] [is compressed by a lossless compression algorithm (such as zlib) to]\nproduce N b bits which are the final bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors q i \u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 defined as\n\n\u03c1 := [N] [b] bits/character.\n\nN c\n\n\n\nLosseless\n\nCompression\n\n\n\nTokens . . ., x i\u2212M, . . ., x i\u22121\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n\nInput sentence s with N c characters\n\n\n\ni i i\u22121 Rank\n\nLLM\ncomputation\n\n\n\nq i = Pr(X i = j|x [i] i [\u2212] \u2212 [M] 1 [)] Rank r i Losseless N b bits\n\n\n\nFig. 4. Schematic showing the prediction at epoch i.\n\n\nA. Entropy bounds\n\n\nLet S \u2208S [\u221e] be a random process that represents language input. The nth character in the sequence is denoted by S n,\nwhereas the string of characters from the beginning to the nth character is expressed as S n . The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X 1, X 2, . . . using a variable-length mapping. In this sequence, X i is the ith\ntoken. The number of characters employed to generate X i depends on the realization of the random process and, as such,\nwe introduce random variable B i to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which B i \u2265 1 and B i is uniformly bounded, with B i < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded N T \u2192\u221e, the number of characters must also approach infinity N c \u2192\u221e. Formally, consider the tokenizer\nfunction T : S [N] \u2192X [N] operating on infinite symbol sequences; that is, T (s) = x where s is an infinite sequence in S [\u221e] . For\nnatural number, i \u2208 N, define m i : S [N] \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Specifically, suppose s is given, then\n\n\nm i (s) = min n [{][length (][T][ (][s] [n] [))][ \u2265] [i][}][ .] (1)\n\n\nWe note that, by construction, lim n\u2192\u221e length (T (s n )) = \u221e and, as such, m i (\u00b7) is well-defined. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple finite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (s n ) = T (s n+1 ). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (finite) series of input symbols in the restricted\nset for which T (y n ) = x i is s m i (s) . Given a fixed sequence s, we can express the number of characters contained in a token\n\nas\nb i = m i (s) \u2212 m i\u22121 (s)\n\n\n4\n\n\nwith initial condition m \u22121 = 0. Consequently, the number of characters embedded in the first N T tokens for a random input\nbecomes N c = [\ufffd] i [N] =1 [T] [B] [i] [.]\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {S k } [\u221e] k=1 [,]\n{B i } [\u221e] i=1 [, and][ {][X] [i] [}] [\u221e] i=1 [are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that]\n\n\n\n\u2212 [1]\n\n\n\n\n[1]\n\nn [log] [2] [ p] [S] [n] [(][S] [1] [, . . ., S] [n] [) =][ \u2212] n [1]\n\n\n\nn [log] [2] [ p] [S] [n] [(][S] [n] [)][ \u2192] [H][(][S][)] almost surely. (2)\n\n\n\nLet \u2126 S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\n\n\n\n[1]\n\ni [log] [2] [ p] [X] [i] [(][X] [1] [, . . ., X] [i] [) =][ \u2212] [1] i\n\n\n\n\u2212 [1]\n\n\n\ni [log] [2] [ p] [X] [i] [(][X] [i] [)][ \u2192] [H][(][X][)] almost surely. (3)\n\n\n\nDefine \u2126 X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\n\n\nm i (S)\nlim = E [B] almost surely. (4)\ni\u2192\u221e i\n\n\nSet \u2126 B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126 S \u2229 \u2126 X \u2229 \u2126 B, we deduce that\n\n\nH(S) = lim\nk\u2192\u221e [\u2212] k [1] [log] [2] [ p] [S] [k] [(][S] [k] [(][\u03c9][))]\n\n= lim i\u2192\u221e [\u2212] l [1] i log 2 p S li (S l i (\u03c9))\n\n\n= lim log 2 Pr (X i = T (S l i (\u03c9)))\ni\u2192\u221e [\u2212] l [1] i\n\n\n\n1\n\ni [log] [2] [ Pr (][X] [i] [ =][ x] [i] [) =][ H] E [(] B [X][)]\n\n\n\n1 1\n\n= \u2212\nE[B] i [lim] \u2192\u221e i\n\n\n\nE[B] [.]\n\n\n\nThe first equality follows from (2). The second equality is a consequence of the fact that {l i = m i (S(\u03c9))|i \u2208 N} is an infinite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\n\ntwo events,\n{\u03c9 \u2208 \u2126|X i (\u03c9) = x i } = {\u03c9 \u2208 \u2126|T (S m i (S(\u03c9)) ) = x i }.\n\n\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126 B . The sets \u2126 S, \u2126 X, and \u2126 B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\n\nH(S) = [H][(][X][)] almost surely.\n\nE[B]\n\n\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\n\n\n\nPr\n\n\ufffd\n\n\n\n\ufffd\n\n\n\nH(X) \u2264 lim\nN T \u2192\u221e [\u2212] N [1] T\n\n\n\nN T\n\ufffd\n\n\n\n\ufffd log 2 q i (X i )\n\n\ni=1\n\n\n\n= 1, (5)\n\n\n\nwhere q i (\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\n\n\n\n\n[1] N T\n\nN T \ufffd i=1 [log] 2 [q] [i] [(][X] [i] [)]\n\n\n\nH(S) \u2264 lim N T \u2192\u221e \u2212 N [1]\n\n\n\n. (6)\nE[B]\n\n\n\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by H ub . The\nnumerator in (6) represents the average number of bits required to represent the tokens X N T and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\n\n\nB. Encoding schemes\n\n\nWe consider three schemes for the lossless compression block in Fig. 3.\n\n\n5\n\n\n1) Compressing the ranks using zlib: The first scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1 LLaMA+zlib .\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token x i at epoch i by using a prefix-free code assuming q i to be the true distribution of\nthe tokens. A natural choice for a prefix-free code is a Huffman code. Instead, for simplicity, we use a prefix-free code where\n1\nthe codeword for the token x i is of length l i = \u2308log 2 q i (x i ) [\u2309][. A prefix-free code with this length for][ x] [i] [ is guaranteed to exist]\nsince this choice of lengths satisfies the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1 LLaMA+TbyT,\nis given by\n\n\n\nN T\n\ufffd\n\n\ni=1\n\n\n\n1\nlog 2\n\ufffd q i (x i )\n\n\n\n\ufffd\n\n\n\n\u03c1 LLaMA+TbyT =\n\n\n\n\ufffd Ni=1 T [b] [i] .\n\n\n\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use q i (x i ) as the probability of token x i at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1 LLM+AC . It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\n\n\n\u03c1 LLM+AC \u2248\n\n\n\nN T 1\n\ufffd i=1 log 2 q i (x i )\n\n\ufffd Ni=1 T [b] [i] . (7)\n\n\n\nClearly, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and \u03c1 LLM+AC also provide upper bounds on H(S). H ub, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and\n\u03c1 LLM+AC are estimated using a finite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\n\n\nIV. R ESULTS\n\n\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it difficult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\n[Nevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of](http://mattmahoney.net/dc/text8.zip)\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M ) is shown in Table II. As expected, the\ncompression performance improves with increasing M . We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n\n\n6\n\n\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\n\n\nV. A CKNOWLEDGEMENT\n\n\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\n\n\nTABLE I\n\nR ESULTS FOR 1MB OF TEXT FROM TEXT 8 DATASET\n\n\n\nBatch\nN c N T\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC ZPAQ pq8h\n\n\n\nNo. (bpc) file size (bits) (bpc) (bpc) (bpc) (bpc)\n\n\n1 466, 650 100, 000 0.6882 1.0513 0.8215 0.689\n\n\n2 461, 477 100, 000 0.6893 1.0558 0.8242 0.6901\n\n\n3 454, 599 100, 000 0.699 1.0681 0.8357 0.6999\n\n\n4 462, 755 100, 000 0.6748 1.0346 0.8093 0.6757\n\n\n5 453, 847 100, 000 0.7481 1.1265 0.8831 0.749\n\n\n6 458, 252 100, 000 0.7218 1.0957 0.8567 0.7227\n\n\n7 451, 036 100, 000 0.6959 1.0729 0.8353 0.6968\n\n\n8 447, 953 100, 000 0.7092 1.0896 0.8489 0.7101\n\n\n9 462, 665 100, 000 0.7394 1.1126 0.8713 0.7402\n\n\n10 449, 621 100, 000 0.7269 1.1046 0.8643 0.7277\n\n\nTotal 9, 137, 710 2, 000, 000 0.7093 1.0812 0.845 0.7101 1.4 [1] 1.2 [2]\n\n\nTABLE II\n\nC OMPRESSION PERFORMANCE OF THE LLM ON THE TEXT 8 DATASET, AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\n\n\n(bpc) file size (bits) (bpc) (bpc)\n\n\n\n31 4, 568, 855 1, 000, 000 0.9139 1.3159 1.0425 0.9145\n\n\n127 4, 568, 855 1, 000, 000 0.7511 1.1303 0.8847 0.752\n\n\n255 4, 568, 855 1, 000, 000 0.7242 1.0985 0.859 0.725\n\n\n511 4, 568, 855 1, 000, 000 0.7093 1.0812 0.845 0.7101\n\n\n1 This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2 This result is taken from [12] and it corresponds to the full 100MB dataset text8\n\n\n7\n\n\n\nTABLE III\n\nM EAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\nM\n\n(bpc) (bpc) (bpc) (bpc)\n\n\n31 0.9139 \u00b1 0.0263 1.3159 \u00b1 0.0329 1.0425 \u00b1 0.0262 0.9145 \u00b1 0.0263\n\n\n127 0.7511 \u00b1 0.0233 1.1303 \u00b1 0.0292 0.8847 \u00b1 0.0231 0.752 \u00b1 0.0233\n\n\n255 0.7242 \u00b1 0.0234 1.0985 \u00b1 0.0289 0.859 \u00b1 0.0232 0.725 \u00b1 0.0234\n\n\n511 0.7093 \u00b1 0.0228 1.0812 \u00b1 0.028 0.845 \u00b1 0.0226 0.7101 \u00b1 0.0228\n\n\nTABLE IV\n\nC OMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN P ROJECT G UTENBERG [7], AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC Standalone Zlib\n\n\n(bpc) (bpc) (bpc) (bpc) (bpc)\n\n\n\n31 508, 463 115, 000 1.0919 1.5316 1.2152 1.0924 2.80\n\n\n127 508, 463 115, 000 0.8973 1.3128 1.0235 0.8982 2.80\n\n\n255 508, 463 115, 000 0.8618 1.2684 0.9899 0.8627 2.80\n\n\n511 508, 463 115, 000 0.8417 1.2465 0.9711 0.8426 2.80\n\n\n8\n\n\nR EFERENCES\n\n\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR filters),\u201d 2023.\n\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and efficient foundation language models,\u201d\n2023.\n\n[7] J. Frank Dobie, Legends of Texas, United States, Texas Folk-Lore Society, 1924; Project Gutenberg, May 25, 2023, 2023,\n[https://www.gutenberg.org/ebooks/70859.](https://www.gutenberg.org/ebooks/70859)\n\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n\n\n"}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Final Answer:[00m [92m
The document "LLMZip: Lossless Text Compression using Large Language Models" will detail the proposed LLMZip scheme, which combines the LLaMA-7B large language model with lossless compression.

**Key Definitions and Concepts:**
*   **Tokens**: Variable-length groups of letters.
*   **Language Model Memory (M)**: Number of past tokens used for prediction.
*   **Probability Mass Function (q_i(x_i))**: Probability of the next token `x_i` given past `M` tokens.
*   **Rank (r_i)**: The position of the actual next token `x_i` in the rank-ordered list of predicted tokens.
*   **Compression Ratio (ρ)**: Defined as `N_b / N_c` bits/character, where `N_b` is the number of bits and `N_c` is the number of characters.
*   **Entropy Rate (H(S))**: The asymptotic upper bound on the entropy of English, `H_ub`, is derived as `lim_{N_T->inf} (-1/N_T * sum_{i=1}^{N_T} log_2 q_i(X_i)) / E[B]`, where `E[B]` is the average number of characters per token.

**Proposed Encoding Schemes:**
1.  **LLaMA+zlib**: Compresses the sequence of predicted ranks using the zlib algorithm.
2.  **LLaMA+Token-by-Token (LLaMA+TbyT)**: Uses a time-varying prefix-free code (e.g., Huffman-like) with codeword length `ceil(log_2(1/q_i(x_i)))` for each token.
3.  **LLM+Arithmetic Coding (LLM+AC)**: Combines the LLM's output probabilities `q_i(x_i)` with arithmetic coding, expected to be nearly optimal with a compression ratio `ρ_LLM+AC ≈ (sum_{i=1}^{N_T} log_2(1/q_i(x_i))) / (sum_{i=1}^{N_T} b_i)`.

**Experimental Setup and Datasets:**
*   **LLM**: LLaMA-7B.
*   **Tokenizer**: SentencePiece tokenizer with a dictionary size of 32000.
*   **Datasets**:
    *   `text8` dataset (from http://mattmahoney.net/dc/text8.zip): Used for benchmarking against state-of-the-art text compression. Experiments conducted on 10 batches of 100,000 tokens (total 1MB).
    *   A recently published book [7] from Project Gutenberg (May 25, 2023): Used to evaluate performance on text not derived from Wikipedia, with 100,000 tokens.
*   **Baselines**: ZPAQ (1.4 bpc on text8 [5]), paq8h (1.2 bpc on text8 [12]), and standalone zlib (2.8 bpc on the Project Gutenberg book).

**Key Results and Findings:**
*   **Entropy Estimates**: The asymptotic upper bound on entropy (`H_ub`) for English is estimated to be 0.7093 bits/character on the `text8` dataset (1MB) and 0.8417 bits/character on the Project Gutenberg book (100KB) for M=511. These estimates are significantly lower than previous estimates (e.g., 1.3 bpc by Cover and King [1], and estimates in [2]).
*   **Compression Performance (LLaMA+AC)**:
    *   On `text8` (1MB): Achieves 0.7101 bits/character, outperforming ZPAQ (1.4 bpc) and paq8h (1.2 bpc).
    *   On Project Gutenberg book (100KB): Achieves 0.8426 bits/character, also outperforming standalone zlib (2.8 bpc) and existing state-of-the-art results for `text8`.
*   **Impact of Memory (M)**: Compression performance improves with increasing LLM memory `M`. For example, `ρ_LLaMA+AC` decreases from 0.9145 bpc (M=31) to 0.7101 bpc (M=511) on `text8`.
*   **Limitations**: The `text8` dataset, being derived from Wikipedia, might lead to optimistic results as LLaMA-7B was likely trained on Wikipedia articles. Comparisons are made with results from different input sizes for state-of-the-art algorithms.

**References**: The paper includes 12 references, covering foundational works in information theory (Shannon [3], Cover and King [1], Cover and Thomas [8]), arithmetic coding (Cleary and Witten [4], Bell et al. [9]), and recent advancements in language models (LLaMA [6]) and neural network-based compression (Goyal et al. [5]).[00m


🚀 Crew: crew
├── 📋 Task: 641b2f3e-e8d5-4289-80b2-840f8c9cbb9d
│   Assigned to: AI Assistant
│   Status: ✅ Completed
│   ├── ✅ Reasoning Completed
│   └── 🔧 Used download_paper (1)
├── 🔧 Using create_reasoning_plan from LLM available_function
├── 🔄 Tool Usage Started
├── 🔧 Using create_reasoning_plan from LLM available_function
└── ✅ Tool Usage Completed┌────────────────────────────── Task Completion ──────────────────────────────┐
│                                                                             │
│  Task Completed                                                             │
│  Name: 641b2f3e-e8d5-4289-80b2-840f8c9cbb9d                                 │
│  Agent: AI Assistant                                                        │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

┌────────────────────────────── Crew Completion ──────────────────────────────┐
│                                                                             │
│  Crew Execution Completed                                                   │
│  Name: crew                                                                 │
│  ID: 7cdf0cfa-08e9-4852-a845-f785163dace1                                   │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

AGENT_RESULT_JSON:{"final_text": "The document \"LLMZip: Lossless Text Compression using Large Language Models\" will detail the proposed LLMZip scheme, which combines the LLaMA-7B large language model with lossless compression.\n\n**Key Definitions and Concepts:**\n*   **Tokens**: Variable-length groups of letters.\n*   **Language Model Memory (M)**: Number of past tokens used for prediction.\n*   **Probability Mass Function (q_i(x_i))**: Probability of the next token `x_i` given past `M` tokens.\n*   **Rank (r_i)**: The position of the actual next token `x_i` in the rank-ordered list of predicted tokens.\n*   **Compression Ratio (\u03c1)**: Defined as `N_b / N_c` bits/character, where `N_b` is the number of bits and `N_c` is the number of characters.\n*   **Entropy Rate (H(S))**: The asymptotic upper bound on the entropy of English, `H_ub`, is derived as `lim_{N_T->inf} (-1/N_T * sum_{i=1}^{N_T} log_2 q_i(X_i)) / E[B]`, where `E[B]` is the average number of characters per token.\n\n**Proposed Encoding Schemes:**\n1.  **LLaMA+zlib**: Compresses the sequence of predicted ranks using the zlib algorithm.\n2.  **LLaMA+Token-by-Token (LLaMA+TbyT)**: Uses a time-varying prefix-free code (e.g., Huffman-like) with codeword length `ceil(log_2(1/q_i(x_i)))` for each token.\n3.  **LLM+Arithmetic Coding (LLM+AC)**: Combines the LLM's output probabilities `q_i(x_i)` with arithmetic coding, expected to be nearly optimal with a compression ratio `\u03c1_LLM+AC \u2248 (sum_{i=1}^{N_T} log_2(1/q_i(x_i))) / (sum_{i=1}^{N_T} b_i)`.\n\n**Experimental Setup and Datasets:**\n*   **LLM**: LLaMA-7B.\n*   **Tokenizer**: SentencePiece tokenizer with a dictionary size of 32000.\n*   **Datasets**:\n    *   `text8` dataset (from http://mattmahoney.net/dc/text8.zip): Used for benchmarking against state-of-the-art text compression. Experiments conducted on 10 batches of 100,000 tokens (total 1MB).\n    *   A recently published book [7] from Project Gutenberg (May 25, 2023): Used to evaluate performance on text not derived from Wikipedia, with 100,000 tokens.\n*   **Baselines**: ZPAQ (1.4 bpc on text8 [5]), paq8h (1.2 bpc on text8 [12]), and standalone zlib (2.8 bpc on the Project Gutenberg book).\n\n**Key Results and Findings:**\n*   **Entropy Estimates**: The asymptotic upper bound on entropy (`H_ub`) for English is estimated to be 0.7093 bits/character on the `text8` dataset (1MB) and 0.8417 bits/character on the Project Gutenberg book (100KB) for M=511. These estimates are significantly lower than previous estimates (e.g., 1.3 bpc by Cover and King [1], and estimates in [2]).\n*   **Compression Performance (LLaMA+AC)**:\n    *   On `text8` (1MB): Achieves 0.7101 bits/character, outperforming ZPAQ (1.4 bpc) and paq8h (1.2 bpc).\n    *   On Project Gutenberg book (100KB): Achieves 0.8426 bits/character, also outperforming standalone zlib (2.8 bpc) and existing state-of-the-art results for `text8`.\n*   **Impact of Memory (M)**: Compression performance improves with increasing LLM memory `M`. For example, `\u03c1_LLaMA+AC` decreases from 0.9145 bpc (M=31) to 0.7101 bpc (M=511) on `text8`.\n*   **Limitations**: The `text8` dataset, being derived from Wikipedia, might lead to optimistic results as LLaMA-7B was likely trained on Wikipedia articles. Comparisons are made with results from different input sizes for state-of-the-art algorithms.\n\n**References**: The paper includes 12 references, covering foundational works in information theory (Shannon [3], Cover and King [1], Cover and Thomas [8]), arithmetic coding (Cleary and Witten [4], Bell et al. [9]), and recent advancements in language models (LLaMA [6]) and neural network-based compression (Goyal et al. [5]).", "duration_ms": 31685}
The document "LLMZip: Lossless Text Compression using Large Language Models" will detail the proposed LLMZip scheme, which combines the LLaMA-7B large language model with lossless compression.

**Key Definitions and Concepts:**
*   **Tokens**: Variable-length groups of letters.
*   **Language Model Memory (M)**: Number of past tokens used for prediction.
*   **Probability Mass Function (q_i(x_i))**: Probability of the next token `x_i` given past `M` tokens.
*   **Rank (r_i)**: The position of the actual next token `x_i` in the rank-ordered list of predicted tokens.
*   **Compression Ratio (ρ)**: Defined as `N_b / N_c` bits/character, where `N_b` is the number of bits and `N_c` is the number of characters.
*   **Entropy Rate (H(S))**: The asymptotic upper bound on the entropy of English, `H_ub`, is derived as `lim_{N_T->inf} (-1/N_T * sum_{i=1}^{N_T} log_2 q_i(X_i)) / E[B]`, where `E[B]` is the average number of characters per token.

**Proposed Encoding Schemes:**
1.  **LLaMA+zlib**: Compresses the sequence of predicted ranks using the zlib algorithm.
2.  **LLaMA+Token-by-Token (LLaMA+TbyT)**: Uses a time-varying prefix-free code (e.g., Huffman-like) with codeword length `ceil(log_2(1/q_i(x_i)))` for each token.
3.  **LLM+Arithmetic Coding (LLM+AC)**: Combines the LLM's output probabilities `q_i(x_i)` with arithmetic coding, expected to be nearly optimal with a compression ratio `ρ_LLM+AC ≈ (sum_{i=1}^{N_T} log_2(1/q_i(x_i))) / (sum_{i=1}^{N_T} b_i)`.

**Experimental Setup and Datasets:**
*   **LLM**: LLaMA-7B.
*   **Tokenizer**: SentencePiece tokenizer with a dictionary size of 32000.
*   **Datasets**:
    *   `text8` dataset (from http://mattmahoney.net/dc/text8.zip): Used for benchmarking against state-of-the-art text compression. Experiments conducted on 10 batches of 100,000 tokens (total 1MB).
    *   A recently published book [7] from Project Gutenberg (May 25, 2023): Used to evaluate performance on text not derived from Wikipedia, with 100,000 tokens.
*   **Baselines**: ZPAQ (1.4 bpc on text8 [5]), paq8h (1.2 bpc on text8 [12]), and standalone zlib (2.8 bpc on the Project Gutenberg book).

**Key Results and Findings:**
*   **Entropy Estimates**: The asymptotic upper bound on entropy (`H_ub`) for English is estimated to be 0.7093 bits/character on the `text8` dataset (1MB) and 0.8417 bits/character on the Project Gutenberg book (100KB) for M=511. These estimates are significantly lower than previous estimates (e.g., 1.3 bpc by Cover and King [1], and estimates in [2]).
*   **Compression Performance (LLaMA+AC)**:
    *   On `text8` (1MB): Achieves 0.7101 bits/character, outperforming ZPAQ (1.4 bpc) and paq8h (1.2 bpc).
    *   On Project Gutenberg book (100KB): Achieves 0.8426 bits/character, also outperforming standalone zlib (2.8 bpc) and existing state-of-the-art results for `text8`.
*   **Impact of Memory (M)**: Compression performance improves with increasing LLM memory `M`. For example, `ρ_LLaMA+AC` decreases from 0.9145 bpc (M=31) to 0.7101 bpc (M=511) on `text8`.
*   **Limitations**: The `text8` dataset, being derived from Wikipedia, might lead to optimistic results as LLaMA-7B was likely trained on Wikipedia articles. Comparisons are made with results from different input sizes for state-of-the-art algorithms.

**References**: The paper includes 12 references, covering foundational works in information theory (Shannon [3], Cover and King [1], Cover and Thomas [8]), arithmetic coding (Cleary and Witten [4], Bell et al. [9]), and recent advancements in language models (LLaMA [6]) and neural network-based compression (Goyal et al. [5]).
C:\tools\Anaconda3\Lib\site-packages\paramiko\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "cipher": algorithms.TripleDES,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.
  "class": algorithms.Blowfish,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "class": algorithms.TripleDES,
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\_internal\_generate_schema.py:628: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.
  warn(
- Reading configuration...
  - Reading registry from registry.yaml
  - Reading catalog from docker-mcp.yaml
  - Reading config from config.yaml
- Configuration read in 22.2229ms
- Watching registry at C:\Users\Noah\.docker\mcp\registry.yaml
- Watching config at C:\Users\Noah\.docker\mcp\config.yaml
- Those servers are enabled: arxiv-mcp-server, duckduckgo, wikipedia-mcp
- Using images:
  - mcp/arxiv-mcp-server@sha256:6dc6bba6dfed97f4ad6eb8d23a5c98ef5b7fa6184937d54b2d675801cd9dd29e
  - mcp/duckduckgo@sha256:68eb20db6109f5c312a695fc5ec3386ad15d93ffb765a0b4eb1baf4328dec14f
  - mcp/wikipedia-mcp@sha256:e90e3f6e3bb795f10d3d8d8ac1523f6d46d4f2ab5ac3b3a4720e1be879466472
> Images pulled in 63.4203ms
- Listing MCP tools...
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
  - Running mcp/duckduckgo with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=duckduckgo -l docker-mcp-transport=stdio]
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers:ro]
  > duckduckgo: (2 tools)
  > wikipedia-mcp: (10 tools) (9 resourceTemplates)
  > arxiv-mcp-server: (4 tools) (1 prompts)
> 16 tools listed in 2.782595s
- Watching for configuration updates...
> Initialized in 2.8708127s
> Start stdio server
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\fields.py:1093: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'items', 'anyOf', 'enum', 'properties'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
  warn(
┌────────────────────────── Crew Execution Started ───────────────────────────┐
│                                                                             │
│  Crew Execution Started                                                     │
│  Name: crew                                                                 │
│  ID: abf55541-aaf9-4fa2-9368-363873c2fdf5                                   │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

🚀 Crew: crew
├── 📋 Task: 36a909db-bc64-46f6-96be-06657d829788
│   Status: Executing Task...
│   └── ✅ Reasoning Completed
├── 🔧 Using create_reasoning_plan from LLM available_function
├── 🔄 Tool Usage Started
├── 🔧 Using create_reasoning_plan from LLM available_function
└── ✅ Tool Usage Completed┌───────────────────────────── 🧠 Reasoning Plan ─────────────────────────────┐
│                                                                             │
│  1. Analyze the document prefix: Carefully read the provided text to        │
│  understand the paper's core topic (lossless text compression using LLMs),  │
│  stated goals (new entropy estimates, LLMZip algorithm), methodologies      │
│  mentioned (LLaMA-7B as predictor, arithmetic coding), preliminary results  │
│  (outperforms BSC, ZPAQ, paq8h), and specific datasets/models referenced    │
│  (LLaMA-7B, text8, dataset [7]).                                            │
│  2. Identify key entities and concepts: Extract names of models (LLaMA-7B,  │
│  GPT-4, RNN), datasets (text8, [7]), compression schemes (BSC, ZPAQ,        │
│  paq8h), authors (Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan,    │
│  Dileep Kalathil, Jean-Francois Chamberland, Srinivas Shakkottai), and key  │
│  terms (entropy, lossless compression, language models, arithmetic coding,  │
│  prediction).                                                               │
│  3. Formulate search queries:                                               │
│      *   Primary: "LLMZip: Lossless Text Compression using Large Language   │
│  Models arXiv: 2306.04050" to find the full paper.                          │
│      *   Secondary (if primary fails or for supplementary details):         │
│          *   "LLaMA-7B arithmetic coding text compression"                  │
│          *   "text8 dataset compression benchmarks"                         │
│          *   "BSC ZPAQ paq8h compression performance"                       │
│          *   "entropy estimation English language large language models"    │
│          *   "Chandra Shekhara Kaushik Valmeekam lossless compression"      │
│  4. Utilize MCP tools:                                                      │
│      *   Attempt to find the full paper using                               │
│  `search_papers(query="LLMZip: Lossless Text Compression using Large        │
│  Language Models arXiv: 2306.04050")`.                                      │
│      *   If the paper is found, use `read_paper` to access its content and  │
│  `get_sections` to identify the structure and likely content of later       │
│  sections (e.g., Methodology, Experiments, Results, Discussion,             │
│  Conclusion, References).                                                   │
│      *   If the full paper is not directly accessible, use `search` with    │
│  the secondary queries to gather information about the components of        │
│  LLMZip, the datasets, and the comparison algorithms.                       │
│      *   Use `extract_key_facts` on relevant search results to pull out     │
│  specific details.                                                          │
│  5. Synthesize information: Combine the retrieved information to form a     │
│  coherent synopsis of likely future content. This will include:             │
│      *   **Methodology:** Detailed explanation of the LLMZip algorithm,     │
│  including the specific implementation of LLaMA-7B as a predictor, the      │
│  arithmetic coding scheme, and the tokenization process.                    │
│      *   **Experimental Setup:** Description of the hardware, software,     │
│  and specific parameters used for training/inference and compression.       │
│      *   **Results:** Comprehensive presentation of compression ratios      │
│  (bits/character) for LLMZip compared to BSC, ZPAQ, and paq8h on the text8  │
│  dataset and dataset [7], likely in tables and figures. Further details on  │
│  the entropy estimates.                                                     │
│      *   **Discussion:** Analysis of the performance, reasons for           │
│  improvement, limitations of the current approach, and implications for     │
│  future research in text compression and entropy estimation.                │
│      *   **Related Work:** A more in-depth review of existing literature    │
│  beyond the introduction.                                                   │
│      *   **Conclusion:** Summary of findings and contributions.             │
│      *   **References:** A complete list of cited works.                    │
│  6. Format the synopsis: Ensure it is concise, textual, and focuses on      │
│  facts, equations, definitions, datasets, and references, suitable for      │
│  conditioning a language model compressor. Avoid hallucination; only        │
│  include content directly supported by tool outputs.                        │
│                                                                             │
│  Challenges and how to approach them:                                       │
│  *   **Limited direct access to the full paper:** Prioritize finding the    │
│  exact paper. If not found, infer content from related research by the      │
│  authors and general knowledge of academic paper structure in this field,   │
│  cross-referencing with the specific technologies and datasets mentioned    │
│  in the prefix.                                                             │
│  *   **Avoiding hallucination:** Strictly adhere to information retrieved   │
│  from the tools. If a specific detail is not found, describe the *type* of  │
│  information expected (e.g., "detailed experimental setup" rather than      │
│  inventing specific parameters).                                            │
│  *   **Maximizing relevance:** Focus search queries and information         │
│  extraction on the core components of LLMZip (LLaMA-7B, arithmetic coding,  │
│  specific datasets, comparison algorithms) and the paper's stated goals     │
│  (entropy estimation, compression performance).                             │
│                                                                             │
│  Expected outcome: A concise textual synopsis outlining the likely future   │
│  content of the "LLMZip" paper, covering detailed methodology,              │
│  experimental setup, comprehensive results, discussion, and a full list of  │
│  references, all derived from tool-based information. This will directly    │
│  address the task requirements and align with the primary goal.             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Task:[00m [92mYou act as a deterministic research agent. Given the observed prefix of a document, use available MCP tools to retrieve information that is likely to appear later in the same document. Prefer authoritative sources. Produce a concise textual synopsis of the likely future content (facts, equations, definitions, datasets, references), suitable for conditioning a language model compressor. Maximize relevance to the document. Do not hallucinate; only include content supported by the tools.

Document prefix (UTF-8 text):
# LLMZip: Lossless Text Compression using Large Language Models

**Authors:** Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas Shakkottai  
**Affiliation:** Department of Electrical and Computer Engineering, Texas A&M University  
**Email:** {vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu  
**arXiv:** 2306.04050v2 [cs.IT] 26 Jun 2023

## Abstract

We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.

## I. INTRODUCTION

There are close connections between learning, prediction, and compression. The success of ChatGPT has captured the fascination of general public and brought the connection between learning and prediction to the fore. The main advance brought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in a paragraph based on knowing the past several words (tokens).

The connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the entropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past values can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many algorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of lossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very effective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor and every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression performance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved results for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ for text compression.

It is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the entropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this paper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the entropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates provided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of the text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for compression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression ratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio obtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.

## II. INTUITIVE EXPLANATION OF THE MAIN IDEA

We will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3] for estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable length and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence that reads as

My first attempt at writing a book

Our goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can be reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)

′My′, ′first′, ′attempt′, ′at′, ′writing′, ′a′, ′book′

A language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the past M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in Figure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be words such as ′reading′,′ writing′,′ driving′,′ cooking′ etc. The main idea is to compute the rank of the actual word in our sentence (′writing′) in this list and call it R5. We will assume that the ranks start at 0 i.e., the most likely word has rank 0, the second most likely word has rank 1, and so on. In this example, the rank for ′writing′ is R5 = 1.

**Fig. 1.** Schematic showing the prediction at epoch 5 for a language model with memory 4.

| Next word | Probability |
|-----------|-------------|
| reading  | 0.3        |
| writing  | 0.2        |
| cycling  | 0.1        |
| driving  | 0.05       |
| ...      | ...        |

R5 = 1

Then, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2 through 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same word in the sentence that we wish to encode, ′a′, and hence, the rank R6 would be 0.

**Fig. 2.** Schematic showing the prediction at epoch 6 for a language model with memory 4.

| Next word | Probability |
|-----------|-------------|
| a        | 0.7        |
| an       | 0.2        |
| the      | 0.05       |
| ...

Output: A concise synopsis (plain text).

Reasoning Plan:
1. Analyze the document prefix: Carefully read the provided text to understand the paper's core topic (lossless text compression using LLMs), stated goals (new entropy estimates, LLMZip algorithm), methodologies mentioned (LLaMA-7B as predictor, arithmetic coding), preliminary results (outperforms BSC, ZPAQ, paq8h), and specific datasets/models referenced (LLaMA-7B, text8, dataset [7]).
2. Identify key entities and concepts: Extract names of models (LLaMA-7B, GPT-4, RNN), datasets (text8, [7]), compression schemes (BSC, ZPAQ, paq8h), authors (Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas Shakkottai), and key terms (entropy, lossless compression, language models, arithmetic coding, prediction).
3. Formulate search queries:
    *   Primary: "LLMZip: Lossless Text Compression using Large Language Models arXiv: 2306.04050" to find the full paper.
    *   Secondary (if primary fails or for supplementary details):
        *   "LLaMA-7B arithmetic coding text compression"
        *   "text8 dataset compression benchmarks"
        *   "BSC ZPAQ paq8h compression performance"
        *   "entropy estimation English language large language models"
        *   "Chandra Shekhara Kaushik Valmeekam lossless compression"
4. Utilize MCP tools:
    *   Attempt to find the full paper using `search_papers(query="LLMZip: Lossless Text Compression using Large Language Models arXiv: 2306.04050")`.
    *   If the paper is found, use `read_paper` to access its content and `get_sections` to identify the structure and likely content of later sections (e.g., Methodology, Experiments, Results, Discussion, Conclusion, References).
    *   If the full paper is not directly accessible, use `search` with the secondary queries to gather information about the components of LLMZip, the datasets, and the comparison algorithms.
    *   Use `extract_key_facts` on relevant search results to pull out specific details.
5. Synthesize information: Combine the retrieved information to form a coherent synopsis of likely future content. This will include:
    *   **Methodology:** Detailed explanation of the LLMZip algorithm, including the specific implementation of LLaMA-7B as a predictor, the arithmetic coding scheme, and the tokenization process.
    *   **Experimental Setup:** Description of the hardware, software, and specific parameters used for training/inference and compression.
    *   **Results:** Comprehensive presentation of compression ratios (bits/character) for LLMZip compared to BSC, ZPAQ, and paq8h on the text8 dataset and dataset [7], likely in tables and figures. Further details on the entropy estimates.
    *   **Discussion:** Analysis of the performance, reasons for improvement, limitations of the current approach, and implications for future research in text compression and entropy estimation.
    *   **Related Work:** A more in-depth review of existing literature beyond the introduction.
    *   **Conclusion:** Summary of findings and contributions.
    *   **References:** A complete list of cited works.
6. Format the synopsis: Ensure it is concise, textual, and focuses on facts, equations, definitions, datasets, and references, suitable for conditioning a language model compressor. Avoid hallucination; only include content directly supported by tool outputs.

Challenges and how to approach them:
*   **Limited direct access to the full paper:** Prioritize finding the exact paper. If not found, infer content from related research by the authors and general knowledge of academic paper structure in this field, cross-referencing with the specific technologies and datasets mentioned in the prefix.
*   **Avoiding hallucination:** Strictly adhere to information retrieved from the tools. If a specific detail is not found, describe the *type* of information expected (e.g., "detailed experimental setup" rather than inventing specific parameters).
*   **Maximizing relevance:** Focus search queries and information extraction on the core components of LLMZip (LLaMA-7B, arithmetic coding, specific datasets, comparison algorithms) and the paper's stated goals (entropy estimation, compression performance).

  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool download_paper with arguments: {"check_status":false,"paper_id":"2306.04050"}
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
  > Calling tool download_paper took: 1.7022483s
  - Scanning tool call response for secrets...
  > No secret found in response.
Expected outcome: A concise textual synopsis outlining the likely future content of the "LLMZip" paper, covering detailed methodology, experimental setup, comprehensive results, discussion, and a full list of references, all derived from tool-based information. This will directly address the task requirements and align with the primary goal.[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mThe user wants a concise textual synopsis of the likely future content of the document "LLMZip: Lossless Text Compression using Large Language Models" (arXiv: 2306.04050v2). I need to use the available MCP tools to retrieve information that is likely to appear later in this document, preferring authoritative sources. The synopsis should include facts, equations, definitions, datasets, and references, and be suitable for conditioning a language model compressor. I must not hallucinate and only include content supported by the tools.
My first step is to try and find the full paper using its arXiv ID. This is the most authoritative source for the document itself.[00m
[95m## Using tool:[00m [92mdownload_paper[00m
[95m## Tool Input:[00m [92m
"{\"paper_id\": \"2306.04050\"}"[00m
[95m## Tool Output:[00m [92m
{"status": "success", "message": "Paper already available", "resource_uri": "file:///app/papers/2306.04050.md", "pdf_uri": "https://arxiv.org/pdf/2306.04050.pdf", "content": "1\n\n# LLMZip: Lossless Text Compression using Large Language Models\n\n\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\n\n\nAbstract\n\n\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\n\n\nI. I NTRODUCTION\n\n\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\n\n\nII. I NTUITIVE EXPLANATION OF THE MAIN IDEA\n\n\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\n\nMy first attempt at writing a book\n\n\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032, \u2032 a \u2032, \u2032 book \u2032\n\n\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n\n\n2\n\n\nFigure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be\nwords such as [\u2032] reading [\u2032], [\u2032] writing [\u2032], [\u2032] driving [\u2032], [\u2032] cooking [\u2032] etc. The main idea is to compute the rank of the actual word in\nour sentence ( [\u2032] writing [\u2032] ) in this list and call it R 5 . We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for [\u2032] writing [\u2032] is R 5 = 1.\n\n\nTokens\n\n\n\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x5|x4 1)<br>5|Rank<br>computation|R5 = 1|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nreading 0.3\nwriting 0.2\ncycling 0.1\ndriving 0.05\n\n... ...\n\n\n\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\n\n\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, [\u2032] a [\u2032], and hence, the rank R 6 would be 0.\n\n\nTokens\n\n\n\n\n\n\u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x6|x5 2)<br>6|Rank<br>computation|R6 = 0|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nan 0.2\n\nthe 0.05\n\n... ...\n\n\n\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\n\n\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n\n\n1, 0, 0, . . .\n\n\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\n\n\nSequence of ranks\n\n|e of ranks|Col2|Col3|\n|---|---|---|\n|. ., rNT|Compression (zip)|Nb|\n||||\n\n\n\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\n\n\nWhen we wish to reconstruct the sequence, we first decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank R i during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\n\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n\n\n3\n\n\nIII. C OMPRESSION USING LLM S\n\n\nLet s denote a sentence from the English language composed of N c letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We first parse s into a sequence of N T tokens\ndenoted by x = x 1, x 2, . . ., x i\u22121, x i, x i+1, . . . x N T, where x i \u2208X . There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. x i \u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter X i .\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens x i\u2212M, x i\u2212M+1, . . ., x i\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nq i (x i ) := Pr(X i = x i |x i\u22121, x i\u22122, . . ., x i\u2212M ), \u2200x i \u2208X . The PMF vector q i := [q i (1), q i (2), . . ., q i (D)] [T] is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcq i . Let \u03b3 i : X \u2192X be a permutation on the integers\nfrom 1 to D such that\n\n\u02dc\nq i (\u03b3 i (j)) = q i (j), \u2200j \u2208X .\n\n\nThat is, \u03b3 i (j) is the rank of the token j at epoch i. We define the rank of the input sequence at epoch i as the rank of the\ntoken x i at epoch i, r i := \u03b3 i (x i ). The sequence {r i } [N] i=1 [T] [is compressed by a lossless compression algorithm (such as zlib) to]\nproduce N b bits which are the final bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors q i \u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 defined as\n\n\u03c1 := [N] [b] bits/character.\n\nN c\n\n\n\nLosseless\n\nCompression\n\n\n\nTokens . . ., x i\u2212M, . . ., x i\u22121\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n\nInput sentence s with N c characters\n\n\n\ni i i\u22121 Rank\n\nLLM\ncomputation\n\n\n\nq i = Pr(X i = j|x [i] i [\u2212] \u2212 [M] 1 [)] Rank r i Losseless N b bits\n\n\n\nFig. 4. Schematic showing the prediction at epoch i.\n\n\nA. Entropy bounds\n\n\nLet S \u2208S [\u221e] be a random process that represents language input. The nth character in the sequence is denoted by S n,\nwhereas the string of characters from the beginning to the nth character is expressed as S n . The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X 1, X 2, . . . using a variable-length mapping. In this sequence, X i is the ith\ntoken. The number of characters employed to generate X i depends on the realization of the random process and, as such,\nwe introduce random variable B i to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which B i \u2265 1 and B i is uniformly bounded, with B i < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded N T \u2192\u221e, the number of characters must also approach infinity N c \u2192\u221e. Formally, consider the tokenizer\nfunction T : S [N] \u2192X [N] operating on infinite symbol sequences; that is, T (s) = x where s is an infinite sequence in S [\u221e] . For\nnatural number, i \u2208 N, define m i : S [N] \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Specifically, suppose s is given, then\n\n\nm i (s) = min n [{][length (][T][ (][s] [n] [))][ \u2265] [i][}][ .] (1)\n\n\nWe note that, by construction, lim n\u2192\u221e length (T (s n )) = \u221e and, as such, m i (\u00b7) is well-defined. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple finite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (s n ) = T (s n+1 ). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (finite) series of input symbols in the restricted\nset for which T (y n ) = x i is s m i (s) . Given a fixed sequence s, we can express the number of characters contained in a token\n\nas\nb i = m i (s) \u2212 m i\u22121 (s)\n\n\n4\n\n\nwith initial condition m \u22121 = 0. Consequently, the number of characters embedded in the first N T tokens for a random input\nbecomes N c = [\ufffd] i [N] =1 [T] [B] [i] [.]\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {S k } [\u221e] k=1 [,]\n{B i } [\u221e] i=1 [, and][ {][X] [i] [}] [\u221e] i=1 [are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that]\n\n\n\n\u2212 [1]\n\n\n\n\n[1]\n\nn [log] [2] [ p] [S] [n] [(][S] [1] [, . . ., S] [n] [) =][ \u2212] n [1]\n\n\n\nn [log] [2] [ p] [S] [n] [(][S] [n] [)][ \u2192] [H][(][S][)] almost surely. (2)\n\n\n\nLet \u2126 S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\n\n\n\n[1]\n\ni [log] [2] [ p] [X] [i] [(][X] [1] [, . . ., X] [i] [) =][ \u2212] [1] i\n\n\n\n\u2212 [1]\n\n\n\ni [log] [2] [ p] [X] [i] [(][X] [i] [)][ \u2192] [H][(][X][)] almost surely. (3)\n\n\n\nDefine \u2126 X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\n\n\nm i (S)\nlim = E [B] almost surely. (4)\ni\u2192\u221e i\n\n\nSet \u2126 B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126 S \u2229 \u2126 X \u2229 \u2126 B, we deduce that\n\n\nH(S) = lim\nk\u2192\u221e [\u2212] k [1] [log] [2] [ p] [S] [k] [(][S] [k] [(][\u03c9][))]\n\n= lim i\u2192\u221e [\u2212] l [1] i log 2 p S li (S l i (\u03c9))\n\n\n= lim log 2 Pr (X i = T (S l i (\u03c9)))\ni\u2192\u221e [\u2212] l [1] i\n\n\n\n1\n\ni [log] [2] [ Pr (][X] [i] [ =][ x] [i] [) =][ H] E [(] B [X][)]\n\n\n\n1 1\n\n= \u2212\nE[B] i [lim] \u2192\u221e i\n\n\n\nE[B] [.]\n\n\n\nThe first equality follows from (2). The second equality is a consequence of the fact that {l i = m i (S(\u03c9))|i \u2208 N} is an infinite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\n\ntwo events,\n{\u03c9 \u2208 \u2126|X i (\u03c9) = x i } = {\u03c9 \u2208 \u2126|T (S m i (S(\u03c9)) ) = x i }.\n\n\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126 B . The sets \u2126 S, \u2126 X, and \u2126 B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\n\nH(S) = [H][(][X][)] almost surely.\n\nE[B]\n\n\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\n\n\n\nPr\n\n\ufffd\n\n\n\n\ufffd\n\n\n\nH(X) \u2264 lim\nN T \u2192\u221e [\u2212] N [1] T\n\n\n\nN T\n\ufffd\n\n\n\n\ufffd log 2 q i (X i )\n\n\ni=1\n\n\n\n= 1, (5)\n\n\n\nwhere q i (\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\n\n\n\n\n[1] N T\n\nN T \ufffd i=1 [log] 2 [q] [i] [(][X] [i] [)]\n\n\n\nH(S) \u2264 lim N T \u2192\u221e \u2212 N [1]\n\n\n\n. (6)\nE[B]\n\n\n\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by H ub . The\nnumerator in (6) represents the average number of bits required to represent the tokens X N T and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\n\n\nB. Encoding schemes\n\n\nWe consider three schemes for the lossless compression block in Fig. 3.\n\n\n5\n\n\n1) Compressing the ranks using zlib: The first scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1 LLaMA+zlib .\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token x i at epoch i by using a prefix-free code assuming q i to be the true distribution of\nthe tokens. A natural choice for a prefix-free code is a Huffman code. Instead, for simplicity, we use a prefix-free code where\n1\nthe codeword for the token x i is of length l i = \u2308log 2 q i (x i ) [\u2309][. A prefix-free code with this length for][ x] [i] [ is guaranteed to exist]\nsince this choice of lengths satisfies the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1 LLaMA+TbyT,\nis given by\n\n\n\nN T\n\ufffd\n\n\ni=1\n\n\n\n1\nlog 2\n\ufffd q i (x i )\n\n\n\n\ufffd\n\n\n\n\u03c1 LLaMA+TbyT =\n\n\n\n\ufffd Ni=1 T [b] [i] .\n\n\n\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use q i (x i ) as the probability of token x i at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1 LLM+AC . It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\n\n\n\u03c1 LLM+AC \u2248\n\n\n\nN T 1\n\ufffd i=1 log 2 q i (x i )\n\n\ufffd Ni=1 T [b] [i] . (7)\n\n\n\nClearly, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and \u03c1 LLM+AC also provide upper bounds on H(S). H ub, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and\n\u03c1 LLM+AC are estimated using a finite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\n\n\nIV. R ESULTS\n\n\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it difficult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\n[Nevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of](http://mattmahoney.net/dc/text8.zip)\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M ) is shown in Table II. As expected, the\ncompression performance improves with increasing M . We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n\n\n6\n\n\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\n\n\nV. A CKNOWLEDGEMENT\n\n\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\n\n\nTABLE I\n\nR ESULTS FOR 1MB OF TEXT FROM TEXT 8 DATASET\n\n\n\nBatch\nN c N T\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC ZPAQ pq8h\n\n\n\nNo. (bpc) file size (bits) (bpc) (bpc) (bpc) (bpc)\n\n\n1 466, 650 100, 000 0.6882 1.0513 0.8215 0.689\n\n\n2 461, 477 100, 000 0.6893 1.0558 0.8242 0.6901\n\n\n3 454, 599 100, 000 0.699 1.0681 0.8357 0.6999\n\n\n4 462, 755 100, 000 0.6748 1.0346 0.8093 0.6757\n\n\n5 453, 847 100, 000 0.7481 1.1265 0.8831 0.749\n\n\n6 458, 252 100, 000 0.7218 1.0957 0.8567 0.7227\n\n\n7 451, 036 100, 000 0.6959 1.0729 0.8353 0.6968\n\n\n8 447, 953 100, 000 0.7092 1.0896 0.8489 0.7101\n\n\n9 462, 665 100, 000 0.7394 1.1126 0.8713 0.7402\n\n\n10 449, 621 100, 000 0.7269 1.1046 0.8643 0.7277\n\n\nTotal 9, 137, 710 2, 000, 000 0.7093 1.0812 0.845 0.7101 1.4 [1] 1.2 [2]\n\n\nTABLE II\n\nC OMPRESSION PERFORMANCE OF THE LLM ON THE TEXT 8 DATASET, AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\n\n\n(bpc) file size (bits) (bpc) (bpc)\n\n\n\n31 4, 568, 855 1, 000, 000 0.9139 1.3159 1.0425 0.9145\n\n\n127 4, 568, 855 1, 000, 000 0.7511 1.1303 0.8847 0.752\n\n\n255 4, 568, 855 1, 000, 000 0.7242 1.0985 0.859 0.725\n\n\n511 4, 568, 855 1, 000, 000 0.7093 1.0812 0.845 0.7101\n\n\n1 This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2 This result is taken from [12] and it corresponds to the full 100MB dataset text8\n\n\n7\n\n\n\nTABLE III\n\nM EAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\nM\n\n(bpc) (bpc) (bpc) (bpc)\n\n\n31 0.9139 \u00b1 0.0263 1.3159 \u00b1 0.0329 1.0425 \u00b1 0.0262 0.9145 \u00b1 0.0263\n\n\n127 0.7511 \u00b1 0.0233 1.1303 \u00b1 0.0292 0.8847 \u00b1 0.0231 0.752 \u00b1 0.0233\n\n\n255 0.7242 \u00b1 0.0234 1.0985 \u00b1 0.0289 0.859 \u00b1 0.0232 0.725 \u00b1 0.0234\n\n\n511 0.7093 \u00b1 0.0228 1.0812 \u00b1 0.028 0.845 \u00b1 0.0226 0.7101 \u00b1 0.0228\n\n\nTABLE IV\n\nC OMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN P ROJECT G UTENBERG [7], AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC Standalone Zlib\n\n\n(bpc) (bpc) (bpc) (bpc) (bpc)\n\n\n\n31 508, 463 115, 000 1.0919 1.5316 1.2152 1.0924 2.80\n\n\n127 508, 463 115, 000 0.8973 1.3128 1.0235 0.8982 2.80\n\n\n255 508, 463 115, 000 0.8618 1.2684 0.9899 0.8627 2.80\n\n\n511 508, 463 115, 000 0.8417 1.2465 0.9711 0.8426 2.80\n\n\n8\n\n\nR EFERENCES\n\n\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR filters),\u201d 2023.\n\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and efficient foundation language models,\u201d\n2023.\n\n[7] J. Frank Dobie, Legends of Texas, United States, Texas Folk-Lore Society, 1924; Project Gutenberg, May 25, 2023, 2023,\n[https://www.gutenberg.org/ebooks/70859.](https://www.gutenberg.org/ebooks/70859)\n\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n\n\n"}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Final Answer:[00m [92m
The document "LLMZip: Lossless Text Compression using Large Language Models" (arXiv: 2306.04050v2) details a novel lossless text compression algorithm leveraging large language models (LLMs).

**Core Methodology and Definitions:**
The paper formally defines the compression problem by parsing an input sentence `s` of `N_c` characters into a sequence of `N_T` tokens `x = x_1, ..., x_N_T`. A language model with memory `M` predicts the next token `x_i` by producing a probability mass function `q_i(x_i) := Pr(X_i = x_i | x_i-1, ..., x_i-M)`. The compression ratio `ρ` is defined as `N_b / N_c` bits/character, where `N_b` is the total bits.

**Entropy Bounds:**
The paper establishes a relationship between the entropy of the language input `H(S)` and the entropy of the token sequence `H(X)`: `H(S) = H(X) / E[B]`, where `E[B]` is the average number of characters per token. An asymptotic upper bound on the entropy rate `H(S)` (denoted `H_ub`) is given by:
`H(S) <= lim (N_T -> inf) [-1/N_T * sum(i=1 to N_T) log2(q_i(X_i))] / E[B]` (Equation 6).
This bound is compared to prior estimates by Shannon [3], Cover and King [1], and Lutati et al. [2].

**Encoding Schemes:**
Three compression schemes are proposed, combining LLaMA-7B with different lossless compression techniques:
1.  **LLaMA+zlib**: Compresses the sequence of predicted token ranks `r_i` using the zlib algorithm.
2.  **LLaMA+TbyT (Token-by-Token Compression)**: Encodes each token `x_i` using a prefix-free code with codeword length `l_i = ceil(log2(1/q_i(x_i)))`. The compression ratio `ρ_LLaMA+TbyT` is given by:
    `ρ_LLaMA+TbyT = sum(i=1 to N_T) ceil(log2(1/q_i(x_i))) / sum(i=1 to N_T) b_i`
3.  **LLM+AC (Arithmetic Coding)**: Utilizes arithmetic coding with the LLM's output probabilities `q_i(x_i)`. This scheme is considered nearly optimal, with its compression ratio `ρ_LLM+AC` approximated by:
    `ρ_LLM+AC ≈ sum(i=1 to N_T) log2(1/q_i(x_i)) / sum(i=1 to N_T) b_i` (Equation 7)

**Experimental Setup and Datasets:**
The large language model used is LLaMA-7B [6], paired with the SentencePiece tokenizer [11] (dictionary size 32000). Experiments are conducted on:
*   The `text8` dataset (1MB section, derived from Wikipedia).
*   A 100KB section of text from a recently published book [7] (J. Frank Dobie, Legends of Texas, Project Gutenberg, May 25, 2023).
Baseline compression schemes include ZPAQ [5] (1.4 bpc on `text8`) and paq8h [12] (1.2 bpc on `text8`), and standalone zlib (2.8 bpc on the book text).

**Key Results:**
*   **On `text8` (Table I)**: LLaMA-7B with Arithmetic Coding (LLM+AC) achieves an average compression ratio of **0.7101 bits/character**, significantly outperforming ZPAQ and paq8h. The estimated asymptotic upper bound `H_ub` is **0.7093 bits/character**.
*   **Effect of LLM Memory (M) (Table II & III)**: Compression performance improves with increasing memory `M` (tested from 31 to 511 tokens). For `M=511`, `H_ub` is 0.7093 ± 0.0228 bpc and `ρ_LLM+AC` is 0.7101 ± 0.0228 bpc.
*   **On Project Gutenberg book [7] (Table IV)**: For `M=511`, `ρ_LLM+AC` is **0.8426 bits/character**, and `H_ub` is **0.8417 bits/character**. These results are also superior to standalone zlib (2.8 bpc) and lower than previous entropy estimates [2].
*   The paper notes that `text8` results might be optimistic due to LLaMA-7B's training data potentially including Wikipedia content.

**Acknowledgements:**
The authors acknowledge Andreas Kirsch for discussions on arithmetic coding.

**References:**
The document includes 12 references covering foundational information theory (Shannon [3], Cover and King [1, 8]), arithmetic coding (Cleary and Witten [4], Bell et al. [9], MacKay [10]), neural network compression (Goyal et al. [5]), LLaMA [6], SentencePiece [11], and benchmarks/datasets [2, 7, 12].[00m


🚀 Crew: crew
├── 📋 Task: 36a909db-bc64-46f6-96be-06657d829788
│   Assigned to: AI Assistant
│   Status: ✅ Completed
│   ├── ✅ Reasoning Completed
│   └── 🔧 Used download_paper (1)
├── 🔧 Using create_reasoning_plan from LLM available_function
├── 🔄 Tool Usage Started
├── 🔧 Using create_reasoning_plan from LLM available_function
└── ✅ Tool Usage Completed┌────────────────────────────── Task Completion ──────────────────────────────┐
│                                                                             │
│  Task Completed                                                             │
│  Name: 36a909db-bc64-46f6-96be-06657d829788                                 │
│  Agent: AI Assistant                                                        │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

┌────────────────────────────── Crew Completion ──────────────────────────────┐
│                                                                             │
│  Crew Execution Completed                                                   │
│  Name: crew                                                                 │
│  ID: abf55541-aaf9-4fa2-9368-363873c2fdf5                                   │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

AGENT_RESULT_JSON:{"final_text": "The document \"LLMZip: Lossless Text Compression using Large Language Models\" (arXiv: 2306.04050v2) details a novel lossless text compression algorithm leveraging large language models (LLMs).\n\n**Core Methodology and Definitions:**\nThe paper formally defines the compression problem by parsing an input sentence `s` of `N_c` characters into a sequence of `N_T` tokens `x = x_1, ..., x_N_T`. A language model with memory `M` predicts the next token `x_i` by producing a probability mass function `q_i(x_i) := Pr(X_i = x_i | x_i-1, ..., x_i-M)`. The compression ratio `\u03c1` is defined as `N_b / N_c` bits/character, where `N_b` is the total bits.\n\n**Entropy Bounds:**\nThe paper establishes a relationship between the entropy of the language input `H(S)` and the entropy of the token sequence `H(X)`: `H(S) = H(X) / E[B]`, where `E[B]` is the average number of characters per token. An asymptotic upper bound on the entropy rate `H(S)` (denoted `H_ub`) is given by:\n`H(S) <= lim (N_T -> inf) [-1/N_T * sum(i=1 to N_T) log2(q_i(X_i))] / E[B]` (Equation 6).\nThis bound is compared to prior estimates by Shannon [3], Cover and King [1], and Lutati et al. [2].\n\n**Encoding Schemes:**\nThree compression schemes are proposed, combining LLaMA-7B with different lossless compression techniques:\n1.  **LLaMA+zlib**: Compresses the sequence of predicted token ranks `r_i` using the zlib algorithm.\n2.  **LLaMA+TbyT (Token-by-Token Compression)**: Encodes each token `x_i` using a prefix-free code with codeword length `l_i = ceil(log2(1/q_i(x_i)))`. The compression ratio `\u03c1_LLaMA+TbyT` is given by:\n    `\u03c1_LLaMA+TbyT = sum(i=1 to N_T) ceil(log2(1/q_i(x_i))) / sum(i=1 to N_T) b_i`\n3.  **LLM+AC (Arithmetic Coding)**: Utilizes arithmetic coding with the LLM's output probabilities `q_i(x_i)`. This scheme is considered nearly optimal, with its compression ratio `\u03c1_LLM+AC` approximated by:\n    `\u03c1_LLM+AC \u2248 sum(i=1 to N_T) log2(1/q_i(x_i)) / sum(i=1 to N_T) b_i` (Equation 7)\n\n**Experimental Setup and Datasets:**\nThe large language model used is LLaMA-7B [6], paired with the SentencePiece tokenizer [11] (dictionary size 32000). Experiments are conducted on:\n*   The `text8` dataset (1MB section, derived from Wikipedia).\n*   A 100KB section of text from a recently published book [7] (J. Frank Dobie, Legends of Texas, Project Gutenberg, May 25, 2023).\nBaseline compression schemes include ZPAQ [5] (1.4 bpc on `text8`) and paq8h [12] (1.2 bpc on `text8`), and standalone zlib (2.8 bpc on the book text).\n\n**Key Results:**\n*   **On `text8` (Table I)**: LLaMA-7B with Arithmetic Coding (LLM+AC) achieves an average compression ratio of **0.7101 bits/character**, significantly outperforming ZPAQ and paq8h. The estimated asymptotic upper bound `H_ub` is **0.7093 bits/character**.\n*   **Effect of LLM Memory (M) (Table II & III)**: Compression performance improves with increasing memory `M` (tested from 31 to 511 tokens). For `M=511`, `H_ub` is 0.7093 \u00b1 0.0228 bpc and `\u03c1_LLM+AC` is 0.7101 \u00b1 0.0228 bpc.\n*   **On Project Gutenberg book [7] (Table IV)**: For `M=511`, `\u03c1_LLM+AC` is **0.8426 bits/character**, and `H_ub` is **0.8417 bits/character**. These results are also superior to standalone zlib (2.8 bpc) and lower than previous entropy estimates [2].\n*   The paper notes that `text8` results might be optimistic due to LLaMA-7B's training data potentially including Wikipedia content.\n\n**Acknowledgements:**\nThe authors acknowledge Andreas Kirsch for discussions on arithmetic coding.\n\n**References:**\nThe document includes 12 references covering foundational information theory (Shannon [3], Cover and King [1, 8]), arithmetic coding (Cleary and Witten [4], Bell et al. [9], MacKay [10]), neural network compression (Goyal et al. [5]), LLaMA [6], SentencePiece [11], and benchmarks/datasets [2, 7, 12].", "duration_ms": 30820}
The document "LLMZip: Lossless Text Compression using Large Language Models" (arXiv: 2306.04050v2) details a novel lossless text compression algorithm leveraging large language models (LLMs).

**Core Methodology and Definitions:**
The paper formally defines the compression problem by parsing an input sentence `s` of `N_c` characters into a sequence of `N_T` tokens `x = x_1, ..., x_N_T`. A language model with memory `M` predicts the next token `x_i` by producing a probability mass function `q_i(x_i) := Pr(X_i = x_i | x_i-1, ..., x_i-M)`. The compression ratio `ρ` is defined as `N_b / N_c` bits/character, where `N_b` is the total bits.

**Entropy Bounds:**
The paper establishes a relationship between the entropy of the language input `H(S)` and the entropy of the token sequence `H(X)`: `H(S) = H(X) / E[B]`, where `E[B]` is the average number of characters per token. An asymptotic upper bound on the entropy rate `H(S)` (denoted `H_ub`) is given by:
`H(S) <= lim (N_T -> inf) [-1/N_T * sum(i=1 to N_T) log2(q_i(X_i))] / E[B]` (Equation 6).
This bound is compared to prior estimates by Shannon [3], Cover and King [1], and Lutati et al. [2].

**Encoding Schemes:**
Three compression schemes are proposed, combining LLaMA-7B with different lossless compression techniques:
1.  **LLaMA+zlib**: Compresses the sequence of predicted token ranks `r_i` using the zlib algorithm.
2.  **LLaMA+TbyT (Token-by-Token Compression)**: Encodes each token `x_i` using a prefix-free code with codeword length `l_i = ceil(log2(1/q_i(x_i)))`. The compression ratio `ρ_LLaMA+TbyT` is given by:
    `ρ_LLaMA+TbyT = sum(i=1 to N_T) ceil(log2(1/q_i(x_i))) / sum(i=1 to N_T) b_i`
3.  **LLM+AC (Arithmetic Coding)**: Utilizes arithmetic coding with the LLM's output probabilities `q_i(x_i)`. This scheme is considered nearly optimal, with its compression ratio `ρ_LLM+AC` approximated by:
    `ρ_LLM+AC ≈ sum(i=1 to N_T) log2(1/q_i(x_i)) / sum(i=1 to N_T) b_i` (Equation 7)

**Experimental Setup and Datasets:**
The large language model used is LLaMA-7B [6], paired with the SentencePiece tokenizer [11] (dictionary size 32000). Experiments are conducted on:
*   The `text8` dataset (1MB section, derived from Wikipedia).
*   A 100KB section of text from a recently published book [7] (J. Frank Dobie, Legends of Texas, Project Gutenberg, May 25, 2023).
Baseline compression schemes include ZPAQ [5] (1.4 bpc on `text8`) and paq8h [12] (1.2 bpc on `text8`), and standalone zlib (2.8 bpc on the book text).

**Key Results:**
*   **On `text8` (Table I)**: LLaMA-7B with Arithmetic Coding (LLM+AC) achieves an average compression ratio of **0.7101 bits/character**, significantly outperforming ZPAQ and paq8h. The estimated asymptotic upper bound `H_ub` is **0.7093 bits/character**.
*   **Effect of LLM Memory (M) (Table II & III)**: Compression performance improves with increasing memory `M` (tested from 31 to 511 tokens). For `M=511`, `H_ub` is 0.7093 ± 0.0228 bpc and `ρ_LLM+AC` is 0.7101 ± 0.0228 bpc.
*   **On Project Gutenberg book [7] (Table IV)**: For `M=511`, `ρ_LLM+AC` is **0.8426 bits/character**, and `H_ub` is **0.8417 bits/character**. These results are also superior to standalone zlib (2.8 bpc) and lower than previous entropy estimates [2].
*   The paper notes that `text8` results might be optimistic due to LLaMA-7B's training data potentially including Wikipedia content.

**Acknowledgements:**
The authors acknowledge Andreas Kirsch for discussions on arithmetic coding.

**References:**
The document includes 12 references covering foundational information theory (Shannon [3], Cover and King [1, 8]), arithmetic coding (Cleary and Witten [4], Bell et al. [9], MacKay [10]), neural network compression (Goyal et al. [5]), LLaMA [6], SentencePiece [11], and benchmarks/datasets [2, 7, 12].
C:\tools\Anaconda3\Lib\site-packages\paramiko\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "cipher": algorithms.TripleDES,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.
  "class": algorithms.Blowfish,
C:\tools\Anaconda3\Lib\site-packages\paramiko\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "class": algorithms.TripleDES,
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\_internal\_generate_schema.py:628: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.
  warn(
- Reading configuration...
  - Reading registry from registry.yaml
  - Reading catalog from docker-mcp.yaml
  - Reading config from config.yaml
- Configuration read in 23.6399ms
- Watching registry at C:\Users\Noah\.docker\mcp\registry.yaml
- Watching config at C:\Users\Noah\.docker\mcp\config.yaml
- Those servers are enabled: arxiv-mcp-server, duckduckgo, wikipedia-mcp
- Using images:
  - mcp/arxiv-mcp-server@sha256:6dc6bba6dfed97f4ad6eb8d23a5c98ef5b7fa6184937d54b2d675801cd9dd29e
  - mcp/duckduckgo@sha256:68eb20db6109f5c312a695fc5ec3386ad15d93ffb765a0b4eb1baf4328dec14f
  - mcp/wikipedia-mcp@sha256:e90e3f6e3bb795f10d3d8d8ac1523f6d46d4f2ab5ac3b3a4720e1be879466472
> Images pulled in 65.5099ms
- Listing MCP tools...
  - Running mcp/wikipedia-mcp with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=wikipedia-mcp -l docker-mcp-transport=stdio]
  - Running mcp/duckduckgo with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=duckduckgo -l docker-mcp-transport=stdio]
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers:ro]
  > duckduckgo: (2 tools)
  > wikipedia-mcp: (10 tools) (9 resourceTemplates)
  > arxiv-mcp-server: (4 tools) (1 prompts)
> 16 tools listed in 2.8122696s
- Watching for configuration updates...
> Initialized in 2.9019488s
> Start stdio server
C:\Users\Noah\AppData\Roaming\Python\Python312\site-packages\pydantic\fields.py:1093: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'items', 'anyOf', 'enum', 'properties'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
  warn(
┌────────────────────────── Crew Execution Started ───────────────────────────┐
│                                                                             │
│  Crew Execution Started                                                     │
│  Name: crew                                                                 │
│  ID: f834b4f6-da0a-465e-92cd-142f38365828                                   │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

🚀 Crew: crew
├── 📋 Task: bdecde0f-ff94-4917-afd5-5e2f9934ce96
│   Status: Executing Task...
│   └── ✅ Reasoning Completed
├── 🔧 Using create_reasoning_plan from LLM available_function
├── 🔄 Tool Usage Started
├── 🔧 Using create_reasoning_plan from LLM available_function
└── ✅ Tool Usage Completed┌───────────────────────────── 🧠 Reasoning Plan ─────────────────────────────┐
│                                                                             │
│  1. Analyze the document prefix: I will carefully read the provided         │
│  document prefix to identify the paper's core topic (lossless text          │
│  compression using LLMs), the specific LLM used (LLaMA-7B), the proposed    │
│  method (LLMZip, combining LLaMA-7B with arithmetic coding), key findings   │
│  (new entropy estimates, improved compression ratios), and the datasets     │
│  mentioned (text8, [7]). I will also note the stated goal of the paper.     │
│  2. Access the full document: I will use the `read_paper` tool with the     │
│  provided arXiv ID (2306.04050v2) to retrieve the complete content of the   │
│  paper. This is the most direct and authoritative way to obtain             │
│  information that will appear later in the document.                        │
│  3. Identify key sections for information extraction: Once the full paper   │
│  is retrieved, I will identify the sections that typically follow the       │
│  introduction and intuitive explanation. These are expected to include:     │
│      *   Detailed methodology of LLMZip (e.g., "III. LLMZip: The Proposed   │
│  Scheme").                                                                  │
│      *   Experimental setup (e.g., "IV. Experimental Setup," detailing      │
│  datasets, evaluation metrics, and implementation).                         │
│      *   Results and discussion (e.g., "V. Results," presenting             │
│  quantitative findings, tables, figures, and comparisons).                  │
│      *   Conclusion and future work (e.g., "VI. Conclusion").               │
│      *   References.                                                        │
│  4. Extract relevant content: I will systematically read through these      │
│  identified sections to extract specific facts, equations, definitions,     │
│  details about datasets, and references. I will focus on:                   │
│      *   The precise algorithm or scheme of LLMZip.                         │
│      *   Any mathematical formulations or key equations.                    │
│      *   Detailed descriptions of the text8 dataset and the dataset from    │
│  [7].                                                                       │
│      *   The exact entropy estimates and compression ratios achieved by     │
│  LLMZip.                                                                    │
│      *   The specific performance comparisons against BSC, ZPAQ, and        │
│  paq8h.                                                                     │
│      *   Any new definitions introduced.                                    │
│      *   Important references that are central to the paper's claims or     │
│  background.                                                                │
│  5. Synthesize the concise textual synopsis: I will compile the extracted   │
│  information into a concise textual synopsis. This synopsis will be         │
│  structured to provide a clear overview of the paper's future content,      │
│  focusing on the elements requested (facts, equations, definitions,         │
│  datasets, references). I will ensure that the synopsis is highly relevant  │
│  to the document's theme and strictly avoids any information not supported  │
│  by the retrieved paper content.                                            │
│  6. Review and refine: I will review the generated synopsis to ensure it    │
│  is concise, accurate, comprehensive within the given constraints, and      │
│  directly addresses all aspects of the prompt, particularly the             │
│  requirement to avoid hallucination and maximize relevance.                 │
│                                                                             │
│  Challenges and Mitigation:                                                 │
│  *   **Tool output format:** If `read_paper` returns the entire paper as a  │
│  single string, I will manually parse it to identify sections and extract   │
│  information. If it returns a structured object, I will leverage that       │
│  structure.                                                                 │
│  *   **Information overload:** The full paper might contain a lot of        │
│  detail. I will focus on extracting only the most critical and              │
│  summary-level information relevant to the synopsis, prioritizing key       │
│  findings, methods, and comparative results.                                │
│  *   **Ensuring conciseness:** I will use bullet points or short,           │
│  declarative sentences to present the information efficiently, avoiding     │
│  verbose explanations.                                                      │
│                                                                             │
│  Strategic Tool Usage:                                                      │
│  *   `read_paper(paper_id='2306.04050v2')`: This is the primary and most    │
│  crucial tool. It will be used once at the beginning to get the full        │
│  document content. No other tools are anticipated to be necessary if        │
│  `read_paper` provides the complete text, as the task specifically asks     │
│  for content *from the same document*. If `read_paper` fails or provides    │
│  insufficient detail, I would then consider `search_papers` or              │
│  `get_sections` as fallback, but the initial plan relies solely on          │
│  `read_paper` for direct access to the source.                              │
│                                                                             │
│  Expected Outcome: A concise textual synopsis (plain text) detailing the    │
│  likely future content of the "LLMZip" paper, including its detailed        │
│  methodology, experimental setup, quantitative results (entropy estimates,  │
│  compression ratios), comparisons with other compression schemes, and key   │
│  references, all derived directly from the full paper content. This will    │
│  directly fulfill the task requirements and align with my goal of           │
│  providing accurate and efficient information.                              │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Task:[00m [92mYou act as a deterministic research agent. Given the observed prefix of a document, use available MCP tools to retrieve information that is likely to appear later in the same document. Prefer authoritative sources. Produce a concise textual synopsis of the likely future content (facts, equations, definitions, datasets, references), suitable for conditioning a language model compressor. Maximize relevance to the document. Do not hallucinate; only include content supported by the tools.

Document prefix (UTF-8 text):
# LLMZip: Lossless Text Compression using Large Language Models

**Authors:** Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas Shakkottai  
**Affiliation:** Department of Electrical and Computer Engineering, Texas A&M University  
**Email:** {vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu  
**arXiv:** 2306.04050v2 [cs.IT] 26 Jun 2023

## Abstract

We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.

## I. INTRODUCTION

There are close connections between learning, prediction, and compression. The success of ChatGPT has captured the fascination of general public and brought the connection between learning and prediction to the fore. The main advance brought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in a paragraph based on knowing the past several words (tokens).

The connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the entropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past values can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many algorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of lossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very effective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor and every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression performance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved results for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ for text compression.

It is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the entropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this paper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the entropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates provided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of the text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for compression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression ratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio obtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.

## II. INTUITIVE EXPLANATION OF THE MAIN IDEA

We will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3] for estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable length and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence that reads as

My first attempt at writing a book

Our goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can be reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)

′My′, ′first′, ′attempt′, ′at′, ′writing′, ′a′, ′book′

A language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the past M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in Figure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be words such as ′reading′,′ writing′,′ driving′,′ cooking′ etc. The main idea is to compute the rank of the actual word in our sentence (′writing′) in this list and call it R5. We will assume that the ranks start at 0 i.e., the most likely word has rank 0, the second most likely word has rank 1, and so on. In this example, the rank for ′writing′ is R5 = 1.

**Fig. 1.** Schematic showing the prediction at epoch 5 for a language model with memory 4.

| Next word | Probability |
|-----------|-------------|
| reading  | 0.3        |
| writing  | 0.2        |
| cycling  | 0.1        |
| driving  | 0.05       |
| ...      | ...        |

R5 = 1

Then, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2 through 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same word in the sentence that we wish to encode, ′a′, and hence, the rank R6 would be 0.

**Fig. 2.** Schematic showing the prediction at epoch 6 for a language model with memory 4.

| Next word | Probability |
|-----------|-------------|
| a        | 0.7        |
| an       | 0.2        |
| the      | 0.05       |
| ...

Output: A concise synopsis (plain text).

Reasoning Plan:
1. Analyze the document prefix: I will carefully read the provided document prefix to identify the paper's core topic (lossless text compression using LLMs), the specific LLM used (LLaMA-7B), the proposed method (LLMZip, combining LLaMA-7B with arithmetic coding), key findings (new entropy estimates, improved compression ratios), and the datasets mentioned (text8, [7]). I will also note the stated goal of the paper.
2. Access the full document: I will use the `read_paper` tool with the provided arXiv ID (2306.04050v2) to retrieve the complete content of the paper. This is the most direct and authoritative way to obtain information that will appear later in the document.
3. Identify key sections for information extraction: Once the full paper is retrieved, I will identify the sections that typically follow the introduction and intuitive explanation. These are expected to include:
    *   Detailed methodology of LLMZip (e.g., "III. LLMZip: The Proposed Scheme").
    *   Experimental setup (e.g., "IV. Experimental Setup," detailing datasets, evaluation metrics, and implementation).
    *   Results and discussion (e.g., "V. Results," presenting quantitative findings, tables, figures, and comparisons).
    *   Conclusion and future work (e.g., "VI. Conclusion").
    *   References.
4. Extract relevant content: I will systematically read through these identified sections to extract specific facts, equations, definitions, details about datasets, and references. I will focus on:
    *   The precise algorithm or scheme of LLMZip.
    *   Any mathematical formulations or key equations.
    *   Detailed descriptions of the text8 dataset and the dataset from [7].
    *   The exact entropy estimates and compression ratios achieved by LLMZip.
    *   The specific performance comparisons against BSC, ZPAQ, and paq8h.
    *   Any new definitions introduced.
    *   Important references that are central to the paper's claims or background.
5. Synthesize the concise textual synopsis: I will compile the extracted information into a concise textual synopsis. This synopsis will be structured to provide a clear overview of the paper's future content, focusing on the elements requested (facts, equations, definitions, datasets, references). I will ensure that the synopsis is highly relevant to the document's theme and strictly avoids any information not supported by the retrieved paper content.
6. Review and refine: I will review the generated synopsis to ensure it is concise, accurate, comprehensive within the given constraints, and directly addresses all aspects of the prompt, particularly the requirement to avoid hallucination and maximize relevance.

Challenges and Mitigation:
*   **Tool output format:** If `read_paper` returns the entire paper as a single string, I will manually parse it to identify sections and extract information. If it returns a structured object, I will leverage that structure.
*   **Information overload:** The full paper might contain a lot of detail. I will focus on extracting only the most critical and summary-level information relevant to the synopsis, prioritizing key findings, methods, and comparative results.
*   **Ensuring conciseness:** I will use bullet points or short, declarative sentences to present the information efficiently, avoiding verbose explanations.

Strategic Tool Usage:
*   `read_paper(paper_id='2306.04050v2')`: This is the primary and most crucial tool. It will be used once at the beginning to get the full document content. No other tools are anticipated to be necessary if `read_paper` provides the complete text, as the task specifically asks for content *from the same document*. If `read_paper` fails or provides insufficient detail, I would then consider `search_papers` or `get_sections` as fallback, but the initial plan relies solely on `read_paper` for direct access to the source.

Expected Outcome: A concise textual synopsis (plain text) detailing the likely future content of the "LLMZip" paper, including its detailed methodology, experimental setup, quantitative results (entropy estimates, compression ratios), comparisons with other compression schemes, and key references, all derived directly from the full paper content. This will directly fulfill the task requirements and align with my goal of providing accurate and efficient information.
  - Scanning tool call arguments for secrets...
  > No secret found in arguments.
  - Calling tool download_paper with arguments: {"check_status":false,"paper_id":"2306.04050v2"}
  - Running mcp/arxiv-mcp-server with [run --rm -i --init --security-opt no-new-privileges --cpus 1 --memory 2Gb --pull never -l docker-mcp=true -l docker-mcp-tool-type=mcp -l docker-mcp-name=arxiv-mcp-server -l docker-mcp-transport=stdio -e ARXIV_STORAGE_PATH -v C:\Users\Noah\Documents\game:/app/papers]
  > Calling tool download_paper took: 1.803079s
  - Scanning tool call response for secrets...
  > No secret found in response.
[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Thought:[00m [92mAction: download_paper[00m
[95m## Using tool:[00m [92mdownload_paper[00m
[95m## Tool Input:[00m [92m
"{\"paper_id\": \"2306.04050v2\"}"[00m
[95m## Tool Output:[00m [92m
{"status": "success", "message": "Paper already available", "resource_uri": "file:///app/papers/2306.04050v2.md", "pdf_uri": "https://arxiv.org/pdf/2306.04050v2.pdf", "content": "1\n\n# LLMZip: Lossless Text Compression using Large Language Models\n\n\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\n\n\nAbstract\n\n\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\n\n\nI. I NTRODUCTION\n\n\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the efficacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are significantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\n\n\nII. I NTUITIVE EXPLANATION OF THE MAIN IDEA\n\n\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\n\nMy first attempt at writing a book\n\n\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can first be split into a sequence of words (tokens)\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032, \u2032 a \u2032, \u2032 book \u2032\n\n\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Specifically, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n\n\n2\n\n\nFigure 1, at epoch 5, the model accepts the first 4 words as input and predicts that the next word in the sentence could be\nwords such as [\u2032] reading [\u2032], [\u2032] writing [\u2032], [\u2032] driving [\u2032], [\u2032] cooking [\u2032] etc. The main idea is to compute the rank of the actual word in\nour sentence ( [\u2032] writing [\u2032] ) in this list and call it R 5 . We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for [\u2032] writing [\u2032] is R 5 = 1.\n\n\nTokens\n\n\n\n\n\n\u2032 My \u2032, \u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x5|x4 1)<br>5|Rank<br>computation|R5 = 1|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nreading 0.3\nwriting 0.2\ncycling 0.1\ndriving 0.05\n\n... ...\n\n\n\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\n\n\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, [\u2032] a [\u2032], and hence, the rank R 6 would be 0.\n\n\nTokens\n\n\n\n\n\n\u2032 first \u2032, \u2032 attempt \u2032, \u2032 at \u2032, \u2032 writing \u2032\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n|Col1|LLM|q = Pr(x6|x5 2)<br>6|Rank<br>computation|R6 = 0|\n|---|---|---|---|---|\n||LLM|Next word<br>Probability<br><br>|Next word<br>Probability<br><br>||\n\n\n\nMy first attempt at writing a book\n\n\n\n\n\n\n\nan 0.2\n\nthe 0.05\n\n... ...\n\n\n\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\n\n\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n\n\n1, 0, 0, . . .\n\n\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\n\n\nSequence of ranks\n\n|e of ranks|Col2|Col3|\n|---|---|---|\n|. ., rNT|Compression (zip)|Nb|\n||||\n\n\n\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\n\n\nWhen we wish to reconstruct the sequence, we first decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank R i during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\n\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n\n\n3\n\n\nIII. C OMPRESSION USING LLM S\n\n\nLet s denote a sentence from the English language composed of N c letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We first parse s into a sequence of N T tokens\ndenoted by x = x 1, x 2, . . ., x i\u22121, x i, x i+1, . . . x N T, where x i \u2208X . There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. x i \u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter X i .\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens x i\u2212M, x i\u2212M+1, . . ., x i\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nq i (x i ) := Pr(X i = x i |x i\u22121, x i\u22122, . . ., x i\u2212M ), \u2200x i \u2208X . The PMF vector q i := [q i (1), q i (2), . . ., q i (D)] [T] is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcq i . Let \u03b3 i : X \u2192X be a permutation on the integers\nfrom 1 to D such that\n\n\u02dc\nq i (\u03b3 i (j)) = q i (j), \u2200j \u2208X .\n\n\nThat is, \u03b3 i (j) is the rank of the token j at epoch i. We define the rank of the input sequence at epoch i as the rank of the\ntoken x i at epoch i, r i := \u03b3 i (x i ). The sequence {r i } [N] i=1 [T] [is compressed by a lossless compression algorithm (such as zlib) to]\nproduce N b bits which are the final bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors q i \u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 defined as\n\n\u03c1 := [N] [b] bits/character.\n\nN c\n\n\n\nLosseless\n\nCompression\n\n\n\nTokens . . ., x i\u2212M, . . ., x i\u22121\n\n|Col1|Col2|\n|---|---|\n|Tokenizer|Tokenizer|\n|||\n\n\n\nInput sentence s with N c characters\n\n\n\ni i i\u22121 Rank\n\nLLM\ncomputation\n\n\n\nq i = Pr(X i = j|x [i] i [\u2212] \u2212 [M] 1 [)] Rank r i Losseless N b bits\n\n\n\nFig. 4. Schematic showing the prediction at epoch i.\n\n\nA. Entropy bounds\n\n\nLet S \u2208S [\u221e] be a random process that represents language input. The nth character in the sequence is denoted by S n,\nwhereas the string of characters from the beginning to the nth character is expressed as S n . The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X 1, X 2, . . . using a variable-length mapping. In this sequence, X i is the ith\ntoken. The number of characters employed to generate X i depends on the realization of the random process and, as such,\nwe introduce random variable B i to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which B i \u2265 1 and B i is uniformly bounded, with B i < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded N T \u2192\u221e, the number of characters must also approach infinity N c \u2192\u221e. Formally, consider the tokenizer\nfunction T : S [N] \u2192X [N] operating on infinite symbol sequences; that is, T (s) = x where s is an infinite sequence in S [\u221e] . For\nnatural number, i \u2208 N, define m i : S [N] \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Specifically, suppose s is given, then\n\n\nm i (s) = min n [{][length (][T][ (][s] [n] [))][ \u2265] [i][}][ .] (1)\n\n\nWe note that, by construction, lim n\u2192\u221e length (T (s n )) = \u221e and, as such, m i (\u00b7) is well-defined. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple finite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (s n ) = T (s n+1 ). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (finite) series of input symbols in the restricted\nset for which T (y n ) = x i is s m i (s) . Given a fixed sequence s, we can express the number of characters contained in a token\n\nas\nb i = m i (s) \u2212 m i\u22121 (s)\n\n\n4\n\n\nwith initial condition m \u22121 = 0. Consequently, the number of characters embedded in the first N T tokens for a random input\nbecomes N c = [\ufffd] i [N] =1 [T] [B] [i] [.]\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {S k } [\u221e] k=1 [,]\n{B i } [\u221e] i=1 [, and][ {][X] [i] [}] [\u221e] i=1 [are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that]\n\n\n\n\u2212 [1]\n\n\n\n\n[1]\n\nn [log] [2] [ p] [S] [n] [(][S] [1] [, . . ., S] [n] [) =][ \u2212] n [1]\n\n\n\nn [log] [2] [ p] [S] [n] [(][S] [n] [)][ \u2192] [H][(][S][)] almost surely. (2)\n\n\n\nLet \u2126 S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\n\n\n\n[1]\n\ni [log] [2] [ p] [X] [i] [(][X] [1] [, . . ., X] [i] [) =][ \u2212] [1] i\n\n\n\n\u2212 [1]\n\n\n\ni [log] [2] [ p] [X] [i] [(][X] [i] [)][ \u2192] [H][(][X][)] almost surely. (3)\n\n\n\nDefine \u2126 X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\n\n\nm i (S)\nlim = E [B] almost surely. (4)\ni\u2192\u221e i\n\n\nSet \u2126 B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126 S \u2229 \u2126 X \u2229 \u2126 B, we deduce that\n\n\nH(S) = lim\nk\u2192\u221e [\u2212] k [1] [log] [2] [ p] [S] [k] [(][S] [k] [(][\u03c9][))]\n\n= lim i\u2192\u221e [\u2212] l [1] i log 2 p S li (S l i (\u03c9))\n\n\n= lim log 2 Pr (X i = T (S l i (\u03c9)))\ni\u2192\u221e [\u2212] l [1] i\n\n\n\n1\n\ni [log] [2] [ Pr (][X] [i] [ =][ x] [i] [) =][ H] E [(] B [X][)]\n\n\n\n1 1\n\n= \u2212\nE[B] i [lim] \u2192\u221e i\n\n\n\nE[B] [.]\n\n\n\nThe first equality follows from (2). The second equality is a consequence of the fact that {l i = m i (S(\u03c9))|i \u2208 N} is an infinite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\n\ntwo events,\n{\u03c9 \u2208 \u2126|X i (\u03c9) = x i } = {\u03c9 \u2208 \u2126|T (S m i (S(\u03c9)) ) = x i }.\n\n\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126 B . The sets \u2126 S, \u2126 X, and \u2126 B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\n\nH(S) = [H][(][X][)] almost surely.\n\nE[B]\n\n\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\n\n\n\nPr\n\n\ufffd\n\n\n\n\ufffd\n\n\n\nH(X) \u2264 lim\nN T \u2192\u221e [\u2212] N [1] T\n\n\n\nN T\n\ufffd\n\n\n\n\ufffd log 2 q i (X i )\n\n\ni=1\n\n\n\n= 1, (5)\n\n\n\nwhere q i (\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\n\n\n\n\n[1] N T\n\nN T \ufffd i=1 [log] 2 [q] [i] [(][X] [i] [)]\n\n\n\nH(S) \u2264 lim N T \u2192\u221e \u2212 N [1]\n\n\n\n. (6)\nE[B]\n\n\n\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by H ub . The\nnumerator in (6) represents the average number of bits required to represent the tokens X N T and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\n\n\nB. Encoding schemes\n\n\nWe consider three schemes for the lossless compression block in Fig. 3.\n\n\n5\n\n\n1) Compressing the ranks using zlib: The first scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1 LLaMA+zlib .\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token x i at epoch i by using a prefix-free code assuming q i to be the true distribution of\nthe tokens. A natural choice for a prefix-free code is a Huffman code. Instead, for simplicity, we use a prefix-free code where\n1\nthe codeword for the token x i is of length l i = \u2308log 2 q i (x i ) [\u2309][. A prefix-free code with this length for][ x] [i] [ is guaranteed to exist]\nsince this choice of lengths satisfies the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1 LLaMA+TbyT,\nis given by\n\n\n\nN T\n\ufffd\n\n\ni=1\n\n\n\n1\nlog 2\n\ufffd q i (x i )\n\n\n\n\ufffd\n\n\n\n\u03c1 LLaMA+TbyT =\n\n\n\n\ufffd Ni=1 T [b] [i] .\n\n\n\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use q i (x i ) as the probability of token x i at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1 LLM+AC . It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\n\n\n\u03c1 LLM+AC \u2248\n\n\n\nN T 1\n\ufffd i=1 log 2 q i (x i )\n\n\ufffd Ni=1 T [b] [i] . (7)\n\n\n\nClearly, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and \u03c1 LLM+AC also provide upper bounds on H(S). H ub, \u03c1 LLaMA+zlib, \u03c1 LLaMA+TbyT, and\n\u03c1 LLM+AC are estimated using a finite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\n\n\nIV. R ESULTS\n\n\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it difficult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\n[Nevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of](http://mattmahoney.net/dc/text8.zip)\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M ) is shown in Table II. As expected, the\ncompression performance improves with increasing M . We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n\n\n6\n\n\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\n\n\nV. A CKNOWLEDGEMENT\n\n\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\n\n\nTABLE I\n\nR ESULTS FOR 1MB OF TEXT FROM TEXT 8 DATASET\n\n\n\nBatch\nN c N T\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC ZPAQ pq8h\n\n\n\nNo. (bpc) file size (bits) (bpc) (bpc) (bpc) (bpc)\n\n\n1 466, 650 100, 000 0.6882 1.0513 0.8215 0.689\n\n\n2 461, 477 100, 000 0.6893 1.0558 0.8242 0.6901\n\n\n3 454, 599 100, 000 0.699 1.0681 0.8357 0.6999\n\n\n4 462, 755 100, 000 0.6748 1.0346 0.8093 0.6757\n\n\n5 453, 847 100, 000 0.7481 1.1265 0.8831 0.749\n\n\n6 458, 252 100, 000 0.7218 1.0957 0.8567 0.7227\n\n\n7 451, 036 100, 000 0.6959 1.0729 0.8353 0.6968\n\n\n8 447, 953 100, 000 0.7092 1.0896 0.8489 0.7101\n\n\n9 462, 665 100, 000 0.7394 1.1126 0.8713 0.7402\n\n\n10 449, 621 100, 000 0.7269 1.1046 0.8643 0.7277\n\n\nTotal 9, 137, 710 2, 000, 000 0.7093 1.0812 0.845 0.7101 1.4 [1] 1.2 [2]\n\n\nTABLE II\n\nC OMPRESSION PERFORMANCE OF THE LLM ON THE TEXT 8 DATASET, AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\n\n\n(bpc) file size (bits) (bpc) (bpc)\n\n\n\n31 4, 568, 855 1, 000, 000 0.9139 1.3159 1.0425 0.9145\n\n\n127 4, 568, 855 1, 000, 000 0.7511 1.1303 0.8847 0.752\n\n\n255 4, 568, 855 1, 000, 000 0.7242 1.0985 0.859 0.725\n\n\n511 4, 568, 855 1, 000, 000 0.7093 1.0812 0.845 0.7101\n\n\n1 This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2 This result is taken from [12] and it corresponds to the full 100MB dataset text8\n\n\n7\n\n\n\nTABLE III\n\nM EAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC\nM\n\n(bpc) (bpc) (bpc) (bpc)\n\n\n31 0.9139 \u00b1 0.0263 1.3159 \u00b1 0.0329 1.0425 \u00b1 0.0262 0.9145 \u00b1 0.0263\n\n\n127 0.7511 \u00b1 0.0233 1.1303 \u00b1 0.0292 0.8847 \u00b1 0.0231 0.752 \u00b1 0.0233\n\n\n255 0.7242 \u00b1 0.0234 1.0985 \u00b1 0.0289 0.859 \u00b1 0.0232 0.725 \u00b1 0.0234\n\n\n511 0.7093 \u00b1 0.0228 1.0812 \u00b1 0.028 0.845 \u00b1 0.0226 0.7101 \u00b1 0.0228\n\n\nTABLE IV\n\nC OMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN P ROJECT G UTENBERG [7], AS A FUNCTION OF ITS MEMORY (M )\n\n\n\nM N c N t\n\n\n\nH ub \u03c1 LLaMA+zlib \u03c1 LLaMA+TbyT \u03c1 LLaMA+AC Standalone Zlib\n\n\n(bpc) (bpc) (bpc) (bpc) (bpc)\n\n\n\n31 508, 463 115, 000 1.0919 1.5316 1.2152 1.0924 2.80\n\n\n127 508, 463 115, 000 0.8973 1.3128 1.0235 0.8982 2.80\n\n\n255 508, 463 115, 000 0.8618 1.2684 0.9899 0.8627 2.80\n\n\n511 508, 463 115, 000 0.8417 1.2465 0.9711 0.8426 2.80\n\n\n8\n\n\nR EFERENCES\n\n\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR filters),\u201d 2023.\n\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and efficient foundation language models,\u201d\n2023.\n\n[7] J. Frank Dobie, Legends of Texas, United States, Texas Folk-Lore Society, 1924; Project Gutenberg, May 25, 2023, 2023,\n[https://www.gutenberg.org/ebooks/70859.](https://www.gutenberg.org/ebooks/70859)\n\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n\n\n"}[00m


[1m[95m# Agent:[00m [1m[92mAI Assistant[00m
[95m## Final Answer:[00m [92m

Thought: The paper "LLMZip: Lossless Text Compression using Large Language Models" (arXiv:2306.04050v2) has been successfully downloaded. I now have the full content of the document. My next step is to extract the information that is likely to appear later in the document, specifically focusing on facts, equations, definitions, datasets, and references, to create a concise textual synopsis.

Based on the document prefix and the full content, the likely future content will include:

*   **III. COMPRESSION USING LLMS**: This section details the formal methodology of LLMZip, including the definition of tokens, the role of the language model in producing probability mass functions (PMFs), and the compression of ranks. It also introduces the compression ratio metric.
    *   **A. Entropy bounds**: This subsection provides the theoretical framework for entropy estimation, including equations relating the entropy of characters (H(S)) to the entropy of tokens (H(X)) and the average number of characters per token (E[B]). It also presents the asymptotic upper bound on the entropy rate H(S) (Equation 6).
    *   **B. Encoding schemes**: This subsection describes the three specific compression schemes considered:
        1.  **Compressing the ranks using zlib (LLaMA+zlib)**.
        2.  **Token-by-Token Compression (LLaMA+TbyT)**, including its compression ratio formula.
        3.  **Arithmetic Coding (LLM+AC)**, highlighting its near-optimality and its compression ratio formula (Equation 7).
*   **IV. RESULTS**: This section presents the experimental findings.
    *   **LLaMA-7B and SentencePiece tokenizer**: Details on the specific LLM and tokenizer used.
    *   **Datasets**: Discussion of the text8 dataset and a recently released book from Project Gutenberg [7].
    *   **Baselines**: Comparison with state-of-the-art text compression schemes like ZPAQ and paq8h.
    *   **Tables**: Tables I, II, III, and IV present quantitative results:
        *   Table I: Results for 1MB of text from the text8 dataset, showing H_ub, compression ratios for LLaMA+zlib, LLaMA+TbyT, LLaMA+AC, and comparisons with ZPAQ and pq8h.
        *   Table II: Compression performance as a function of LLM memory (M) on the text8 dataset.
        *   Table III: Mean and standard deviation of entropy bounds and compression ratios over multiple batches.
        *   Table IV: Compression performance on the Project Gutenberg book [7], including a comparison with standalone zlib.
    *   **Key findings**: Specific entropy estimates (e.g., 0.709 bits/character for text8, 0.85 bits/character for the book) and compression ratios (e.g., 0.7101 bits/character for LLaMA+AC on text8, 0.8426 bits/character on the book).
    *   **Limitations**: Discussion on the potential optimism of text8 results due to LLaMA-7B's training corpus and the differing input sizes for comparisons.
*   **V. ACKNOWLEDGEMENT**: A brief section acknowledging contributions.
*   **REFERENCES**: A list of cited works.

I will now synthesize this information into a concise textual synopsis.The document "LLMZip: Lossless Text Compression using Large Language Models" will further detail its methodology, experimental results, and conclusions.

**Methodology (III. Compression Using LLMs):**
The paper will formally define the compression process. It will explain how a sentence `s` of `N_c` characters is parsed into `N_T` tokens `x = x_1, ..., x_{N_T}` using a dictionary `X`. A language model with memory `M` predicts the next token `x_i` by producing a probability mass function `q_i(x_i) := Pr(X_i = x_i | x_{i-1}, ..., x_{i-M})`. The rank `r_i` of the actual token `x_i` in the sorted PMF `~q_i` is then computed. The compression ratio `ρ` is defined as `N_b / N_c` bits/character, where `N_b` is the number of bits after compression.

**Entropy Bounds (III.A. Entropy bounds):**
The paper will establish a theoretical connection between the entropy of characters `H(S)` and the entropy of tokens `H(X)`. Key equations will include:
*   `m_i(s) = min n {length(T(s^n)) >= i}` (Equation 1), defining the character index for the i-th token.
*   `H(S) = H(X) / E[B]` almost surely, where `E[B]` is the average number of characters per token.
*   An asymptotic upper bound on the entropy rate `H(S)`: `H_ub <= lim_{N_T->∞} (-1/N_T) Σ_{i=1}^{N_T} log_2 q_i(X_i) / E[B]` (Equation 6). This bound is compared to previous estimates like 1.3 bits/character by Cover and King [1].

**Encoding Schemes (III.B. Encoding schemes):**
Three specific lossless compression schemes will be described:
1.  **LLaMA+zlib**: Compressing the sequence of ranks `r_i` using the zlib algorithm.
2.  **LLaMA+TbyT (Token-by-Token Compression)**: Using a time-varying prefix-free code (e.g., Huffman code) with codeword length `l_i = ⌈log_2(1/q_i(x_i))⌉`. The compression ratio `ρ_LLaMA+TbyT` is given by `(Σ_{i=1}^{N_T} ⌈log_2(1/q_i(x_i))⌉) / (Σ_{i=1}^{N_T} b_i)`.
3.  **LLM+AC (Arithmetic Coding)**: Combining the LLM's output probabilities `q_i(x_i)` with arithmetic coding, which is expected to be nearly optimal. The compression ratio `ρ_LLM+AC` is approximated by `(Σ_{i=1}^{N_T} log_2(1/q_i(x_i))) / (Σ_{i=1}^{N_T} b_i)` (Equation 7).

**Results (IV. Results):**
This section will present the empirical performance of LLMZip.
*   **LLM and Tokenizer**: LLaMA-7B [6] is used as the language model, and SentencePiece tokenizer [11] with a dictionary size of 32,000.
*   **Datasets**: Experiments are conducted on the `text8` dataset (derived from Wikipedia) and a recently published book from Project Gutenberg [7] (released May 25, 2023).
*   **Baselines**: Performance is compared against state-of-the-art text compression schemes like ZPAQ [5] (1.4 bpc on text8) and paq8h [12] (1.2 bpc on text8). Standalone zlib is also used for comparison (2.8 bpc on the Project Gutenberg book).
*   **Key Findings**:
    *   **text8 dataset (1MB section)**: The asymptotic upper bound on entropy `H_ub` is estimated at 0.709 bits/character. LLaMA+AC achieves a compression ratio of 0.7101 bits/character, outperforming ZPAQ and paq8h. LLaMA+zlib and LLaMA+TbyT also show improved performance.
    *   **Project Gutenberg book (100KB section)**: `H_ub` is estimated at 0.8417 bits/character. LLaMA+AC achieves 0.8426 bits/character.
    *   **Memory (M) dependence**: Compression performance improves with increasing LLM memory (M), as shown in Table II.
    *   **Variance**: Results show variance depending on input text, and comparisons are made with different input sizes.
    *   **Optimism**: Results on text8 might be optimistic due to LLaMA-7B's training on Wikipedia.
*   **Tables**:
    *   **Table I**: Detailed results for 1MB of text from the text8 dataset, showing `N_c`, `N_T`, `H_ub`, `ρ_LLaMA+zlib`, `ρ_LLaMA+TbyT`, `ρ_LLM+AC`, and baseline `ZPAQ` and `pq8h` values.
    *   **Table II**: Compression performance on the text8 dataset as a function of LLM memory `M` (31, 127, 255, 511 tokens).
    *   **Table III**: Mean and standard deviation of entropy bounds and compression ratios over 10 batches of 100,000 tokens for different `M`.
    *   **Table IV**: Compression performance on the Project Gutenberg book [7] as a function of LLM memory `M`, including standalone Zlib results.

**Acknowledgement (V. Acknowledgement):**
The authors will acknowledge Andreas Kirsch for discussions on arithmetic coding.

**References:**
A list of 12 references will be provided, including foundational works by Shannon [3], Cover and King [1], and works on arithmetic coding [4, 9], RNN-based compression [5], LLaMA [6], SentencePiece [11], and the datasets used [7, 12].[00m


🚀 Crew: crew
├── 📋 Task: bdecde0f-ff94-4917-afd5-5e2f9934ce96
│   Assigned to: AI Assistant
│   Status: ✅ Completed
│   ├── ✅ Reasoning Completed
│   └── 🔧 Used download_paper (1)
├── 🔧 Using create_reasoning_plan from LLM available_function
├── 🔄 Tool Usage Started
├── 🔧 Using create_reasoning_plan from LLM available_function
└── ✅ Tool Usage Completed┌────────────────────────────── Task Completion ──────────────────────────────┐
│                                                                             │
│  Task Completed                                                             │
│  Name: bdecde0f-ff94-4917-afd5-5e2f9934ce96                                 │
│  Agent: AI Assistant                                                        │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

┌────────────────────────────── Crew Completion ──────────────────────────────┐
│                                                                             │
│  Crew Execution Completed                                                   │
│  Name: crew                                                                 │
│  ID: f834b4f6-da0a-465e-92cd-142f38365828                                   │
│                                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

AGENT_RESULT_JSON:{"final_text": "\nThought: The paper \"LLMZip: Lossless Text Compression using Large Language Models\" (arXiv:2306.04050v2) has been successfully downloaded. I now have the full content of the document. My next step is to extract the information that is likely to appear later in the document, specifically focusing on facts, equations, definitions, datasets, and references, to create a concise textual synopsis.\n\nBased on the document prefix and the full content, the likely future content will include:\n\n*   **III. COMPRESSION USING LLMS**: This section details the formal methodology of LLMZip, including the definition of tokens, the role of the language model in producing probability mass functions (PMFs), and the compression of ranks. It also introduces the compression ratio metric.\n    *   **A. Entropy bounds**: This subsection provides the theoretical framework for entropy estimation, including equations relating the entropy of characters (H(S)) to the entropy of tokens (H(X)) and the average number of characters per token (E[B]). It also presents the asymptotic upper bound on the entropy rate H(S) (Equation 6).\n    *   **B. Encoding schemes**: This subsection describes the three specific compression schemes considered:\n        1.  **Compressing the ranks using zlib (LLaMA+zlib)**.\n        2.  **Token-by-Token Compression (LLaMA+TbyT)**, including its compression ratio formula.\n        3.  **Arithmetic Coding (LLM+AC)**, highlighting its near-optimality and its compression ratio formula (Equation 7).\n*   **IV. RESULTS**: This section presents the experimental findings.\n    *   **LLaMA-7B and SentencePiece tokenizer**: Details on the specific LLM and tokenizer used.\n    *   **Datasets**: Discussion of the text8 dataset and a recently released book from Project Gutenberg [7].\n    *   **Baselines**: Comparison with state-of-the-art text compression schemes like ZPAQ and paq8h.\n    *   **Tables**: Tables I, II, III, and IV present quantitative results:\n        *   Table I: Results for 1MB of text from the text8 dataset, showing H_ub, compression ratios for LLaMA+zlib, LLaMA+TbyT, LLaMA+AC, and comparisons with ZPAQ and pq8h.\n        *   Table II: Compression performance as a function of LLM memory (M) on the text8 dataset.\n        *   Table III: Mean and standard deviation of entropy bounds and compression ratios over multiple batches.\n        *   Table IV: Compression performance on the Project Gutenberg book [7], including a comparison with standalone zlib.\n    *   **Key findings**: Specific entropy estimates (e.g., 0.709 bits/character for text8, 0.85 bits/character for the book) and compression ratios (e.g., 0.7101 bits/character for LLaMA+AC on text8, 0.8426 bits/character on the book).\n    *   **Limitations**: Discussion on the potential optimism of text8 results due to LLaMA-7B's training corpus and the differing input sizes for comparisons.\n*   **V. ACKNOWLEDGEMENT**: A brief section acknowledging contributions.\n*   **REFERENCES**: A list of cited works.\n\nI will now synthesize this information into a concise textual synopsis.The document \"LLMZip: Lossless Text Compression using Large Language Models\" will further detail its methodology, experimental results, and conclusions.\n\n**Methodology (III. Compression Using LLMs):**\nThe paper will formally define the compression process. It will explain how a sentence `s` of `N_c` characters is parsed into `N_T` tokens `x = x_1, ..., x_{N_T}` using a dictionary `X`. A language model with memory `M` predicts the next token `x_i` by producing a probability mass function `q_i(x_i) := Pr(X_i = x_i | x_{i-1}, ..., x_{i-M})`. The rank `r_i` of the actual token `x_i` in the sorted PMF `~q_i` is then computed. The compression ratio `\u03c1` is defined as `N_b / N_c` bits/character, where `N_b` is the number of bits after compression.\n\n**Entropy Bounds (III.A. Entropy bounds):**\nThe paper will establish a theoretical connection between the entropy of characters `H(S)` and the entropy of tokens `H(X)`. Key equations will include:\n*   `m_i(s) = min n {length(T(s^n)) >= i}` (Equation 1), defining the character index for the i-th token.\n*   `H(S) = H(X) / E[B]` almost surely, where `E[B]` is the average number of characters per token.\n*   An asymptotic upper bound on the entropy rate `H(S)`: `H_ub <= lim_{N_T->\u221e} (-1/N_T) \u03a3_{i=1}^{N_T} log_2 q_i(X_i) / E[B]` (Equation 6). This bound is compared to previous estimates like 1.3 bits/character by Cover and King [1].\n\n**Encoding Schemes (III.B. Encoding schemes):**\nThree specific lossless compression schemes will be described:\n1.  **LLaMA+zlib**: Compressing the sequence of ranks `r_i` using the zlib algorithm.\n2.  **LLaMA+TbyT (Token-by-Token Compression)**: Using a time-varying prefix-free code (e.g., Huffman code) with codeword length `l_i = \u2308log_2(1/q_i(x_i))\u2309`. The compression ratio `\u03c1_LLaMA+TbyT` is given by `(\u03a3_{i=1}^{N_T} \u2308log_2(1/q_i(x_i))\u2309) / (\u03a3_{i=1}^{N_T} b_i)`.\n3.  **LLM+AC (Arithmetic Coding)**: Combining the LLM's output probabilities `q_i(x_i)` with arithmetic coding, which is expected to be nearly optimal. The compression ratio `\u03c1_LLM+AC` is approximated by `(\u03a3_{i=1}^{N_T} log_2(1/q_i(x_i))) / (\u03a3_{i=1}^{N_T} b_i)` (Equation 7).\n\n**Results (IV. Results):**\nThis section will present the empirical performance of LLMZip.\n*   **LLM and Tokenizer**: LLaMA-7B [6] is used as the language model, and SentencePiece tokenizer [11] with a dictionary size of 32,000.\n*   **Datasets**: Experiments are conducted on the `text8` dataset (derived from Wikipedia) and a recently published book from Project Gutenberg [7] (released May 25, 2023).\n*   **Baselines**: Performance is compared against state-of-the-art text compression schemes like ZPAQ [5] (1.4 bpc on text8) and paq8h [12] (1.2 bpc on text8). Standalone zlib is also used for comparison (2.8 bpc on the Project Gutenberg book).\n*   **Key Findings**:\n    *   **text8 dataset (1MB section)**: The asymptotic upper bound on entropy `H_ub` is estimated at 0.709 bits/character. LLaMA+AC achieves a compression ratio of 0.7101 bits/character, outperforming ZPAQ and paq8h. LLaMA+zlib and LLaMA+TbyT also show improved performance.\n    *   **Project Gutenberg book (100KB section)**: `H_ub` is estimated at 0.8417 bits/character. LLaMA+AC achieves 0.8426 bits/character.\n    *   **Memory (M) dependence**: Compression performance improves with increasing LLM memory (M), as shown in Table II.\n    *   **Variance**: Results show variance depending on input text, and comparisons are made with different input sizes.\n    *   **Optimism**: Results on text8 might be optimistic due to LLaMA-7B's training on Wikipedia.\n*   **Tables**:\n    *   **Table I**: Detailed results for 1MB of text from the text8 dataset, showing `N_c`, `N_T`, `H_ub`, `\u03c1_LLaMA+zlib`, `\u03c1_LLaMA+TbyT`, `\u03c1_LLM+AC`, and baseline `ZPAQ` and `pq8h` values.\n    *   **Table II**: Compression performance on the text8 dataset as a function of LLM memory `M` (31, 127, 255, 511 tokens).\n    *   **Table III**: Mean and standard deviation of entropy bounds and compression ratios over 10 batches of 100,000 tokens for different `M`.\n    *   **Table IV**: Compression performance on the Project Gutenberg book [7] as a function of LLM memory `M`, including standalone Zlib results.\n\n**Acknowledgement (V. Acknowledgement):**\nThe authors will acknowledge Andreas Kirsch for discussions on arithmetic coding.\n\n**References:**\nA list of 12 references will be provided, including foundational works by Shannon [3], Cover and King [1], and works on arithmetic coding [4, 9], RNN-based compression [5], LLaMA [6], SentencePiece [11], and the datasets used [7, 12].", "duration_ms": 31223}

Thought: The paper "LLMZip: Lossless Text Compression using Large Language Models" (arXiv:2306.04050v2) has been successfully downloaded. I now have the full content of the document. My next step is to extract the information that is likely to appear later in the document, specifically focusing on facts, equations, definitions, datasets, and references, to create a concise textual synopsis.

Based on the document prefix and the full content, the likely future content will include:

*   **III. COMPRESSION USING LLMS**: This section details the formal methodology of LLMZip, including the definition of tokens, the role of the language model in producing probability mass functions (PMFs), and the compression of ranks. It also introduces the compression ratio metric.
    *   **A. Entropy bounds**: This subsection provides the theoretical framework for entropy estimation, including equations relating the entropy of characters (H(S)) to the entropy of tokens (H(X)) and the average number of characters per token (E[B]). It also presents the asymptotic upper bound on the entropy rate H(S) (Equation 6).
    *   **B. Encoding schemes**: This subsection describes the three specific compression schemes considered:
        1.  **Compressing the ranks using zlib (LLaMA+zlib)**.
        2.  **Token-by-Token Compression (LLaMA+TbyT)**, including its compression ratio formula.
        3.  **Arithmetic Coding (LLM+AC)**, highlighting its near-optimality and its compression ratio formula (Equation 7).
*   **IV. RESULTS**: This section presents the experimental findings.
    *   **LLaMA-7B and SentencePiece tokenizer**: Details on the specific LLM and tokenizer used.
    *   **Datasets**: Discussion of the text8 dataset and a recently released book from Project Gutenberg [7].
    *   **Baselines**: Comparison with state-of-the-art text compression schemes like ZPAQ and paq8h.
    *   **Tables**: Tables I, II, III, and IV present quantitative results:
        *   Table I: Results for 1MB of text from the text8 dataset, showing H_ub, compression ratios for LLaMA+zlib, LLaMA+TbyT, LLaMA+AC, and comparisons with ZPAQ and pq8h.
        *   Table II: Compression performance as a function of LLM memory (M) on the text8 dataset.
        *   Table III: Mean and standard deviation of entropy bounds and compression ratios over multiple batches.
        *   Table IV: Compression performance on the Project Gutenberg book [7], including a comparison with standalone zlib.
    *   **Key findings**: Specific entropy estimates (e.g., 0.709 bits/character for text8, 0.85 bits/character for the book) and compression ratios (e.g., 0.7101 bits/character for LLaMA+AC on text8, 0.8426 bits/character on the book).
    *   **Limitations**: Discussion on the potential optimism of text8 results due to LLaMA-7B's training corpus and the differing input sizes for comparisons.
*   **V. ACKNOWLEDGEMENT**: A brief section acknowledging contributions.
*   **REFERENCES**: A list of cited works.

I will now synthesize this information into a concise textual synopsis.The document "LLMZip: Lossless Text Compression using Large Language Models" will further detail its methodology, experimental results, and conclusions.

**Methodology (III. Compression Using LLMs):**
The paper will formally define the compression process. It will explain how a sentence `s` of `N_c` characters is parsed into `N_T` tokens `x = x_1, ..., x_{N_T}` using a dictionary `X`. A language model with memory `M` predicts the next token `x_i` by producing a probability mass function `q_i(x_i) := Pr(X_i = x_i | x_{i-1}, ..., x_{i-M})`. The rank `r_i` of the actual token `x_i` in the sorted PMF `~q_i` is then computed. The compression ratio `ρ` is defined as `N_b / N_c` bits/character, where `N_b` is the number of bits after compression.

**Entropy Bounds (III.A. Entropy bounds):**
The paper will establish a theoretical connection between the entropy of characters `H(S)` and the entropy of tokens `H(X)`. Key equations will include:
*   `m_i(s) = min n {length(T(s^n)) >= i}` (Equation 1), defining the character index for the i-th token.
*   `H(S) = H(X) / E[B]` almost surely, where `E[B]` is the average number of characters per token.
*   An asymptotic upper bound on the entropy rate `H(S)`: `H_ub <= lim_{N_T->∞} (-1/N_T) Σ_{i=1}^{N_T} log_2 q_i(X_i) / E[B]` (Equation 6). This bound is compared to previous estimates like 1.3 bits/character by Cover and King [1].

**Encoding Schemes (III.B. Encoding schemes):**
Three specific lossless compression schemes will be described:
1.  **LLaMA+zlib**: Compressing the sequence of ranks `r_i` using the zlib algorithm.
2.  **LLaMA+TbyT (Token-by-Token Compression)**: Using a time-varying prefix-free code (e.g., Huffman code) with codeword length `l_i = ⌈log_2(1/q_i(x_i))⌉`. The compression ratio `ρ_LLaMA+TbyT` is given by `(Σ_{i=1}^{N_T} ⌈log_2(1/q_i(x_i))⌉) / (Σ_{i=1}^{N_T} b_i)`.
3.  **LLM+AC (Arithmetic Coding)**: Combining the LLM's output probabilities `q_i(x_i)` with arithmetic coding, which is expected to be nearly optimal. The compression ratio `ρ_LLM+AC` is approximated by `(Σ_{i=1}^{N_T} log_2(1/q_i(x_i))) / (Σ_{i=1}^{N_T} b_i)` (Equation 7).

**Results (IV. Results):**
This section will present the empirical performance of LLMZip.
*   **LLM and Tokenizer**: LLaMA-7B [6] is used as the language model, and SentencePiece tokenizer [11] with a dictionary size of 32,000.
*   **Datasets**: Experiments are conducted on the `text8` dataset (derived from Wikipedia) and a recently published book from Project Gutenberg [7] (released May 25, 2023).
*   **Baselines**: Performance is compared against state-of-the-art text compression schemes like ZPAQ [5] (1.4 bpc on text8) and paq8h [12] (1.2 bpc on text8). Standalone zlib is also used for comparison (2.8 bpc on the Project Gutenberg book).
*   **Key Findings**:
    *   **text8 dataset (1MB section)**: The asymptotic upper bound on entropy `H_ub` is estimated at 0.709 bits/character. LLaMA+AC achieves a compression ratio of 0.7101 bits/character, outperforming ZPAQ and paq8h. LLaMA+zlib and LLaMA+TbyT also show improved performance.
    *   **Project Gutenberg book (100KB section)**: `H_ub` is estimated at 0.8417 bits/character. LLaMA+AC achieves 0.8426 bits/character.
    *   **Memory (M) dependence**: Compression performance improves with increasing LLM memory (M), as shown in Table II.
    *   **Variance**: Results show variance depending on input text, and comparisons are made with different input sizes.
    *   **Optimism**: Results on text8 might be optimistic due to LLaMA-7B's training on Wikipedia.
*   **Tables**:
    *   **Table I**: Detailed results for 1MB of text from the text8 dataset, showing `N_c`, `N_T`, `H_ub`, `ρ_LLaMA+zlib`, `ρ_LLaMA+TbyT`, `ρ_LLM+AC`, and baseline `ZPAQ` and `pq8h` values.
    *   **Table II**: Compression performance on the text8 dataset as a function of LLM memory `M` (31, 127, 255, 511 tokens).
    *   **Table III**: Mean and standard deviation of entropy bounds and compression ratios over 10 batches of 100,000 tokens for different `M`.
    *   **Table IV**: Compression performance on the Project Gutenberg book [7] as a function of LLM memory `M`, including standalone Zlib results.

**Acknowledgement (V. Acknowledgement):**
The authors will acknowledge Andreas Kirsch for discussions on arithmetic coding.

**References:**
A list of 12 references will be provided, including foundational works by Shannon [3], Cover and King [1], and works on arithmetic coding [4, 9], RNN-based compression [5], LLaMA [6], SentencePiece [11], and the datasets used [7, 12].
