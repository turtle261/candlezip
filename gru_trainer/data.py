"""Data loading utilities for GRU distillation and RL."""

from __future__ import annotations

import json
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Sequence, Tuple

import torch
from torch.utils.data import Dataset


@dataclass
class CorpusStats:
    """Summary statistics for a single corpus."""

    dataset: str
    num_segments: int
    num_characters: int
    source_path: Path


def collect_agent_corpora(root: Path | str, *, min_text_length: int = 0) -> Tuple[Dict[str, List[str]], List[CorpusStats]]:
    """Collect agent distillation corpora from a Candlezip results directory.

    Parameters
    ----------
    root:
        Root directory that contains Candlezip run outputs. Every ``agent_cache.jsonl``
        file encountered recursively beneath this root is parsed and contributes a
        corpus. Each corpus is ordered by ``chunk_index`` when present so that the
        reconstructed text respects the original streaming order.
    min_text_length:
        Optional minimum length for an ``agent_text`` segment. Empty and extremely
        short strings often correspond to metadata or degenerate hints, so the
        threshold helps filter noise while keeping useful samples.

    Returns
    -------
    corpora:
        Mapping from directory stem (the parent folder name of the cache file) to the
        ordered list of text segments extracted from that cache.
    stats:
        Human-readable statistics describing each collected corpus.
    """

    root_path = Path(root)
    if not root_path.exists():
        raise FileNotFoundError(f"Results root {root_path} does not exist")

    corpora: Dict[str, List[str]] = {}
    stats: List[CorpusStats] = []

    for cache_path in sorted(root_path.rglob("agent_cache.jsonl")):
        dataset_name = cache_path.parent.name
        segments: List[Tuple[int, str]] = []

        with cache_path.open("r", encoding="utf-8") as handle:
            for line_number, line in enumerate(handle, start=1):
                line = line.strip()
                if not line:
                    continue
                try:
                    record = json.loads(line)
                except json.JSONDecodeError as exc:  # pragma: no cover - defensive
                    raise ValueError(
                        f"Invalid JSON in {cache_path} line {line_number}: {exc}"
                    ) from exc

                text = record.get("agent_text")
                if not text:
                    continue
                text = text.strip("\n")
                if len(text) < min_text_length:
                    continue
                chunk_index = record.get("chunk_index")
                try:
                    chunk_idx_int = int(chunk_index) if chunk_index is not None else len(segments)
                except (TypeError, ValueError):  # pragma: no cover - fallback
                    chunk_idx_int = len(segments)
                segments.append((chunk_idx_int, text))

        if not segments:
            continue

        segments.sort(key=lambda item: item[0])
        ordered_texts = [segment for _, segment in segments]
        corpora[dataset_name] = ordered_texts
        stats.append(
            CorpusStats(
                dataset=dataset_name,
                num_segments=len(ordered_texts),
                num_characters=sum(len(seg) for seg in ordered_texts),
                source_path=cache_path,
            )
        )

    if not corpora:
        raise FileNotFoundError(
            f"No agent_cache.jsonl files found beneath {root_path}. Ensure Candlezip "
            "distillation outputs are present."
        )

    return corpora, stats


def build_vocabulary(corpora: Dict[str, Sequence[str]]) -> Tuple[List[str], Dict[str, int]]:
    """Create a character-level vocabulary from the collected corpora.

    The vocabulary is intentionally compact: we reserve index ``0`` for padding to
    support efficient mini-batching, and then enumerate the sorted unique characters
    observed in the corpora. This keeps the embedding matrix small while retaining
    fidelity for the agent transcripts.
    """

    charset = {ch for segments in corpora.values() for text in segments for ch in text}
    symbols = sorted(charset)
    vocab = ["<pad>"] + symbols
    stoi = {symbol: idx for idx, symbol in enumerate(vocab)}
    return vocab, stoi


def encode_corpora(
    corpora: Dict[str, Sequence[str]],
    stoi: Dict[str, int],
    *,
    delimiter: str = "\n\n",
) -> Dict[str, torch.Tensor]:
    """Encode corpora into contiguous tensors of token identifiers.

    Parameters
    ----------
    corpora:
        Mapping of corpus name to the ordered sequence of text segments.
    stoi:
        Vocabulary mapping generated by :func:`build_vocabulary`.
    delimiter:
        String inserted between segments to preserve separation. Two newlines by
        default approximate original paragraph boundaries while keeping the model's
        receptive field small.
    """

    encoded: Dict[str, torch.Tensor] = {}
    pad_index = stoi["<pad>"]

    for dataset_name, segments in corpora.items():
        joined = delimiter.join(segments)
        indices = [stoi.get(ch, pad_index) for ch in joined]
        encoded[dataset_name] = torch.tensor(indices, dtype=torch.long)

    return encoded


class DistillationDataset(Dataset):
    """Sliding-window dataset for autoregressive character modelling."""

    def __init__(self, tokens: torch.Tensor, seq_len: int) -> None:
        if tokens.ndim != 1:
            raise ValueError("Expected a 1-D tensor of token ids")
        if tokens.numel() <= seq_len:
            raise ValueError("Token sequence is shorter than the requested context")
        self.tokens = tokens
        self.seq_len = seq_len

    def __len__(self) -> int:
        return self.tokens.numel() - self.seq_len

    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:
        window = self.tokens[index : index + self.seq_len + 1]
        src = window[:-1]
        tgt = window[1:]
        return src, tgt

    @property
    def total_tokens(self) -> int:
        return self.tokens.numel()

